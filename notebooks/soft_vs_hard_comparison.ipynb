{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Prompt vs Hard Prompt (GNN+LLM) Comparison\n",
    "\n",
    "This notebook compares two approaches for injecting graph knowledge into LLMs:\n",
    "\n",
    "## 1. Soft Prompt (Graph as Text)\n",
    "- Graph → Text serialization\n",
    "- Added to LLM's context window\n",
    "- LLM processes structure through text attention\n",
    "\n",
    "## 2. Hard Prompt (GNN Encoding)\n",
    "- Graph → GNN → Embedding\n",
    "- Structure explicitly encoded\n",
    "- Can be injected into LLM hidden states\n",
    "\n",
    "### Key Differences\n",
    "| Aspect | Soft Prompt | Hard Prompt |\n",
    "|--------|-------------|-------------|\n",
    "| Structure | Implicit (text) | Explicit (GNN) |\n",
    "| Context Length | O(nodes + edges) | O(1) virtual tokens |\n",
    "| Multi-hop | LLM must infer | GNN propagates |\n",
    "| Training | Zero-shot | May need finetuning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup (Run first in Colab)\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment in Colab)\n",
    "# !pip install torch_geometric transformers accelerate sentence-transformers neo4j\n",
    "# !pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from soft_vs_hard_experiment import (\n",
    "    ExperimentConfig, \n",
    "    SoftVsHardExperiment,\n",
    "    SoftPromptFormatter,\n",
    "    GNNEncoder,\n",
    "    PromptType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    # Neo4j (update with your settings)\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_password=\"password\",\n",
    "    neo4j_database=\"finderlpg\",\n",
    "    \n",
    "    # LLM\n",
    "    llm_model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    # llm_model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # For testing\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    embedding_dim=384,\n",
    "    \n",
    "    # GNN\n",
    "    gnn_hidden_dim=256,\n",
    "    gnn_num_layers=2,\n",
    "    gnn_heads=4,\n",
    "    \n",
    "    # Retrieval\n",
    "    top_k_nodes=20,\n",
    "    max_hops=2,\n",
    "    \n",
    "    # Memory optimization\n",
    "    use_4bit=True,  # Enable for Colab T4/V100\n",
    ")\n",
    "\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = SoftVsHardExperiment(config)\n",
    "exp.setup([\"neo4j\", \"embeddings\", \"llm\", \"gnn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions\n",
    "questions_df = exp.data_loader.load_questions(limit=50)\n",
    "print(f\"Loaded {len(questions_df)} questions\")\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Prompt Formatting Examples\n",
    "\n",
    "Let's see how graph data looks in different text formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample subgraph\n",
    "sample_id = questions_df.iloc[0]['id']\n",
    "sample_subgraph = exp.data_loader.get_subgraph(sample_id, max_hops=2)\n",
    "\n",
    "print(f\"Subgraph: {len(sample_subgraph['nodes'])} nodes, {len(sample_subgraph['edges'])} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 1: Structured\n",
    "print(\"=\" * 60)\n",
    "print(\"FORMAT: STRUCTURED\")\n",
    "print(\"=\" * 60)\n",
    "structured = SoftPromptFormatter.format_structured(\n",
    "    sample_subgraph['nodes'][:10], \n",
    "    sample_subgraph['edges'][:15],\n",
    "    include_props=True\n",
    ")\n",
    "print(structured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 2: Natural Language\n",
    "print(\"=\" * 60)\n",
    "print(\"FORMAT: NATURAL LANGUAGE\")\n",
    "print(\"=\" * 60)\n",
    "natural = SoftPromptFormatter.format_natural(\n",
    "    sample_subgraph['nodes'][:10], \n",
    "    sample_subgraph['edges'][:10]\n",
    ")\n",
    "print(natural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 3: Triples\n",
    "print(\"=\" * 60)\n",
    "print(\"FORMAT: TRIPLES\")\n",
    "print(\"=\" * 60)\n",
    "triples = SoftPromptFormatter.format_triples(\n",
    "    sample_subgraph['nodes'][:10], \n",
    "    sample_subgraph['edges'][:15]\n",
    ")\n",
    "print(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOKEN COUNT COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, text in [(\"Structured\", structured), (\"Natural\", natural), (\"Triples\", triples)]:\n",
    "    tokens = len(exp.tokenizer.encode(text))\n",
    "    chars = len(text)\n",
    "    print(f\"{name}: {tokens} tokens, {chars} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Single Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a question\n",
    "idx = 0\n",
    "row = questions_df.iloc[idx]\n",
    "print(f\"Question: {row['text']}\")\n",
    "print(f\"Answer: {row['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "result = exp.run_comparison(\n",
    "    question_id=row['id'],\n",
    "    question=row['text'],\n",
    "    ground_truth=row['answer']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nQuestion: {result['question']}\")\n",
    "print(f\"Ground Truth: {result['ground_truth']}\")\n",
    "print(f\"Subgraph: {result['subgraph_nodes']} nodes, {result['subgraph_edges']} edges\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"[1] LLM ONLY (No Context)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Response: {result['llm_only_response']}\")\n",
    "print(f\"Tokens: {result['llm_only_meta']['input_tokens']} in, {result['llm_only_meta']['output_tokens']} out\")\n",
    "print(f\"Time: {result['llm_only_meta']['generation_time']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"[2] SOFT PROMPT (Graph as Text)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Response: {result['soft_prompt_response']}\")\n",
    "print(f\"Tokens: {result['soft_prompt_meta']['input_tokens']} in, {result['soft_prompt_meta']['output_tokens']} out\")\n",
    "print(f\"Context: {result['soft_prompt_meta']['context_length']} chars\")\n",
    "print(f\"Time: {result['soft_prompt_meta']['generation_time']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"[3] HARD PROMPT (GNN Encoding)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Response: {result['hard_prompt_response']}\")\n",
    "print(f\"Tokens: {result['hard_prompt_meta']['input_tokens']} in, {result['hard_prompt_meta']['output_tokens']} out\")\n",
    "print(f\"GNN time: {result['hard_prompt_meta'].get('gnn_time', 0):.4f}s\")\n",
    "print(f\"Graph embedding norm: {result['hard_prompt_meta'].get('graph_emb_norm', 0):.4f}\")\n",
    "print(f\"Total time: {result['hard_prompt_meta']['generation_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on multiple questions\n",
    "results_df = exp.run_experiment(questions_df, sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "exp.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "metrics = {\n",
    "    'Method': [],\n",
    "    'Input Tokens': [],\n",
    "    'Generation Time': [],\n",
    "}\n",
    "\n",
    "for r in exp.results:\n",
    "    metrics['Method'].append('LLM Only')\n",
    "    metrics['Input Tokens'].append(r['llm_only_meta']['input_tokens'])\n",
    "    metrics['Generation Time'].append(r['llm_only_meta']['generation_time'])\n",
    "    \n",
    "    metrics['Method'].append('Soft Prompt')\n",
    "    metrics['Input Tokens'].append(r['soft_prompt_meta']['input_tokens'])\n",
    "    metrics['Generation Time'].append(r['soft_prompt_meta']['generation_time'])\n",
    "    \n",
    "    metrics['Method'].append('Hard Prompt')\n",
    "    metrics['Input Tokens'].append(r['hard_prompt_meta']['input_tokens'])\n",
    "    metrics['Generation Time'].append(r['hard_prompt_meta']['generation_time'])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Token comparison\n",
    "sns.boxplot(data=metrics_df, x='Method', y='Input Tokens', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Input Token Count by Method')\n",
    "axes[0].set_ylabel('Tokens')\n",
    "\n",
    "# Time comparison\n",
    "sns.boxplot(data=metrics_df, x='Method', y='Generation Time', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Generation Time by Method')\n",
    "axes[1].set_ylabel('Seconds')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('soft_vs_hard_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token reduction analysis\n",
    "soft_tokens = [r['soft_prompt_meta']['input_tokens'] for r in exp.results]\n",
    "hard_tokens = [r['hard_prompt_meta']['input_tokens'] for r in exp.results]\n",
    "\n",
    "reduction = [(s - h) / s * 100 for s, h in zip(soft_tokens, hard_tokens)]\n",
    "\n",
    "print(f\"Token Reduction (Soft → Hard):\")\n",
    "print(f\"  Mean: {np.mean(reduction):.1f}%\")\n",
    "print(f\"  Min:  {np.min(reduction):.1f}%\")\n",
    "print(f\"  Max:  {np.max(reduction):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: When does each method work better?\n",
    "\n",
    "### Soft Prompt Strengths:\n",
    "- Zero-shot (no training needed)\n",
    "- Interpretable context\n",
    "- Works with any LLM\n",
    "- Good for small graphs\n",
    "\n",
    "### Hard Prompt (GNN) Strengths:\n",
    "- Fixed context length regardless of graph size\n",
    "- Explicitly encodes structure (multi-hop paths)\n",
    "- More efficient for large graphs\n",
    "- Better captures graph topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by subgraph size\n",
    "subgraph_sizes = [r['subgraph_nodes'] for r in exp.results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.scatter(subgraph_sizes, soft_tokens, label='Soft Prompt', alpha=0.7, s=100)\n",
    "ax.scatter(subgraph_sizes, hard_tokens, label='Hard Prompt', alpha=0.7, s=100)\n",
    "\n",
    "ax.set_xlabel('Subgraph Size (nodes)')\n",
    "ax.set_ylabel('Input Tokens')\n",
    "ax.set_title('Token Usage vs Subgraph Size')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tokens_vs_subgraph_size.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "exp.save_results('soft_vs_hard_results.json')\n",
    "results_df.to_csv('soft_vs_hard_results.csv', index=False)\n",
    "print(\"Results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "exp.cleanup()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Quality Evaluation**: Compare answer accuracy using metrics (F1, exact match)\n",
    "2. **GNN Training**: Fine-tune GNN on your dataset\n",
    "3. **Full Hard Prompt Integration**: Inject GNN embeddings directly into LLM hidden states\n",
    "4. **Ablation Studies**: Test different:\n",
    "   - Soft prompt formats\n",
    "   - GNN architectures (GCN, GAT, GraphSAGE)\n",
    "   - Number of virtual tokens for hard prompt\n",
    "   - Subgraph pruning strategies (PCST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
