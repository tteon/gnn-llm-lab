{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-Retrieval Style Comparison: LPG (GAT) vs RDF (TransE / DistMult)\n",
    "\n",
    "This notebook trains graph encoders on the FinDER dual-graph PyG dataset and evaluates\n",
    "how well learned graph embeddings capture answer-relevant information.\n",
    "\n",
    "**Models:**\n",
    "- **GAT** (Graph Attention Network) on LPG subgraphs — 384d node features + message passing\n",
    "- **TransE** on RDF triples — translation-based: h + r ≈ t (asymmetric)\n",
    "- **DistMult** on RDF triples — bilinear: h · diag(r) · t (symmetric)\n",
    "\n",
    "**Evaluation:**\n",
    "- Link prediction (self-supervised training objective)\n",
    "- Graph→Answer retrieval (cosine similarity with sentence embeddings)\n",
    "- Category-wise and graph-size breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup & Install\n# Uncomment for Colab:\n# !pip install torch torch-geometric sentence-transformers rouge-score\n# !pip install torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n\nimport sys, os, json, time, warnings\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 11, 'figure.dpi': 120})\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom torch_geometric.nn import GATConv, global_mean_pool\nfrom torch_geometric.nn.kge import TransE, DistMult\nfrom torch_geometric.utils import negative_sampling, scatter\n\n# Project imports — adjust path for Colab\nPROJECT_ROOT = Path('..').resolve()\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom src.data import FinDERGraphQADataset, DualGraphBatch, dual_graph_collate_fn, VocabularyBuilder\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif device.type == 'cuda':\n    print(f'  GPU: {torch.cuda.get_device_name()}')\n    print(f'  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading\n",
    "DATA_ROOT = PROJECT_ROOT / 'data' / 'processed' / 'finder_pyg'\n",
    "\n",
    "train_ds = FinDERGraphQADataset(root=str(DATA_ROOT), split='train')\n",
    "val_ds   = FinDERGraphQADataset(root=str(DATA_ROOT), split='val')\n",
    "test_ds  = FinDERGraphQADataset(root=str(DATA_ROOT), split='test')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=dual_graph_collate_fn, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=dual_graph_collate_fn, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=dual_graph_collate_fn, num_workers=0)\n",
    "\n",
    "# Load vocabularies\n",
    "vocabs = FinDERGraphQADataset.get_vocab(root=str(DATA_ROOT))\n",
    "metadata = json.loads((DATA_ROOT / 'processed' / 'metadata.json').read_text())\n",
    "\n",
    "NUM_RDF_ENTITIES  = metadata['vocab_sizes']['rdf_entities']   # 17,534\n",
    "NUM_RDF_RELATIONS = metadata['vocab_sizes']['rdf_relations']  # 4,340\n",
    "LPG_FEATURE_DIM   = metadata['lpg_feature_dim']               # 384\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset Statistics\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Train: {len(train_ds):,} | Val: {len(val_ds):,} | Test: {len(test_ds):,}\")\n",
    "print(f\"  LPG feature dim: {LPG_FEATURE_DIM}\")\n",
    "print(f\"  RDF entities: {NUM_RDF_ENTITIES:,} | relations: {NUM_RDF_RELATIONS:,}\")\n",
    "\n",
    "# Category distribution\n",
    "categories = [train_ds[i].category for i in range(len(train_ds))]\n",
    "cat_counts = pd.Series(categories).value_counts()\n",
    "print(f\"\\nCategory distribution (train):\")\n",
    "for cat, count in cat_counts.items():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Average graph sizes\n",
    "lpg_nodes = [train_ds[i].lpg_num_nodes.item() for i in range(min(200, len(train_ds)))]\n",
    "rdf_edges = [train_ds[i].rdf_edge_index.shape[1] for i in range(min(200, len(train_ds)))]\n",
    "print(f\"\\nAvg LPG nodes/sample: {np.mean(lpg_nodes):.1f} (±{np.std(lpg_nodes):.1f})\")\n",
    "print(f\"Avg RDF triples/sample: {np.mean(rdf_edges):.1f} (±{np.std(rdf_edges):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Model Definitions\n\n# --- GAT for LPG (batched) ---\n\nclass BatchedGAT(nn.Module):\n    \"\"\"GAT encoder for LPG subgraphs with batched graph-level pooling.\n\n    Based on MessagePassingGNN from src/_legacy/models.py but uses\n    global_mean_pool for proper mini-batch support.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 384,\n        hidden_dim: int = 256,\n        output_dim: int = 384,\n        num_layers: int = 2,\n        heads: int = 4,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.input_proj = nn.Linear(input_dim, hidden_dim)\n        self.convs = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        for i in range(num_layers):\n            in_ch = hidden_dim if i == 0 else hidden_dim * heads\n            self.convs.append(GATConv(in_ch, hidden_dim, heads=heads, dropout=dropout))\n            self.norms.append(nn.LayerNorm(hidden_dim * heads))\n        self.output_proj = nn.Linear(hidden_dim * heads, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, edge_index, batch):\n        \"\"\"Forward pass with batched graph-level pooling.\n\n        Args:\n            x: [sum(N_i), input_dim] node features\n            edge_index: [2, sum(E_i)] COO edges\n            batch: [sum(N_i)] graph membership index\n\n        Returns:\n            [B, output_dim] graph-level embeddings\n        \"\"\"\n        x = torch.relu(self.input_proj(x))\n        for conv, norm in zip(self.convs, self.norms):\n            x = conv(x, edge_index)\n            x = norm(x)\n            x = torch.relu(x)\n            x = self.dropout(x)\n        node_emb = self.output_proj(x)  # [sum(N_i), output_dim]\n        return global_mean_pool(node_emb, batch)  # [B, output_dim]\n\n    def get_node_embeddings(self, x, edge_index):\n        \"\"\"Get per-node embeddings (no pooling). For link prediction decoding.\"\"\"\n        x = torch.relu(self.input_proj(x))\n        for conv, norm in zip(self.convs, self.norms):\n            x = conv(x, edge_index)\n            x = norm(x)\n            x = torch.relu(x)\n            x = self.dropout(x)\n        return self.output_proj(x)  # [sum(N_i), output_dim]\n\n\n# --- KGE for RDF (TransE / DistMult) with per-graph aggregation ---\n\nclass BatchedKGE(nn.Module):\n    \"\"\"KGE encoder for RDF triples with per-graph embedding aggregation.\n\n    Learns global entity and relation embeddings, then for each question's\n    RDF subgraph, aggregates triple representations via scatter mean.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,  # 'transe' or 'distmult'\n        num_entities: int,\n        num_relations: int,\n        hidden_dim: int = 256,\n        output_dim: int = 384,\n    ):\n        super().__init__()\n        self.model_type = model_type\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        if model_type == 'transe':\n            self.kge = TransE(\n                num_nodes=num_entities,\n                num_relations=num_relations,\n                hidden_channels=hidden_dim,\n                margin=1.0,\n                p_norm=1.0,\n            )\n        elif model_type == 'distmult':\n            self.kge = DistMult(\n                num_nodes=num_entities,\n                num_relations=num_relations,\n                hidden_channels=hidden_dim,\n            )\n        else:\n            raise ValueError(f\"Unknown model_type: {model_type}\")\n\n        self.output_proj = nn.Linear(hidden_dim, output_dim)\n\n    def loss(self, head_index, rel_type, tail_index):\n        \"\"\"KGE training loss with built-in negative sampling.\"\"\"\n        return self.kge.loss(head_index, rel_type, tail_index)\n\n    def forward(self, batch: 'DualGraphBatch'):\n        \"\"\"Compute per-graph embeddings from RDF triples.\n\n        For each triple (h, r, t) in the batch, computes h_emb + r_emb,\n        then aggregates per-graph via scatter mean on the head node's\n        graph membership.\n\n        Returns:\n            [B, output_dim] graph-level embeddings\n        \"\"\"\n        edge_index = batch.rdf_edge_index  # [2, sum(T_i)]\n        edge_type = batch.rdf_edge_type    # [sum(T_i)]\n        rdf_batch = batch.rdf_batch        # [sum(N_rdf_i)]\n        global_idx = batch.rdf_global_node_idx  # [sum(N_rdf_i)]\n\n        if edge_index.shape[1] == 0:\n            return torch.zeros(batch.batch_size, self.output_dim, device=edge_index.device)\n\n        head_local = edge_index[0]  # local node indices\n        tail_local = edge_index[1]\n\n        # Map local indices to global for embedding lookup\n        head_global = global_idx[head_local]\n        tail_global = global_idx[tail_local]\n\n        head_emb = self.kge.node_emb(head_global)  # [sum(T_i), hidden_dim]\n        rel_emb = self.kge.rel_emb(edge_type)       # [sum(T_i), hidden_dim]\n\n        # Triple representation: h + r (translation-style, works for both)\n        triple_emb = head_emb + rel_emb  # [sum(T_i), hidden_dim]\n\n        # Determine graph membership for each triple (from head node)\n        triple_graph = rdf_batch[head_local]  # [sum(T_i)]\n\n        # Aggregate triples per graph (using PyG's scatter with reduce='mean')\n        graph_emb = scatter(triple_emb, triple_graph, dim=0,\n                            dim_size=batch.batch_size, reduce='mean')  # [B, hidden_dim]\n\n        return self.output_proj(graph_emb)  # [B, output_dim]\n\n    def get_entity_embeddings(self):\n        \"\"\"Export projected entity embeddings [num_entities, output_dim].\"\"\"\n        with torch.no_grad():\n            return self.output_proj(self.kge.node_emb.weight)\n\n\n# Quick sanity check\nprint('BatchedGAT params:', sum(p.numel() for p in BatchedGAT().parameters()) / 1e3, 'K')\nprint('BatchedKGE (TransE) params:',\n      sum(p.numel() for p in BatchedKGE('transe', NUM_RDF_ENTITIES, NUM_RDF_RELATIONS).parameters()) / 1e6, 'M')\nprint('BatchedKGE (DistMult) params:',\n      sum(p.numel() for p in BatchedKGE('distmult', NUM_RDF_ENTITIES, NUM_RDF_RELATIONS).parameters()) / 1e6, 'M')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Training — GAT (LPG) via Link Prediction\n",
    "\n",
    "def train_gat_epoch(model, loader, optimizer):\n",
    "    \"\"\"Train GAT via link prediction with negative sampling.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get node embeddings (no pooling)\n",
    "        z = model.get_node_embeddings(batch.lpg_x, batch.lpg_edge_index)\n",
    "\n",
    "        # Positive edges\n",
    "        pos_edge = batch.lpg_edge_index\n",
    "        num_nodes = batch.lpg_x.shape[0]\n",
    "\n",
    "        if pos_edge.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        # Negative sampling\n",
    "        neg_edge = negative_sampling(\n",
    "            pos_edge, num_nodes=num_nodes,\n",
    "            num_neg_samples=pos_edge.shape[1],\n",
    "        )\n",
    "\n",
    "        # Score positive and negative edges via dot product\n",
    "        pos_score = (z[pos_edge[0]] * z[pos_edge[1]]).sum(dim=-1)\n",
    "        neg_score = (z[neg_edge[0]] * z[neg_edge[1]]).sum(dim=-1)\n",
    "\n",
    "        # BCE loss\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(pos_score, torch.ones_like(pos_score))\n",
    "        neg_loss = F.binary_cross_entropy_with_logits(neg_score, torch.zeros_like(neg_score))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_gat_link_prediction(model, loader):\n",
    "    \"\"\"Evaluate GAT link prediction: MRR and Hits@10.\"\"\"\n",
    "    model.eval()\n",
    "    mrr_sum, hits10_sum, count = 0.0, 0.0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        z = model.get_node_embeddings(batch.lpg_x, batch.lpg_edge_index)\n",
    "\n",
    "        pos_edge = batch.lpg_edge_index\n",
    "        if pos_edge.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        num_nodes = z.shape[0]\n",
    "        # Score a sample of edges (full ranking is too slow)\n",
    "        sample_size = min(500, pos_edge.shape[1])\n",
    "        idx = torch.randperm(pos_edge.shape[1])[:sample_size]\n",
    "        src, dst = pos_edge[0, idx], pos_edge[1, idx]\n",
    "\n",
    "        for s, d in zip(src, dst):\n",
    "            # Score true tail vs all nodes\n",
    "            scores = (z[s].unsqueeze(0) * z).sum(dim=-1)  # [num_nodes]\n",
    "            rank = (scores >= scores[d]).sum().item()\n",
    "            mrr_sum += 1.0 / rank\n",
    "            hits10_sum += 1.0 if rank <= 10 else 0.0\n",
    "            count += 1\n",
    "\n",
    "    mrr = mrr_sum / max(count, 1)\n",
    "    hits10 = hits10_sum / max(count, 1)\n",
    "    return {'mrr': mrr, 'hits@10': hits10}\n",
    "\n",
    "\n",
    "# Train GAT\n",
    "GAT_EPOCHS = 50\n",
    "GAT_LR = 1e-3\n",
    "\n",
    "gat_model = BatchedGAT(input_dim=LPG_FEATURE_DIM).to(device)\n",
    "gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=GAT_LR, weight_decay=1e-5)\n",
    "\n",
    "gat_history = {'train_loss': [], 'val_mrr': [], 'val_hits10': []}\n",
    "best_val_mrr = 0.0\n",
    "best_gat_state = None\n",
    "\n",
    "print(f'Training GAT for {GAT_EPOCHS} epochs...')\n",
    "for epoch in range(1, GAT_EPOCHS + 1):\n",
    "    loss = train_gat_epoch(gat_model, train_loader, gat_optimizer)\n",
    "    gat_history['train_loss'].append(loss)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        val_metrics = eval_gat_link_prediction(gat_model, val_loader)\n",
    "        gat_history['val_mrr'].append(val_metrics['mrr'])\n",
    "        gat_history['val_hits10'].append(val_metrics['hits@10'])\n",
    "        print(f'  Epoch {epoch:3d} | Loss: {loss:.4f} | Val MRR: {val_metrics[\"mrr\"]:.4f} | Val Hits@10: {val_metrics[\"hits@10\"]:.3f}')\n",
    "\n",
    "        if val_metrics['mrr'] > best_val_mrr:\n",
    "            best_val_mrr = val_metrics['mrr']\n",
    "            best_gat_state = {k: v.cpu().clone() for k, v in gat_model.state_dict().items()}\n",
    "\n",
    "# Restore best model\n",
    "if best_gat_state:\n",
    "    gat_model.load_state_dict(best_gat_state)\n",
    "    gat_model.to(device)\n",
    "print(f'\\nBest GAT Val MRR: {best_val_mrr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training — TransE & DistMult (RDF)\n",
    "\n",
    "def collect_all_rdf_triples(loader):\n",
    "    \"\"\"Collect all (head_global, rel, tail_global) triples from the dataset.\"\"\"\n",
    "    heads, rels, tails = [], [], []\n",
    "    for batch in loader:\n",
    "        ei = batch.rdf_edge_index\n",
    "        et = batch.rdf_edge_type\n",
    "        gi = batch.rdf_global_node_idx\n",
    "        if ei.shape[1] == 0:\n",
    "            continue\n",
    "        heads.append(gi[ei[0]])\n",
    "        tails.append(gi[ei[1]])\n",
    "        rels.append(et)\n",
    "    return torch.cat(heads), torch.cat(rels), torch.cat(tails)\n",
    "\n",
    "\n",
    "def train_kge_epoch(model, head, rel, tail, optimizer, batch_size=512):\n",
    "    \"\"\"Train KGE model for one epoch over all triples.\"\"\"\n",
    "    model.train()\n",
    "    perm = torch.randperm(head.shape[0], device=head.device)\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i in range(0, head.shape[0], batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head[idx], rel[idx], tail[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_kge(model, head, rel, tail, sample_size=1000, k=10):\n",
    "    \"\"\"Evaluate KGE with sampled ranking: MRR and Hits@K.\"\"\"\n",
    "    model.eval()\n",
    "    n = min(sample_size, head.shape[0])\n",
    "    idx = torch.randperm(head.shape[0])[:n]\n",
    "    h, r, t = head[idx], rel[idx], tail[idx]\n",
    "\n",
    "    node_emb = model.kge.node_emb.weight  # [num_entities, hidden_dim]\n",
    "    rel_emb = model.kge.rel_emb(r)        # [n, hidden_dim]\n",
    "    h_emb = model.kge.node_emb(h)          # [n, hidden_dim]\n",
    "\n",
    "    mrr_sum, hits_sum = 0.0, 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        if model.model_type == 'transe':\n",
    "            # score = -||h + r - t||  (higher = better)\n",
    "            pred = h_emb[i] + rel_emb[i]  # [hidden_dim]\n",
    "            scores = -torch.norm(node_emb - pred.unsqueeze(0), p=1, dim=-1)  # [num_entities]\n",
    "        else:  # distmult\n",
    "            # score = sum(h * r * t)\n",
    "            pred = h_emb[i] * rel_emb[i]  # [hidden_dim]\n",
    "            scores = (node_emb * pred.unsqueeze(0)).sum(dim=-1)  # [num_entities]\n",
    "\n",
    "        rank = (scores >= scores[t[i]]).sum().item()\n",
    "        mrr_sum += 1.0 / max(rank, 1)\n",
    "        hits_sum += 1.0 if rank <= k else 0.0\n",
    "\n",
    "    return {'mrr': mrr_sum / n, f'hits@{k}': hits_sum / n}\n",
    "\n",
    "\n",
    "# Collect triples\n",
    "print('Collecting RDF triples...')\n",
    "train_h, train_r, train_t = collect_all_rdf_triples(train_loader)\n",
    "val_h, val_r, val_t = collect_all_rdf_triples(val_loader)\n",
    "train_h, train_r, train_t = train_h.to(device), train_r.to(device), train_t.to(device)\n",
    "val_h, val_r, val_t = val_h.to(device), val_r.to(device), val_t.to(device)\n",
    "print(f'  Train triples: {train_h.shape[0]:,} | Val triples: {val_h.shape[0]:,}')\n",
    "\n",
    "# Train both models\n",
    "KGE_EPOCHS = 100\n",
    "KGE_LR = 1e-2\n",
    "KGE_BATCH = 512\n",
    "\n",
    "kge_models = {}\n",
    "kge_histories = {}\n",
    "\n",
    "for model_type in ['transe', 'distmult']:\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Training {model_type.upper()} for {KGE_EPOCHS} epochs...')\n",
    "    print(f'{\"=\"*50}')\n",
    "\n",
    "    model = BatchedKGE(model_type, NUM_RDF_ENTITIES, NUM_RDF_RELATIONS).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=KGE_LR)\n",
    "    history = {'train_loss': [], 'val_mrr': [], 'val_hits10': []}\n",
    "    best_mrr = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, KGE_EPOCHS + 1):\n",
    "        loss = train_kge_epoch(model, train_h, train_r, train_t, optimizer, KGE_BATCH)\n",
    "        history['train_loss'].append(loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            val_metrics = eval_kge(model, val_h, val_r, val_t)\n",
    "            history['val_mrr'].append(val_metrics['mrr'])\n",
    "            history['val_hits10'].append(val_metrics['hits@10'])\n",
    "            print(f'  Epoch {epoch:3d} | Loss: {loss:.4f} | Val MRR: {val_metrics[\"mrr\"]:.4f} | Val Hits@10: {val_metrics[\"hits@10\"]:.3f}')\n",
    "\n",
    "            if val_metrics['mrr'] > best_mrr:\n",
    "                best_mrr = val_metrics['mrr']\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "        model.to(device)\n",
    "    print(f'Best {model_type.upper()} Val MRR: {best_mrr:.4f}')\n",
    "\n",
    "    kge_models[model_type] = model\n",
    "    kge_histories[model_type] = history\n",
    "\n",
    "# Loss curve comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for mt, h in kge_histories.items():\n",
    "    axes[0].plot(h['train_loss'], label=mt.upper())\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('KGE Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Also plot GAT loss\n",
    "axes[1].plot(gat_history['train_loss'], label='GAT', color='tab:green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('GAT Training Loss')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Track B: G-Retrieval E2E Pipeline + Bottleneck Profiling\n\n**G-Retrieval Architecture:** Graph Encoder → Projection → LLM (Llama 3 via HuggingFace)\n\nProfile the **full inference pipeline** stage-by-stage:\n\n| Stage | LPG (GAT) | RDF (TransE / DistMult) |\n|-------|-----------|------------------------|\n| **SUBGRAPH_LOAD** | Extract tensors from batch | Extract edge_index, edge_type, global_idx |\n| **CPU_GATHER / LOOKUP** | Feature store indexing on CPU | Embedding table lookup on CPU |\n| **H2D_COPY** | Synchronous transfer to GPU | Synchronous transfer to GPU |\n| **ENCODER_FWD** | GAT layers + global_mean_pool | h+r aggregation + scatter + proj |\n| **PROJECTION** | Linear(384 → LLM hidden) | Linear(384 → LLM hidden) |\n| **LLM_PREFILL** | LLM forward with graph soft tokens prepended | Same |\n\nSimulates a **CPU-resident feature/embedding store** (G-Retrieval production pattern).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Profiling Utilities + CPU Store Simulators\n\nimport time\nfrom torch.profiler import profile as torch_profile, ProfilerActivity, record_function\n\n# ── Constants ──\nWARMUP_BATCHES = 3\nPROFILE_BATCHES = 20\nPROFILE_BATCH_SIZE = 32\n\n# ── Timing Infrastructure ──\n\n@dataclass\nclass StageTiming:\n    \"\"\"Timing result for a single profiling stage.\"\"\"\n    stage: str\n    cpu_ms: float\n    cuda_ms: float = float('nan')  # NaN for CPU-only stages\n    bytes_transferred: int = 0\n\nHAS_CUDA = device.type == 'cuda'\n\n\nclass CUDATimer:\n    \"\"\"Context manager: measures both CPU wall-clock and CUDA kernel time.\"\"\"\n    def __init__(self, stage: str):\n        self.stage = stage\n        if HAS_CUDA:\n            self.start_evt = torch.cuda.Event(enable_timing=True)\n            self.end_evt = torch.cuda.Event(enable_timing=True)\n\n    def __enter__(self):\n        if HAS_CUDA:\n            torch.cuda.synchronize()\n        self.cpu_start = time.perf_counter()\n        if HAS_CUDA:\n            self.start_evt.record()\n        return self\n\n    def __exit__(self, *args):\n        if HAS_CUDA:\n            self.end_evt.record()\n            torch.cuda.synchronize()\n        self.cpu_end = time.perf_counter()\n        cpu_ms = (self.cpu_end - self.cpu_start) * 1000\n        cuda_ms = self.start_evt.elapsed_time(self.end_evt) if HAS_CUDA else float('nan')\n        self._result = StageTiming(stage=self.stage, cpu_ms=cpu_ms, cuda_ms=cuda_ms)\n\n    def result(self) -> StageTiming:\n        return self._result\n\n\nclass CPUTimer:\n    \"\"\"Context manager: measures CPU wall-clock only (no GPU sync).\"\"\"\n    def __init__(self, stage: str):\n        self.stage = stage\n\n    def __enter__(self):\n        self.cpu_start = time.perf_counter()\n        return self\n\n    def __exit__(self, *args):\n        self.cpu_end = time.perf_counter()\n        cpu_ms = (self.cpu_end - self.cpu_start) * 1000\n        self._result = StageTiming(stage=self.stage, cpu_ms=cpu_ms)\n\n    def result(self) -> StageTiming:\n        return self._result\n\n\n# ── CPU Store Simulators (G-Retrieval production pattern) ──\n\nclass LPGCPUStore:\n    \"\"\"CPU-resident node feature store for LPG.\n\n    Simulates the G-Retrieval pattern where graph node features\n    come from a CPU feature database, not pre-loaded on GPU.\n    \"\"\"\n    def __init__(self, global_node_features: torch.Tensor):\n        self.features = global_node_features.cpu().clone()  # [13920, 384]\n        print(f'  LPGCPUStore: {self.features.shape}, '\n              f'{self.features.nelement() * self.features.element_size() / 1e6:.1f} MB on CPU')\n\n    def gather(self, global_node_idx: torch.LongTensor) -> torch.Tensor:\n        return self.features[global_node_idx.cpu()]\n\n\nclass RDFCPUStore:\n    \"\"\"CPU-resident KGE embedding tables for RDF.\n\n    Moves trained entity/relation embeddings to CPU to simulate\n    the production pattern where embedding tables are too large for GPU.\n    \"\"\"\n    def __init__(self, kge_model: 'BatchedKGE'):\n        self.node_emb_weight = kge_model.kge.node_emb.weight.detach().cpu().clone()\n        self.rel_emb_weight = kge_model.kge.rel_emb.weight.detach().cpu().clone()\n        node_mb = self.node_emb_weight.nelement() * self.node_emb_weight.element_size() / 1e6\n        rel_mb = self.rel_emb_weight.nelement() * self.rel_emb_weight.element_size() / 1e6\n        print(f'  RDFCPUStore: node_emb {self.node_emb_weight.shape} ({node_mb:.1f} MB) + '\n              f'rel_emb {self.rel_emb_weight.shape} ({rel_mb:.1f} MB) on CPU')\n\n    def lookup(self, head_global: torch.LongTensor,\n               edge_type: torch.LongTensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        h_emb = self.node_emb_weight[head_global.cpu()]\n        r_emb = self.rel_emb_weight[edge_type.cpu()]\n        return h_emb, r_emb\n\n\n# ── Result Aggregation ──\n\ndef aggregate_timings(all_timings: List[Dict[str, StageTiming]],\n                      batch_sizes: List[int],\n                      node_counts: List[int],\n                      edge_counts: List[int]) -> pd.DataFrame:\n    \"\"\"Aggregate per-batch timings into summary statistics.\"\"\"\n    stages = list(all_timings[0].keys())\n    rows = []\n    for stage in stages:\n        cpu_vals = [t[stage].cpu_ms for t in all_timings]\n        cuda_vals = [t[stage].cuda_ms for t in all_timings]\n        bytes_vals = [t[stage].bytes_transferred for t in all_timings]\n        row = {\n            'stage': stage,\n            'cpu_mean': np.mean(cpu_vals),\n            'cpu_std': np.std(cpu_vals),\n            'cpu_p95': np.percentile(cpu_vals, 95),\n            'cuda_mean': np.nanmean(cuda_vals) if not all(np.isnan(cuda_vals)) else float('nan'),\n            'cuda_std': np.nanstd(cuda_vals) if not all(np.isnan(cuda_vals)) else float('nan'),\n            'cuda_p95': np.nanpercentile(cuda_vals, 95) if not all(np.isnan(cuda_vals)) else float('nan'),\n            'bytes_mean': np.mean(bytes_vals),\n        }\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\ndef print_profiling_table(df: pd.DataFrame, model_name: str, n_batches: int):\n    \"\"\"Pretty-print a profiling summary table.\"\"\"\n    total_cpu = df['cpu_mean'].sum()\n    print(f'\\n{\"=\"*65}')\n    print(f'{model_name} Profiling — {n_batches} batches, BS={PROFILE_BATCH_SIZE}')\n    print(f'{\"=\"*65}')\n    print(f'{\"Stage\":<18} {\"CPU ms\":>10} {\"CUDA ms\":>10} {\"% total\":>8} {\"H2D MB/s\":>10}')\n    print(f'{\"-\"*65}')\n    for _, r in df.iterrows():\n        pct = r['cpu_mean'] / total_cpu * 100 if total_cpu > 0 else 0\n        cuda_str = f'{r[\"cuda_mean\"]:.2f}' if not np.isnan(r['cuda_mean']) else '—'\n        bw_str = ''\n        if r['bytes_mean'] > 0 and not np.isnan(r['cuda_mean']) and r['cuda_mean'] > 0:\n            bw_gbps = (r['bytes_mean'] / 1e9) / (r['cuda_mean'] / 1e3)\n            bw_str = f'{bw_gbps:.1f} GB/s'\n        print(f'{r[\"stage\"]:<18} {r[\"cpu_mean\"]:>8.2f}±{r[\"cpu_std\"]:<4.1f} '\n              f'{cuda_str:>8} {pct:>7.1f}%  {bw_str}')\n    print(f'{\"-\"*65}')\n    print(f'{\"TOTAL\":<18} {total_cpu:>10.2f}')\n\n\n# ── Load Global LPG Features for CPU Store ──\nprint('Loading global LPG features for CPU store simulation...')\n_lpg_cache_path = PROJECT_ROOT / 'data' / 'processed' / 'lpg_full_graph.pt'\n\nif _lpg_cache_path.exists():\n    _lpg_cache = torch.load(str(_lpg_cache_path), weights_only=False, map_location='cpu')\n    global_node_features = _lpg_cache['data'].x  # [13920, 384]\n    del _lpg_cache\nelse:\n    # Fallback: reconstruct global node features from dataset samples\n    print('  lpg_full_graph.pt not found — reconstructing from dataset...')\n    _max_idx = 0\n    for ds in [train_ds, val_ds, test_ds]:\n        for i in range(len(ds)):\n            gi = ds[i].lpg_global_node_idx\n            if gi.numel() > 0:\n                _max_idx = max(_max_idx, gi.max().item())\n    global_node_features = torch.zeros(_max_idx + 1, LPG_FEATURE_DIM)\n    for ds in [train_ds, val_ds, test_ds]:\n        for i in range(len(ds)):\n            sample = ds[i]\n            gi = sample.lpg_global_node_idx\n            if gi.numel() > 0:\n                global_node_features[gi] = sample.lpg_x\n    print(f'  Reconstructed from {len(train_ds)+len(val_ds)+len(test_ds)} samples')\nprint(f'  Global LPG features: {global_node_features.shape}')\n\n# Create CPU stores\nprint('\\nInitializing CPU stores...')\nlpg_cpu_store = LPGCPUStore(global_node_features)\nrdf_cpu_stores = {}\nfor mt, model in kge_models.items():\n    print(f'  {mt.upper()}:')\n    rdf_cpu_stores[mt] = RDFCPUStore(model)\n\n# Profiling DataLoader (no pin_memory, no shuffle for reproducibility)\nprofile_loader = DataLoader(train_ds, batch_size=PROFILE_BATCH_SIZE, shuffle=False,\n                            collate_fn=dual_graph_collate_fn, num_workers=0,\n                            pin_memory=False)\nprint(f'\\nProfile loader: {len(train_ds)} samples, ~{len(train_ds)//PROFILE_BATCH_SIZE} batches')\nprint(f'Will use {WARMUP_BATCHES} warmup + {PROFILE_BATCHES} measurement batches')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: G-Retrieval Model Definition + Llama 3 Loading\n#\n# Architecture: Graph Encoder → Projection MLP → LLM (soft token prepend)\n# The graph encoder output is projected into the LLM's embedding space\n# and prepended as \"graph tokens\" before the text question tokens.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# ── G-Retrieval Wrapper ──\n\nclass GraphLLM(nn.Module):\n    \"\"\"G-Retrieval style model: Graph Encoder → Projection → LLM.\n\n    Encodes a graph subgraph into soft tokens and prepends them\n    to the LLM's text input embeddings for graph-conditioned generation.\n    \"\"\"\n    def __init__(self, graph_encoder, graph_output_dim: int, llm,\n                 num_graph_tokens: int = 1):\n        super().__init__()\n        self.graph_encoder = graph_encoder\n        self.llm = llm\n        self.num_graph_tokens = num_graph_tokens\n        self.llm_hidden = llm.config.hidden_size  # e.g. 4096 for Llama 3\n\n        # 2-layer projection MLP: graph_dim → llm_hidden × num_tokens\n        self.projector = nn.Sequential(\n            nn.Linear(graph_output_dim, self.llm_hidden),\n            nn.GELU(),\n            nn.Linear(self.llm_hidden, self.llm_hidden * num_graph_tokens),\n        )\n\n    def encode_graph_lpg(self, batch, dev):\n        \"\"\"Encode LPG subgraph → graph embedding [B, graph_output_dim].\"\"\"\n        return self.graph_encoder(\n            batch.lpg_x.to(dev), batch.lpg_edge_index.to(dev), batch.lpg_batch.to(dev)\n        )\n\n    def encode_graph_rdf(self, batch, dev):\n        \"\"\"Encode RDF subgraph → graph embedding [B, graph_output_dim].\"\"\"\n        batch_gpu = batch.to(dev)\n        return self.graph_encoder(batch_gpu)\n\n    def project(self, graph_emb):\n        \"\"\"Project graph embedding to LLM token space [B, T, llm_hidden].\"\"\"\n        proj = self.projector(graph_emb)  # [B, llm_hidden * T]\n        B = proj.shape[0]\n        # Cast to LLM dtype (bfloat16) to match text embeddings\n        proj = proj.to(self.llm.dtype)\n        return proj.view(B, self.num_graph_tokens, self.llm_hidden)\n\n    @torch.no_grad()\n    def forward_prefill(self, graph_tokens, input_ids, attention_mask):\n        \"\"\"Single LLM forward pass with graph soft tokens prepended.\n\n        Returns logits for next-token prediction (profiling target).\n        \"\"\"\n        # Text embeddings from LLM's own embedding layer\n        text_emb = self.llm.get_input_embeddings()(input_ids)  # [B, seq, H]\n\n        # Prepend graph tokens\n        combined = torch.cat([graph_tokens, text_emb], dim=1)  # [B, T+seq, H]\n\n        # Extend attention mask for graph tokens\n        graph_mask = torch.ones(\n            graph_tokens.shape[0], graph_tokens.shape[1],\n            device=attention_mask.device, dtype=attention_mask.dtype\n        )\n        combined_mask = torch.cat([graph_mask, attention_mask], dim=1)\n\n        # LLM forward (no gradient)\n        outputs = self.llm(inputs_embeds=combined, attention_mask=combined_mask)\n        return outputs.logits\n\n    @torch.no_grad()\n    def generate(self, graph_tokens, input_ids, attention_mask, tokenizer,\n                 max_new_tokens=128):\n        \"\"\"Generate answer conditioned on graph tokens.\"\"\"\n        text_emb = self.llm.get_input_embeddings()(input_ids)\n        combined = torch.cat([graph_tokens, text_emb], dim=1)\n        graph_mask = torch.ones(\n            graph_tokens.shape[0], graph_tokens.shape[1],\n            device=attention_mask.device, dtype=attention_mask.dtype\n        )\n        combined_mask = torch.cat([graph_mask, attention_mask], dim=1)\n\n        outputs = self.llm.generate(\n            inputs_embeds=combined,\n            attention_mask=combined_mask,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n        # Decode only the generated tokens (skip input length)\n        gen_ids = outputs[:, input_ids.shape[1] + graph_tokens.shape[1]:]\n        return tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n\n\n# ── Load Llama 3 ──\n\nLLM_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\nNUM_GRAPH_TOKENS = 1\nGRAPH_OUTPUT_DIM = 384  # output dim of both BatchedGAT and BatchedKGE\n\nprint(f'Loading LLM: {LLM_MODEL_ID}')\nprint(f'  Graph tokens: {NUM_GRAPH_TOKENS}')\n\n# 4-bit quantization for memory efficiency (T4: 16GB, A100: 40/80GB)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nllm = AutoModelForCausalLM.from_pretrained(\n    LLM_MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\nllm.eval()\nfor p in llm.parameters():\n    p.requires_grad = False\n\nllm_mem_gb = sum(p.nelement() * p.element_size() for p in llm.parameters()) / 1e9\nprint(f'  LLM loaded: {llm_mem_gb:.1f} GB, hidden_size={llm.config.hidden_size}')\n\n# ── Create GraphLLM instances ──\n\ngraph_llms = {}\n\n# GAT (LPG)\ngraph_llms['gat'] = GraphLLM(gat_model, GRAPH_OUTPUT_DIM, llm, NUM_GRAPH_TOKENS)\ngraph_llms['gat'].projector.to(device)\n\n# TransE / DistMult (RDF)\nfor mt in ['transe', 'distmult']:\n    graph_llms[mt] = GraphLLM(kge_models[mt], GRAPH_OUTPUT_DIM, llm, NUM_GRAPH_TOKENS)\n    graph_llms[mt].projector.to(device)\n\nproj_params = sum(p.numel() for p in graph_llms['gat'].projector.parameters())\nprint(f'  Projector params: {proj_params/1e3:.1f}K  ({GRAPH_OUTPUT_DIM} → {llm.config.hidden_size} × {NUM_GRAPH_TOKENS})')\nprint(f'  GraphLLM instances: {list(graph_llms.keys())}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 8: E2E Stage-wise Profiling\n#\n# Profile the full G-Retrieval inference pipeline for all 3 encoder types.\n# 6 stages: SUBGRAPH_LOAD → CPU_GATHER/LOOKUP → H2D_COPY → ENCODER_FWD → PROJECTION → LLM_PREFILL\n\nPROMPT_TEMPLATE = (\n    \"Based on the graph context, answer the following question concisely.\\n\"\n    \"Question: {question}\\nAnswer:\"\n)\n\ndef tokenize_questions(questions: List[str], max_length: int = 128):\n    \"\"\"Tokenize questions with prompt template → (input_ids, attention_mask) on device.\"\"\"\n    prompts = [PROMPT_TEMPLATE.format(question=q) for q in questions]\n    enc = tokenizer(prompts, return_tensors='pt', padding=True,\n                    truncation=True, max_length=max_length)\n    return enc['input_ids'].to(device), enc['attention_mask'].to(device)\n\n\n# ── LPG (GAT) E2E Profiling ──\n\ndef profile_lpg_e2e(gllm, lpg_store, loader, n_warmup, n_measure):\n    \"\"\"Profile full GAT → Projection → LLM pipeline.\"\"\"\n    gllm.graph_encoder.eval()\n    timings_list, batch_sizes, node_counts, edge_counts = [], [], [], []\n\n    for i, batch in enumerate(loader):\n        if i < n_warmup:\n            with torch.no_grad():\n                x = lpg_store.gather(batch.lpg_global_node_idx).to(device)\n                ei = batch.lpg_edge_index.to(device)\n                b = batch.lpg_batch.to(device)\n                g_emb = gllm.graph_encoder(x, ei, b)\n                g_tok = gllm.project(g_emb)\n                ids, mask = tokenize_questions(batch.questions)\n                _ = gllm.forward_prefill(g_tok, ids, mask)\n            continue\n        if i >= n_warmup + n_measure:\n            break\n\n        timings = {}\n\n        with CPUTimer('SUBGRAPH_LOAD') as t:\n            global_idx = batch.lpg_global_node_idx\n            edge_index = batch.lpg_edge_index\n            batch_vec = batch.lpg_batch\n        timings['SUBGRAPH_LOAD'] = t.result()\n\n        with CPUTimer('CPU_GATHER') as t:\n            x_cpu = lpg_store.gather(global_idx)\n        timings['CPU_GATHER'] = t.result()\n\n        with CUDATimer('H2D_COPY') as t:\n            x_gpu = x_cpu.to(device, non_blocking=False)\n            ei_gpu = edge_index.to(device, non_blocking=False)\n            b_gpu = batch_vec.to(device, non_blocking=False)\n        timings['H2D_COPY'] = t.result()\n        timings['H2D_COPY'].bytes_transferred = (\n            x_cpu.nelement() * x_cpu.element_size() +\n            edge_index.nelement() * edge_index.element_size() +\n            batch_vec.nelement() * batch_vec.element_size()\n        )\n\n        with torch.no_grad():\n            with CUDATimer('ENCODER_FWD') as t:\n                graph_emb = gllm.graph_encoder(x_gpu, ei_gpu, b_gpu)\n        timings['ENCODER_FWD'] = t.result()\n\n        with torch.no_grad():\n            with CUDATimer('PROJECTION') as t:\n                graph_tokens = gllm.project(graph_emb)\n        timings['PROJECTION'] = t.result()\n\n        with torch.no_grad():\n            with CUDATimer('LLM_PREFILL') as t:\n                input_ids, attn_mask = tokenize_questions(batch.questions)\n                _ = gllm.forward_prefill(graph_tokens, input_ids, attn_mask)\n        timings['LLM_PREFILL'] = t.result()\n\n        timings_list.append(timings)\n        batch_sizes.append(batch.batch_size)\n        node_counts.append(x_cpu.shape[0])\n        edge_counts.append(edge_index.shape[1])\n\n    return timings_list, batch_sizes, node_counts, edge_counts\n\n\n# ── RDF (KGE) E2E Profiling ──\n\ndef profile_rdf_e2e(gllm, rdf_store, output_proj_gpu, loader, n_warmup, n_measure):\n    \"\"\"Profile KGE → Projection → LLM pipeline with CPU-resident embeddings.\"\"\"\n    timings_list, batch_sizes, node_counts, edge_counts = [], [], [], []\n\n    for i, batch in enumerate(loader):\n        ei = batch.rdf_edge_index\n        if ei.shape[1] == 0:\n            continue\n\n        if i < n_warmup:\n            with torch.no_grad():\n                hg = batch.rdf_global_node_idx[ei[0]]\n                h, r = rdf_store.lookup(hg, batch.rdf_edge_type)\n                tg = batch.rdf_batch[ei[0]].to(device)\n                g = scatter(h.to(device) + r.to(device), tg, dim=0,\n                            dim_size=batch.batch_size, reduce='mean')\n                g_emb = output_proj_gpu(g)\n                g_tok = gllm.project(g_emb)\n                ids, mask = tokenize_questions(batch.questions)\n                _ = gllm.forward_prefill(g_tok, ids, mask)\n            continue\n        if i >= n_warmup + n_measure:\n            break\n\n        timings = {}\n\n        with CPUTimer('SUBGRAPH_LOAD') as t:\n            head_local = ei[0]\n            global_idx = batch.rdf_global_node_idx\n            head_global = global_idx[head_local]\n            edge_type = batch.rdf_edge_type\n            rdf_batch = batch.rdf_batch\n        timings['SUBGRAPH_LOAD'] = t.result()\n\n        with CPUTimer('CPU_LOOKUP') as t:\n            h_emb_cpu, r_emb_cpu = rdf_store.lookup(head_global, edge_type)\n        timings['CPU_LOOKUP'] = t.result()\n\n        with CUDATimer('H2D_COPY') as t:\n            h_emb_gpu = h_emb_cpu.to(device, non_blocking=False)\n            r_emb_gpu = r_emb_cpu.to(device, non_blocking=False)\n            triple_graph = rdf_batch[head_local].to(device, non_blocking=False)\n        timings['H2D_COPY'] = t.result()\n        timings['H2D_COPY'].bytes_transferred = (\n            h_emb_cpu.nelement() * h_emb_cpu.element_size() +\n            r_emb_cpu.nelement() * r_emb_cpu.element_size() +\n            head_local.nelement() * 8\n        )\n\n        with torch.no_grad():\n            with CUDATimer('ENCODER_FWD') as t:\n                triple_emb = h_emb_gpu + r_emb_gpu\n                graph_emb_raw = scatter(triple_emb, triple_graph, dim=0,\n                                        dim_size=batch.batch_size, reduce='mean')\n                graph_emb = output_proj_gpu(graph_emb_raw)\n        timings['ENCODER_FWD'] = t.result()\n\n        with torch.no_grad():\n            with CUDATimer('PROJECTION') as t:\n                graph_tokens = gllm.project(graph_emb)\n        timings['PROJECTION'] = t.result()\n\n        with torch.no_grad():\n            with CUDATimer('LLM_PREFILL') as t:\n                input_ids, attn_mask = tokenize_questions(batch.questions)\n                _ = gllm.forward_prefill(graph_tokens, input_ids, attn_mask)\n        timings['LLM_PREFILL'] = t.result()\n\n        timings_list.append(timings)\n        batch_sizes.append(batch.batch_size)\n        node_counts.append(global_idx.shape[0])\n        edge_counts.append(ei.shape[1])\n\n    return timings_list, batch_sizes, node_counts, edge_counts\n\n\n# ── Run All Profiling ──\n\nall_profile_results = {}\n\nprint('Profiling GAT (LPG) E2E pipeline...')\ntl, bs, nc, ec = profile_lpg_e2e(\n    graph_llms['gat'], lpg_cpu_store, profile_loader, WARMUP_BATCHES, PROFILE_BATCHES)\ngat_df = aggregate_timings(tl, bs, nc, ec)\nall_profile_results['gat'] = {'df': gat_df, 'raw': tl, 'bs': bs, 'nc': nc, 'ec': ec}\nprint_profiling_table(gat_df, 'GAT (LPG)', len(tl))\ntorch.cuda.empty_cache()\n\nfor mt in ['transe', 'distmult']:\n    print(f'\\nProfiling {mt.upper()} (RDF) E2E pipeline...')\n    tl, bs, nc, ec = profile_rdf_e2e(\n        graph_llms[mt], rdf_cpu_stores[mt], kge_models[mt].output_proj,\n        profile_loader, WARMUP_BATCHES, PROFILE_BATCHES)\n    df = aggregate_timings(tl, bs, nc, ec)\n    all_profile_results[mt] = {'df': df, 'raw': tl, 'bs': bs, 'nc': nc, 'ec': ec}\n    print_profiling_table(df, f'{mt.upper()} (RDF)', len(tl))\n    torch.cuda.empty_cache()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Combined Comparison Table + Bottleneck Analysis\n#\n# 3-model side-by-side profiling summary with bottleneck identification\n# and H2D bandwidth analysis.\n\nMODEL_LABELS = {'gat': 'GAT (LPG)', 'transe': 'TransE (RDF)', 'distmult': 'DistMult (RDF)'}\nSTAGE_ORDER = ['SUBGRAPH_LOAD', 'CPU_GATHER', 'CPU_LOOKUP', 'H2D_COPY',\n               'ENCODER_FWD', 'PROJECTION', 'LLM_PREFILL']\n\n# Normalize stage names: GAT uses CPU_GATHER, RDF uses CPU_LOOKUP → unify to CPU_GATHER/LKP\ndef get_stage_ms(df, stage):\n    \"\"\"Get cpu_mean for a stage, returning 0 if not present.\"\"\"\n    row = df[df['stage'] == stage]\n    return row['cpu_mean'].values[0] if len(row) > 0 else 0.0\n\ndef get_stage_p95(df, stage):\n    row = df[df['stage'] == stage]\n    return row['cpu_p95'].values[0] if len(row) > 0 else 0.0\n\n\n# ── Combined Table ──\n\n# Map each model to unified stage list\nUNIFIED_STAGES = ['SUBGRAPH_LOAD', 'CPU_GATHER/LKP', 'H2D_COPY',\n                  'ENCODER_FWD', 'PROJECTION', 'LLM_PREFILL']\n\ndef get_unified_stage(df, unified_name, model_key):\n    \"\"\"Get timing for a unified stage name.\"\"\"\n    if unified_name == 'CPU_GATHER/LKP':\n        # GAT uses CPU_GATHER, RDF models use CPU_LOOKUP\n        if model_key == 'gat':\n            return get_stage_ms(df, 'CPU_GATHER'), get_stage_p95(df, 'CPU_GATHER')\n        else:\n            return get_stage_ms(df, 'CPU_LOOKUP'), get_stage_p95(df, 'CPU_LOOKUP')\n    return get_stage_ms(df, unified_name), get_stage_p95(df, unified_name)\n\n\n# Build combined table\nheader = f\"\\n{'='*85}\"\nheader += f\"\\nG-Retrieval E2E Bottleneck Analysis — {PROFILE_BATCHES} batches, BS={PROFILE_BATCH_SIZE}\"\nheader += f\"\\n{'='*85}\"\nheader += f\"\\n{'Stage':<18}\"\nfor mk in ['gat', 'transe', 'distmult']:\n    header += f\"  {'mean':>6}  {'p95':>6}  {'%':>5}  \"\nprint(header)\n\n# Column headers\ncol_header = f\"{'':18}\"\nfor mk in ['gat', 'transe', 'distmult']:\n    label = MODEL_LABELS[mk]\n    col_header += f\"  {label:^23}\"\nprint(col_header)\nprint('-' * 85)\n\n# Per-model totals for % calculation\nmodel_totals = {}\nfor mk in ['gat', 'transe', 'distmult']:\n    df = all_profile_results[mk]['df']\n    total = sum(get_unified_stage(df, s, mk)[0] for s in UNIFIED_STAGES)\n    model_totals[mk] = total\n\n# Print each stage row\nbottleneck = {}\nfor stage in UNIFIED_STAGES:\n    row_str = f\"{stage:<18}\"\n    for mk in ['gat', 'transe', 'distmult']:\n        df = all_profile_results[mk]['df']\n        mean_ms, p95_ms = get_unified_stage(df, stage, mk)\n        pct = mean_ms / model_totals[mk] * 100 if model_totals[mk] > 0 else 0\n        row_str += f\"  {mean_ms:6.2f}  {p95_ms:6.2f}  {pct:4.1f}%  \"\n        # Track bottleneck (highest %)\n        if mk not in bottleneck or pct > bottleneck[mk][1]:\n            bottleneck[mk] = (stage, pct)\n    print(row_str)\n\nprint('-' * 85)\ntotal_str = f\"{'TOTAL':<18}\"\nfor mk in ['gat', 'transe', 'distmult']:\n    total_str += f\"  {model_totals[mk]:6.2f}  {'':>6}  {'':>5}  \"\nprint(total_str)\n\n# ── Bottleneck Identification ──\nprint(f\"\\n{'='*85}\")\nprint(\"Bottleneck Identification:\")\nfor mk in ['gat', 'transe', 'distmult']:\n    stage, pct = bottleneck[mk]\n    print(f\"  {MODEL_LABELS[mk]:20s}: {stage} ({pct:.1f}%)\")\n\n# ── Encoder vs LLM Time Ratio ──\nprint(f\"\\nEncoder vs LLM Time Ratio:\")\nfor mk in ['gat', 'transe', 'distmult']:\n    df = all_profile_results[mk]['df']\n    enc_ms = get_unified_stage(df, 'ENCODER_FWD', mk)[0]\n    proj_ms = get_unified_stage(df, 'PROJECTION', mk)[0]\n    llm_ms = get_unified_stage(df, 'LLM_PREFILL', mk)[0]\n    encoder_total = enc_ms + proj_ms\n    ratio = encoder_total / llm_ms if llm_ms > 0 else float('inf')\n    print(f\"  {MODEL_LABELS[mk]:20s}: Encoder+Proj={encoder_total:.2f}ms, \"\n          f\"LLM={llm_ms:.2f}ms, Ratio={ratio:.3f}x\")\n\n# ── H2D Bandwidth Analysis ──\nprint(f\"\\nH2D Transfer Bandwidth:\")\n# Theoretical PCIe bandwidth (T4 ≈ 12 GB/s, A100 ≈ 32 GB/s)\nif HAS_CUDA:\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    if 'a100' in gpu_name:\n        theoretical_bw = 32.0\n    elif 'v100' in gpu_name:\n        theoretical_bw = 16.0\n    elif 'l4' in gpu_name:\n        theoretical_bw = 32.0\n    else:\n        theoretical_bw = 12.0  # T4 default\n    print(f\"  Theoretical PCIe BW: ~{theoretical_bw:.0f} GB/s ({gpu_name})\")\nelse:\n    theoretical_bw = None\n    print(\"  (No CUDA device — bandwidth measurement not applicable)\")\n\nfor mk in ['gat', 'transe', 'distmult']:\n    df = all_profile_results[mk]['df']\n    h2d_row = df[df['stage'] == 'H2D_COPY']\n    if len(h2d_row) > 0:\n        bytes_mean = h2d_row['bytes_mean'].values[0]\n        cuda_mean = h2d_row['cuda_mean'].values[0]\n        if not np.isnan(cuda_mean) and cuda_mean > 0:\n            bw_gbps = (bytes_mean / 1e9) / (cuda_mean / 1e3)\n            efficiency = bw_gbps / theoretical_bw * 100 if theoretical_bw else 0\n            print(f\"  {MODEL_LABELS[mk]:20s}: {bytes_mean/1e6:.2f} MB, \"\n                  f\"{cuda_mean:.2f} ms → {bw_gbps:.1f} GB/s \"\n                  f\"({efficiency:.0f}% of theoretical)\" if theoretical_bw\n                  else f\"  {MODEL_LABELS[mk]:20s}: {bytes_mean/1e6:.2f} MB, {cuda_mean:.2f} ms → {bw_gbps:.1f} GB/s\")\n\n# ── Graph Size → Latency Correlation ──\nprint(f\"\\nGraph Size → Total Latency Correlation (Spearman):\")\nfrom scipy.stats import spearmanr as _spearmanr\nfor mk in ['gat', 'transe', 'distmult']:\n    res = all_profile_results[mk]\n    sizes = np.array(res['nc'] if mk == 'gat' else res['ec'])\n    total_ms = np.array([\n        sum(t[s].cpu_ms for s in t) for t in res['raw']\n    ])\n    if len(sizes) > 3:\n        corr, pval = _spearmanr(sizes, total_ms)\n        size_label = \"nodes\" if mk == 'gat' else \"triples\"\n        print(f\"  {MODEL_LABELS[mk]:20s}: rho={corr:.3f}, p={pval:.3e} ({size_label})\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Profiling Visualization\n#\n# 4 charts: (1) Stacked bar — stage-wise breakdown\n#           (2) Scatter — graph size vs total latency\n#           (3) Pie charts — time distribution per model\n#           (4) Encoder vs LLM ratio bar chart\n\nfig = plt.figure(figsize=(18, 14))\n\n# ── Chart 1: Stacked Bar — Stage-wise Time Breakdown ──\nax1 = fig.add_subplot(2, 2, 1)\n\nmodel_keys = ['gat', 'transe', 'distmult']\nx_pos = np.arange(len(model_keys))\nbar_width = 0.5\n\n# Colors for each stage\nstage_colors = {\n    'SUBGRAPH_LOAD': '#2196F3',\n    'CPU_GATHER/LKP': '#FF9800',\n    'H2D_COPY': '#F44336',\n    'ENCODER_FWD': '#4CAF50',\n    'PROJECTION': '#9C27B0',\n    'LLM_PREFILL': '#795548',\n}\n\nbottom = np.zeros(len(model_keys))\nfor stage in UNIFIED_STAGES:\n    vals = []\n    for mk in model_keys:\n        df = all_profile_results[mk]['df']\n        mean_ms, _ = get_unified_stage(df, stage, mk)\n        vals.append(mean_ms)\n    vals = np.array(vals)\n    ax1.bar(x_pos, vals, bar_width, bottom=bottom,\n            label=stage, color=stage_colors[stage], edgecolor='white', linewidth=0.5)\n    bottom += vals\n\nax1.set_xticks(x_pos)\nax1.set_xticklabels([MODEL_LABELS[mk] for mk in model_keys], fontsize=9)\nax1.set_ylabel('Time (ms)')\nax1.set_title('E2E Latency Breakdown by Stage')\nax1.legend(fontsize=7, loc='upper left')\n\n# Add total labels on top\nfor i, mk in enumerate(model_keys):\n    ax1.text(i, bottom[i] + 0.5, f'{bottom[i]:.1f}ms', ha='center', fontsize=8, fontweight='bold')\n\n\n# ── Chart 2: Scatter — Graph Size vs Total Latency ──\nax2 = fig.add_subplot(2, 2, 2)\n\nscatter_colors = {'gat': '#4CAF50', 'transe': '#2196F3', 'distmult': '#FF9800'}\nfor mk in model_keys:\n    res = all_profile_results[mk]\n    sizes = np.array(res['nc'] if mk == 'gat' else res['ec'])\n    total_ms = np.array([sum(t[s].cpu_ms for s in t) for t in res['raw']])\n\n    ax2.scatter(sizes, total_ms, alpha=0.5, s=20, color=scatter_colors[mk], label=MODEL_LABELS[mk])\n\n    # Trend line\n    if len(sizes) > 3:\n        z = np.polyfit(sizes, total_ms, 1)\n        p = np.poly1d(z)\n        x_line = np.linspace(sizes.min(), sizes.max(), 50)\n        ax2.plot(x_line, p(x_line), '--', color=scatter_colors[mk], alpha=0.7, linewidth=1.5)\n\nax2.set_xlabel('Graph Size (nodes for GAT, triples for KGE)')\nax2.set_ylabel('Total Latency (ms)')\nax2.set_title('Graph Size vs E2E Latency')\nax2.legend(fontsize=8)\n\n\n# ── Chart 3: Pie Charts — Time Distribution per Model ──\nfor idx, mk in enumerate(model_keys):\n    ax = fig.add_subplot(2, 6, 7 + idx * 2, aspect='equal')\n    df = all_profile_results[mk]['df']\n\n    sizes_pie = []\n    labels_pie = []\n    colors_pie = []\n    for stage in UNIFIED_STAGES:\n        mean_ms, _ = get_unified_stage(df, stage, mk)\n        if mean_ms > 0:\n            sizes_pie.append(mean_ms)\n            labels_pie.append(stage.replace('CPU_GATHER/LKP', 'GATHER/LKP'))\n            colors_pie.append(stage_colors[stage])\n\n    wedges, texts, autotexts = ax.pie(\n        sizes_pie, labels=None, colors=colors_pie,\n        autopct=lambda p: f'{p:.0f}%' if p > 5 else '',\n        pctdistance=0.75, startangle=90, textprops={'fontsize': 6}\n    )\n    ax.set_title(MODEL_LABELS[mk], fontsize=9, fontweight='bold')\n\n\n# ── Chart 4: Encoder vs LLM Ratio ──\nax4 = fig.add_subplot(2, 6, 12)\n\nenc_times = []\nllm_times = []\nfor mk in model_keys:\n    df = all_profile_results[mk]['df']\n    enc_ms = get_unified_stage(df, 'ENCODER_FWD', mk)[0]\n    proj_ms = get_unified_stage(df, 'PROJECTION', mk)[0]\n    llm_ms = get_unified_stage(df, 'LLM_PREFILL', mk)[0]\n    enc_times.append(enc_ms + proj_ms)\n    llm_times.append(llm_ms)\n\nx_bar = np.arange(len(model_keys))\nbar_w = 0.35\nax4.barh(x_bar - bar_w/2, enc_times, bar_w, label='Encoder+Proj', color='#4CAF50')\nax4.barh(x_bar + bar_w/2, llm_times, bar_w, label='LLM Prefill', color='#795548')\nax4.set_yticks(x_bar)\nax4.set_yticklabels([mk.upper() for mk in model_keys], fontsize=8)\nax4.set_xlabel('Time (ms)')\nax4.set_title('Encoder vs LLM', fontsize=9)\nax4.legend(fontsize=7)\n\nplt.suptitle('G-Retrieval E2E Pipeline Profiling', fontsize=14, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.show()\n\nprint('\\nTrack B profiling complete. Proceeding to Track A (retrieval evaluation)...')\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Graph Embedding Extraction\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(gat_model, kge_models, loader):\n",
    "    \"\"\"Extract per-question graph embeddings from all models.\n",
    "\n",
    "    Returns dict with:\n",
    "        'gat': [N, 384] LPG-GAT embeddings\n",
    "        'transe': [N, 384] RDF-TransE embeddings\n",
    "        'distmult': [N, 384] RDF-DistMult embeddings\n",
    "        'questions': list of question strings\n",
    "        'answers': list of answer strings\n",
    "        'question_ids': list of question IDs\n",
    "        'categories': list of category strings\n",
    "    \"\"\"\n",
    "    gat_model.eval()\n",
    "    for m in kge_models.values():\n",
    "        m.eval()\n",
    "\n",
    "    all_gat, all_transe, all_distmult = [], [], []\n",
    "    all_questions, all_answers, all_qids, all_cats = [], [], [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch_gpu = batch.to(device)\n",
    "\n",
    "        # GAT embedding\n",
    "        gat_emb = gat_model(batch_gpu.lpg_x, batch_gpu.lpg_edge_index, batch_gpu.lpg_batch)\n",
    "        all_gat.append(gat_emb.cpu())\n",
    "\n",
    "        # KGE embeddings\n",
    "        for name, model in kge_models.items():\n",
    "            emb = model(batch_gpu)\n",
    "            if name == 'transe':\n",
    "                all_transe.append(emb.cpu())\n",
    "            else:\n",
    "                all_distmult.append(emb.cpu())\n",
    "\n",
    "        all_questions.extend(batch.questions)\n",
    "        all_answers.extend(batch.answers)\n",
    "        all_qids.extend(batch.question_ids)\n",
    "        all_cats.extend(batch.categories)\n",
    "\n",
    "    return {\n",
    "        'gat': F.normalize(torch.cat(all_gat, dim=0), dim=-1),\n",
    "        'transe': F.normalize(torch.cat(all_transe, dim=0), dim=-1),\n",
    "        'distmult': F.normalize(torch.cat(all_distmult, dim=0), dim=-1),\n",
    "        'questions': all_questions,\n",
    "        'answers': all_answers,\n",
    "        'question_ids': all_qids,\n",
    "        'categories': all_cats,\n",
    "    }\n",
    "\n",
    "\n",
    "print('Extracting embeddings...')\n",
    "train_emb = extract_embeddings(gat_model, kge_models, train_loader)\n",
    "val_emb   = extract_embeddings(gat_model, kge_models, val_loader)\n",
    "test_emb  = extract_embeddings(gat_model, kge_models, test_loader)\n",
    "\n",
    "print(f'  Train: {train_emb[\"gat\"].shape[0]} samples')\n",
    "print(f'  Val:   {val_emb[\"gat\"].shape[0]} samples')\n",
    "print(f'  Test:  {test_emb[\"gat\"].shape[0]} samples')\n",
    "print(f'  Embedding dim: {train_emb[\"gat\"].shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Question-Answer Embedding Baseline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2', device=str(device))\n",
    "\n",
    "\n",
    "def encode_texts(texts, batch_size=64):\n",
    "    \"\"\"Encode texts with sentence-transformers → [N, 384] normalized.\"\"\"\n",
    "    embs = st_model.encode(texts, batch_size=batch_size, show_progress_bar=False,\n",
    "                           convert_to_tensor=True, normalize_embeddings=True)\n",
    "    return embs.cpu()\n",
    "\n",
    "\n",
    "print('Encoding answers with sentence-transformers...')\n",
    "test_answer_emb = encode_texts(test_emb['answers'])\n",
    "test_question_emb = encode_texts(test_emb['questions'])\n",
    "\n",
    "# Quick check: cosine similarity between graph embeddings and answer embeddings\n",
    "print('\\nMean cosine similarity (graph_emb · answer_emb):')\n",
    "for name in ['gat', 'transe', 'distmult']:\n",
    "    cos_sim = (test_emb[name] * test_answer_emb).sum(dim=-1).mean().item()\n",
    "    print(f'  {name:10s}: {cos_sim:.4f}')\n",
    "\n",
    "# Baseline: question-only embedding\n",
    "q_cos = (test_question_emb * test_answer_emb).sum(dim=-1).mean().item()\n",
    "print(f'  {\"question\":10s}: {q_cos:.4f} (text-only baseline)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Retrieval Evaluation\n",
    "\n",
    "def retrieval_eval(query_emb, corpus_emb, ks=(1, 5, 10)):\n",
    "    \"\"\"Compute retrieval metrics: Recall@K and MRR.\n",
    "\n",
    "    Each query[i] should retrieve corpus[i] (diagonal = ground truth).\n",
    "\n",
    "    Args:\n",
    "        query_emb: [N, D] query embeddings (graph or question)\n",
    "        corpus_emb: [N, D] corpus embeddings (answers)\n",
    "        ks: tuple of K values for Recall@K\n",
    "\n",
    "    Returns:\n",
    "        dict with 'mrr' and 'recall@k' for each k\n",
    "    \"\"\"\n",
    "    # Similarity matrix [N, N]\n",
    "    sim = query_emb @ corpus_emb.T\n",
    "    N = sim.shape[0]\n",
    "\n",
    "    # Rank of the correct answer (diagonal)\n",
    "    diag = sim.diag().unsqueeze(1)  # [N, 1]\n",
    "    ranks = (sim >= diag).sum(dim=1).float()  # [N] — 1-based rank\n",
    "\n",
    "    results = {'mrr': (1.0 / ranks).mean().item()}\n",
    "    for k in ks:\n",
    "        results[f'recall@{k}'] = (ranks <= k).float().mean().item()\n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "print(f'{\"Model\":<12} {\"MRR\":>8} {\"R@1\":>8} {\"R@5\":>8} {\"R@10\":>8}')\n",
    "print('-' * 48)\n",
    "\n",
    "retrieval_results = {}\n",
    "for name in ['gat', 'transe', 'distmult']:\n",
    "    r = retrieval_eval(test_emb[name], test_answer_emb)\n",
    "    retrieval_results[name] = r\n",
    "    print(f'{name:<12} {r[\"mrr\"]:8.4f} {r[\"recall@1\"]:8.4f} {r[\"recall@5\"]:8.4f} {r[\"recall@10\"]:8.4f}')\n",
    "\n",
    "# Baseline: question text → answer text retrieval\n",
    "r_baseline = retrieval_eval(test_question_emb, test_answer_emb)\n",
    "retrieval_results['question'] = r_baseline\n",
    "print(f'{\"question\":<12} {r_baseline[\"mrr\"]:8.4f} {r_baseline[\"recall@1\"]:8.4f} {r_baseline[\"recall@5\"]:8.4f} {r_baseline[\"recall@10\"]:8.4f}')\n",
    "print('  (question = text-only baseline, no graph)')\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "models = list(retrieval_results.keys())\n",
    "metrics = ['mrr', 'recall@1', 'recall@5', 'recall@10']\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    vals = [retrieval_results[model][m] for model in models]\n",
    "    ax.bar(x + i * width, vals, width, label=m.upper())\n",
    "\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([m.upper() for m in models])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Graph → Answer Retrieval Performance')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Category-wise Analysis\n",
    "\n",
    "def category_retrieval_eval(emb_dict, answer_emb, categories):\n",
    "    \"\"\"Evaluate retrieval per category.\n",
    "\n",
    "    Returns DataFrame: rows=categories, columns=model×metric.\n",
    "    \"\"\"\n",
    "    cats = sorted(set(categories))\n",
    "    cat_array = np.array(categories)\n",
    "    rows = []\n",
    "\n",
    "    for cat in cats:\n",
    "        mask = cat_array == cat\n",
    "        n = mask.sum()\n",
    "        if n < 2:\n",
    "            continue\n",
    "        indices = np.where(mask)[0]\n",
    "\n",
    "        row = {'category': cat, 'n': int(n)}\n",
    "        for model_name in ['gat', 'transe', 'distmult']:\n",
    "            q_emb = emb_dict[model_name][indices]\n",
    "            a_emb = answer_emb[indices]\n",
    "            r = retrieval_eval(q_emb, a_emb)\n",
    "            for metric, val in r.items():\n",
    "                row[f'{model_name}_{metric}'] = val\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "cat_df = category_retrieval_eval(test_emb, test_answer_emb, test_emb['categories'])\n",
    "print('Category-wise MRR:')\n",
    "print(cat_df[['category', 'n', 'gat_mrr', 'transe_mrr', 'distmult_mrr']].to_string(index=False))\n",
    "\n",
    "# Bar chart: category × model MRR\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "cats = cat_df['category'].values\n",
    "x = np.arange(len(cats))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(['gat', 'transe', 'distmult']):\n",
    "    vals = cat_df[f'{model}_mrr'].values\n",
    "    ax.bar(x + i * width, vals, width, label=model.upper())\n",
    "\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(cats, rotation=30, ha='right')\n",
    "ax.set_ylabel('MRR')\n",
    "ax.set_title('Category-wise Graph → Answer Retrieval MRR')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap: model advantage per category\n",
    "print('\\nBest model per category (MRR):')\n",
    "for _, row in cat_df.iterrows():\n",
    "    mrrs = {m: row[f'{m}_mrr'] for m in ['gat', 'transe', 'distmult']}\n",
    "    best = max(mrrs, key=mrrs.get)\n",
    "    print(f'  {row[\"category\"]:25s} → {best.upper()} ({mrrs[best]:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Graph Size Effect\n",
    "\n",
    "def compute_per_sample_rank(query_emb, corpus_emb):\n",
    "    \"\"\"Compute rank of correct answer for each sample.\"\"\"\n",
    "    sim = query_emb @ corpus_emb.T\n",
    "    diag = sim.diag().unsqueeze(1)\n",
    "    ranks = (sim >= diag).sum(dim=1).float()\n",
    "    return ranks.numpy()\n",
    "\n",
    "\n",
    "# Collect graph sizes from test set\n",
    "test_lpg_nodes = []\n",
    "test_lpg_edges = []\n",
    "test_rdf_triples = []\n",
    "\n",
    "for i in range(len(test_ds)):\n",
    "    d = test_ds[i]\n",
    "    test_lpg_nodes.append(d.lpg_num_nodes.item())\n",
    "    test_lpg_edges.append(d.lpg_edge_index.shape[1])\n",
    "    test_rdf_triples.append(d.rdf_edge_index.shape[1])\n",
    "\n",
    "test_lpg_nodes = np.array(test_lpg_nodes)\n",
    "test_lpg_edges = np.array(test_lpg_edges)\n",
    "test_rdf_triples = np.array(test_rdf_triples)\n",
    "\n",
    "# Compute per-sample reciprocal rank\n",
    "rr = {}\n",
    "for name in ['gat', 'transe', 'distmult']:\n",
    "    ranks = compute_per_sample_rank(test_emb[name], test_answer_emb)\n",
    "    rr[name] = 1.0 / ranks\n",
    "\n",
    "# Scatter plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "for ax, (name, size_arr, size_label) in zip(axes, [\n",
    "    ('gat', test_lpg_nodes, 'LPG Nodes'),\n",
    "    ('transe', test_rdf_triples, 'RDF Triples'),\n",
    "    ('distmult', test_rdf_triples, 'RDF Triples'),\n",
    "]):\n",
    "    ax.scatter(size_arr, rr[name], alpha=0.3, s=10)\n",
    "    ax.set_xlabel(size_label)\n",
    "    ax.set_ylabel('Reciprocal Rank')\n",
    "    ax.set_title(f'{name.upper()}')\n",
    "\n",
    "    # Trend line via binning\n",
    "    bins = np.percentile(size_arr, np.linspace(0, 100, 11))\n",
    "    bins = np.unique(bins)\n",
    "    if len(bins) >= 2:\n",
    "        bin_idx = np.digitize(size_arr, bins) - 1\n",
    "        bin_idx = np.clip(bin_idx, 0, len(bins) - 2)\n",
    "        bin_centers = []\n",
    "        bin_means = []\n",
    "        for b in range(len(bins) - 1):\n",
    "            mask = bin_idx == b\n",
    "            if mask.sum() > 0:\n",
    "                bin_centers.append((bins[b] + bins[b+1]) / 2)\n",
    "                bin_means.append(rr[name][mask].mean())\n",
    "        ax.plot(bin_centers, bin_means, 'r-o', linewidth=2, markersize=4, label='Binned mean')\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle('Graph Size vs Retrieval Quality', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "print('Spearman correlation (graph size vs reciprocal rank):')\n",
    "from scipy.stats import spearmanr\n",
    "for name, sizes in [('gat', test_lpg_nodes), ('transe', test_rdf_triples), ('distmult', test_rdf_triples)]:\n",
    "    corr, pval = spearmanr(sizes, rr[name])\n",
    "    print(f'  {name:10s}: rho={corr:.3f}, p={pval:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: TransE vs DistMult Deep Dive\n",
    "\n",
    "# 1. Loss curve comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(kge_histories['transe']['train_loss'], label='TransE')\n",
    "axes[0].plot(kge_histories['distmult']['train_loss'], label='DistMult')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Val MRR over time\n",
    "eval_epochs_kge = [e for e in range(1, KGE_EPOCHS+1) if e % 10 == 0 or e == 1]\n",
    "for mt in ['transe', 'distmult']:\n",
    "    n = len(kge_histories[mt]['val_mrr'])\n",
    "    axes[1].plot(eval_epochs_kge[:n], kge_histories[mt]['val_mrr'][:n], '-o', label=mt.upper())\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Val MRR')\n",
    "axes[1].set_title('Validation MRR Over Training')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. Per-sample comparison: where does one beat the other?\n",
    "transe_rr = rr['transe']\n",
    "distmult_rr = rr['distmult']\n",
    "diff = transe_rr - distmult_rr  # positive = TransE better\n",
    "\n",
    "transe_wins = (diff > 0).sum()\n",
    "distmult_wins = (diff < 0).sum()\n",
    "ties = (diff == 0).sum()\n",
    "print(f'Per-sample comparison (test set, N={len(diff)}):')\n",
    "print(f'  TransE wins:  {transe_wins} ({100*transe_wins/len(diff):.1f}%)')\n",
    "print(f'  DistMult wins: {distmult_wins} ({100*distmult_wins/len(diff):.1f}%)')\n",
    "print(f'  Ties:         {ties} ({100*ties/len(diff):.1f}%)')\n",
    "\n",
    "\n",
    "# 3. Category-level TransE vs DistMult advantage\n",
    "print('\\nCategory-level advantage (MRR difference = TransE - DistMult):')\n",
    "for _, row in cat_df.iterrows():\n",
    "    delta = row['transe_mrr'] - row['distmult_mrr']\n",
    "    arrow = '→ TransE' if delta > 0 else '→ DistMult'\n",
    "    print(f'  {row[\"category\"]:25s}: {delta:+.4f} {arrow}')\n",
    "\n",
    "\n",
    "# 4. t-SNE visualization of embedding spaces\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Use test set embeddings (subsample if needed)\n",
    "n_vis = min(300, test_emb['gat'].shape[0])\n",
    "vis_idx = np.random.choice(test_emb['gat'].shape[0], n_vis, replace=False)\n",
    "vis_cats = np.array(test_emb['categories'])[vis_idx]\n",
    "unique_cats = sorted(set(vis_cats))\n",
    "cat_colors = {c: plt.cm.tab10(i) for i, c in enumerate(unique_cats)}\n",
    "colors = [cat_colors[c] for c in vis_cats]\n",
    "\n",
    "for ax, name in zip(axes, ['gat', 'transe', 'distmult']):\n",
    "    emb = test_emb[name][vis_idx].numpy()\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    proj = tsne.fit_transform(emb)\n",
    "\n",
    "    for cat in unique_cats:\n",
    "        mask = vis_cats == cat\n",
    "        ax.scatter(proj[mask, 0], proj[mask, 1], c=[cat_colors[cat]],\n",
    "                   s=15, alpha=0.6, label=cat[:15])\n",
    "    ax.set_title(f'{name.upper()} Embedding Space')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "plt.suptitle('t-SNE of Graph Embeddings (colored by category)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Summary & Conclusions\n",
    "\n",
    "print('=' * 60)\n",
    "print('SUMMARY: G-Retrieval Style Comparison')\n",
    "print('=' * 60)\n",
    "\n",
    "# Overall retrieval results table\n",
    "results_df = pd.DataFrame([\n",
    "    {'model': name, **metrics}\n",
    "    for name, metrics in retrieval_results.items()\n",
    "    if name != 'question'\n",
    "])\n",
    "print('\\n--- Graph → Answer Retrieval (Test Set) ---')\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Best model\n",
    "best_model = results_df.loc[results_df['mrr'].idxmax(), 'model']\n",
    "print(f'\\nBest overall model: {best_model.upper()} (MRR={results_df[\"mrr\"].max():.4f})')\n",
    "\n",
    "# Category breakdown summary\n",
    "print('\\n--- Best Model per Category (MRR) ---')\n",
    "category_results = []\n",
    "for _, row in cat_df.iterrows():\n",
    "    mrrs = {m: row[f'{m}_mrr'] for m in ['gat', 'transe', 'distmult']}\n",
    "    best = max(mrrs, key=mrrs.get)\n",
    "    category_results.append({'category': row['category'], 'best_model': best,\n",
    "                             'mrr': mrrs[best], 'n': row['n']})\n",
    "    print(f'  {row[\"category\"]:25s} → {best.upper():10s} (MRR={mrrs[best]:.4f}, n={int(row[\"n\"])})')\n",
    "\n",
    "category_results = pd.DataFrame(category_results)\n",
    "\n",
    "# Model strengths\n",
    "print('\\n--- Model Strengths & Weaknesses ---')\n",
    "print('GAT (LPG):      Pre-computed 384d node features + message passing.')\n",
    "print('                 Best for: categories with rich LPG structure.')\n",
    "print('TransE (RDF):    Translation h+r≈t. Asymmetric, handles directed relations.')\n",
    "print('                 Best for: categories with directional relationships (OWNS, REPORTED).')\n",
    "print('DistMult (RDF):  Bilinear h·r·t. Symmetric, simpler training dynamics.')\n",
    "print('                 Best for: symmetric or co-occurrence patterns.')\n",
    "\n",
    "# Verification assertions\n",
    "assert len(results_df) == 3, f'Expected 3 models, got {len(results_df)}'\n",
    "assert all(col in results_df.columns for col in ['model', 'mrr', 'recall@1', 'recall@5'])\n",
    "assert len(category_results) >= 1, 'No category results'\n",
    "print(f'\\nVerification passed: {len(results_df)} models, {len(category_results)} categories.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}