{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinDER KG: Full Comparison Experiment (Clean)\n",
    "\n",
    "## 4Í∞ÄÏßÄ Î∞©Î≤ï ÎπÑÍµê Ïã§Ìóò\n",
    "\n",
    "| Ïã§Ìóò | Î∞©Î≤ï | ÏÑ§Î™Ö |\n",
    "|------|------|------|\n",
    "| **[A]** | LLM Only | Context ÏóÜÏù¥ LLMÎßå ÏÇ¨Ïö© |\n",
    "| **[B]** | LLM + Text RAG | References ÌÖçÏä§Ìä∏Î•º contextÎ°ú ÏÇ¨Ïö© |\n",
    "| **[C]** | LLM + Graph RAG (LPG) | Message-Passing GNN (GAT) |\n",
    "| **[D]** | LLM + Graph RAG (RDF) | Knowledge Graph Embedding (TransE) |\n",
    "\n",
    "**Requirements**: A100 GPU, High RAM runtime\n",
    "\n",
    "**Í≤∞Í≥º Ï†ÄÏû•**: Google DriveÏóê ÏûêÎèô Ï†ÄÏû•Îê®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ÌôïÏù∏\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò\n",
    "!pip install -q torch torch_geometric neo4j transformers sentence-transformers tqdm pandas\n",
    "!pip install -q pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device ÏÑ§Ï†ï\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBEDDING_DIM = 384\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Google Drive Mount & Result Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Google Drive ÎßàÏö¥Ìä∏\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï\n",
    "RESULT_DIR = '/content/drive/MyDrive/Colab Notebooks/kg-fibo-finder/experiment_results'\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# Ïã§Ìóò ID (ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ)\n",
    "EXPERIMENT_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(f\"Experiment ID: {EXPERIMENT_ID}\")\n",
    "print(f\"Results will be saved to: {RESULT_DIR}\")\n",
    "\n",
    "def save_checkpoint(data, name):\n",
    "    \"\"\"Ï§ëÍ∞Ñ Í≤∞Í≥º Ï†ÄÏû•\"\"\"\n",
    "    filepath = f\"{RESULT_DIR}/{EXPERIMENT_ID}_{name}.csv\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data.to_csv(filepath, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(data).to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Saved checkpoint: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_log(message):\n",
    "    \"\"\"Î°úÍ∑∏ Ï†ÄÏû•\"\"\"\n",
    "    log_path = f\"{RESULT_DIR}/{EXPERIMENT_ID}_log.txt\"\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(f\"[{datetime.now().strftime('%H:%M:%S')}] {message}\\n\")\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinDER KG Merged Parquet Î°úÎìú\n",
    "PARQUET_PATH = '/content/drive/MyDrive/Colab Notebooks/kg-fibo-finder/FinDER_KG_Merged.parquet'\n",
    "\n",
    "if os.path.exists(PARQUET_PATH):\n",
    "    df = pd.read_parquet(PARQUET_PATH)\n",
    "    save_log(f\"Loaded FinDER_KG_Merged.parquet: {len(df)} rows\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    display(df.head(3))\n",
    "else:\n",
    "    save_log(f\"File not found: {PARQUET_PATH}, trying HuggingFace...\")\n",
    "    df = pd.read_parquet(\"hf://datasets/Linq-AI-Research/FinDER/data/train-00000-of-00001.parquet\")\n",
    "    save_log(f\"Loaded from HuggingFace: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import login\n\n# Set your HuggingFace token (get from https://huggingface.co/settings/tokens)\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"YOUR_HF_TOKEN_HERE\")\nlogin(token=HF_TOKEN)\nsave_log(\"HuggingFace Î°úÍ∑∏Ïù∏ ÏôÑÎ£å!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Neo4j Connection (ngrok)\n",
    "\n",
    "‚ö†Ô∏è **Î°úÏª¨ÏóêÏÑú Neo4j + ngrok Ïã§Ìñâ ÌïÑÏöî**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrok Ï£ºÏÜå ÏÑ§Ï†ï (Î°úÏª¨ÏóêÏÑú ngrok tcp 7687 Ïã§Ìñâ ÌõÑ ÎÇòÏò§Îäî Ï£ºÏÜå)\n",
    "NGROK_ADDRESS = \"0.tcp.jp.ngrok.io:17320\"  # <-- Î≥∏Ïù∏ ngrok Ï£ºÏÜåÎ°ú Î≥ÄÍ≤Ω!\n",
    "\n",
    "NEO4J_URI = f\"bolt://{NGROK_ADDRESS}\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "\n",
    "print(f\"Neo4j URI: {NEO4J_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_CONNECTED = False\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "    driver.verify_connectivity()\n",
    "    save_log(\"Neo4j Ïó∞Í≤∞ ÏÑ±Í≥µ!\")\n",
    "    NEO4J_CONNECTED = True\n",
    "    \n",
    "    for db_name in [\"finderlpg\", \"finderrdf\"]:\n",
    "        try:\n",
    "            with driver.session(database=db_name) as session:\n",
    "                result = session.run(\"MATCH (n) RETURN count(n) as count\")\n",
    "                count = result.single()[\"count\"]\n",
    "                save_log(f\"  {db_name}: {count:,} nodes\")\n",
    "        except Exception as e:\n",
    "            save_log(f\"  {db_name}: Error - {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    save_log(f\"Neo4j Ïó∞Í≤∞ Ïã§Ìå®: {e}\")\n",
    "    save_log(\"‚ö†Ô∏è Graph RAG Ïã§Ìóò [C], [D]Îäî Ïä§ÌÇµÎê©ÎãàÎã§. LLM Only, Text RAGÎßå Ïã§ÌñâÌï©ÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Load Models (LLM + Embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Sentence Transformer\n",
    "save_log(\"Loading SentenceTransformer...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "save_log(f\"Embedder loaded: {EMBEDDING_DIM}-dim embeddings\")\n",
    "\n",
    "# LLM (Llama 3.1)\n",
    "LLM_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "save_log(f\"Loading LLM: {LLM_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "llm.eval()\n",
    "save_log(f\"LLM loaded on: {next(llm.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. GNN Models Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn.kge import TransE\n",
    "\n",
    "class MessagePassingGNN(torch.nn.Module):\n",
    "    \"\"\"LPGÏö© GNN: GAT\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=384, num_layers=2, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_ch = hidden_dim if i == 0 else hidden_dim * heads\n",
    "            self.convs.append(GATConv(in_ch, hidden_dim, heads=heads, dropout=dropout))\n",
    "            self.norms.append(torch.nn.LayerNorm(hidden_dim * heads))\n",
    "        self.output_proj = torch.nn.Linear(hidden_dim * heads, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_type=None):\n",
    "        x = torch.relu(self.input_proj(x))\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x, edge_index)\n",
    "            x = norm(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output_proj(x.mean(dim=0))\n",
    "\n",
    "class TransEEncoder(torch.nn.Module):\n",
    "    \"\"\"RDFÏö© KGE: TransE\"\"\"\n",
    "    def __init__(self, num_nodes, num_relations, hidden_dim=256, output_dim=384):\n",
    "        super().__init__()\n",
    "        self.transe = TransE(num_nodes=num_nodes, num_relations=num_relations, \n",
    "                             hidden_channels=hidden_dim, margin=1.0, p_norm=1.0)\n",
    "        self.output_proj = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, head_index, rel_type, tail_index):\n",
    "        if len(head_index) == 0:\n",
    "            return torch.zeros(self.output_dim, device=head_index.device)\n",
    "        head_emb = self.transe.node_emb(head_index)\n",
    "        rel_emb = self.transe.rel_emb(rel_type)\n",
    "        return self.output_proj((head_emb + rel_emb).mean(dim=0))\n",
    "\n",
    "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "save_log(\"Initializing GNN models...\")\n",
    "gnn_lpg = MessagePassingGNN(input_dim=EMBEDDING_DIM).to(DEVICE)\n",
    "save_log(f\"MessagePassingGNN (LPG/GAT): {sum(p.numel() for p in gnn_lpg.parameters()):,} params\")\n",
    "\n",
    "NUM_NODES_RDF, NUM_RELATIONS_RDF = 15000, 100\n",
    "kge_rdf = TransEEncoder(num_nodes=NUM_NODES_RDF, num_relations=NUM_RELATIONS_RDF, \n",
    "                        hidden_dim=256, output_dim=EMBEDDING_DIM).to(DEVICE)\n",
    "save_log(f\"TransEEncoder (RDF/TransE): {sum(p.numel() for p in kge_rdf.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = {}\n",
    "\n",
    "def get_entity_embedding(entity_text):\n",
    "    if entity_text not in entity_embeddings:\n",
    "        entity_embeddings[entity_text] = embedder.encode(entity_text, convert_to_tensor=True)\n",
    "    return entity_embeddings[entity_text]\n",
    "\n",
    "def generate_response(question, context=None, max_new_tokens=256):\n",
    "    if context:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial expert. Answer based ONLY on the provided context. Be concise.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial expert. Answer concisely.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(llm.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm.generate(input_ids, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "def load_subgraph_lpg(driver, question_id, max_hops=2):\n",
    "    query = f\"\"\"\n",
    "    MATCH (e:Entity) WHERE $qid IN e.question_ids\n",
    "    WITH collect(e) as seed_nodes UNWIND seed_nodes as seed\n",
    "    OPTIONAL MATCH path = (seed)-[*1..{max_hops}]-(connected:Entity)\n",
    "    WITH seed_nodes + collect(DISTINCT connected) as all_nodes UNWIND all_nodes as n\n",
    "    WITH DISTINCT n OPTIONAL MATCH (n)-[r]->(m:Entity)\n",
    "    RETURN collect(DISTINCT {{id: n.id, label: n.label, name: coalesce(n.name, n.id)}}) as nodes,\n",
    "           collect(DISTINCT {{source: startNode(r).id, target: endNode(r).id, type: type(r)}}) as edges\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with driver.session(database=\"finderlpg\") as session:\n",
    "            record = session.run(query, qid=question_id).single()\n",
    "        if record:\n",
    "            return {\"nodes\": record[\"nodes\"], \"edges\": [e for e in record[\"edges\"] if e[\"source\"] and e[\"target\"]]}\n",
    "    except: pass\n",
    "    return {\"nodes\": [], \"edges\": []}\n",
    "\n",
    "def load_subgraph_rdf(driver, question_id):\n",
    "    query = \"\"\"MATCH (r:Resource)-[rel]-(r2:Resource) WITH r, r2, rel LIMIT 100\n",
    "    RETURN collect(DISTINCT {id: r.uri, uri: r.uri}) + collect(DISTINCT {id: r2.uri, uri: r2.uri}) as nodes,\n",
    "           collect(DISTINCT {source: r.uri, target: r2.uri, type: type(rel)}) as edges\"\"\"\n",
    "    try:\n",
    "        with driver.session(database=\"finderrdf\") as session:\n",
    "            record = session.run(query, qid=question_id).single()\n",
    "        if record:\n",
    "            seen = set()\n",
    "            unique_nodes = [n for n in record[\"nodes\"] if n and n.get(\"id\") and n[\"id\"] not in seen and not seen.add(n[\"id\"])]\n",
    "            return {\"nodes\": unique_nodes[:50], \"edges\": [e for e in record[\"edges\"] if e[\"source\"] and e[\"target\"]][:100]}\n",
    "    except: pass\n",
    "    return {\"nodes\": [], \"edges\": []}\n",
    "\n",
    "def build_lpg_graph(subgraph, embedding_dim):\n",
    "    nodes, edges = subgraph.get(\"nodes\", []), subgraph.get(\"edges\", [])\n",
    "    if not nodes: return None\n",
    "    node_to_idx = {n[\"id\"]: i for i, n in enumerate(nodes)}\n",
    "    node_texts = [f\"{n.get('label', '')}: {n.get('name', n['id'])}\" for n in nodes]\n",
    "    x = torch.stack([get_entity_embedding(t) for t in node_texts])\n",
    "    edge_index, edge_descs = [], []\n",
    "    for e in edges:\n",
    "        src, tgt = e.get(\"source\"), e.get(\"target\")\n",
    "        if src in node_to_idx and tgt in node_to_idx:\n",
    "            edge_index.append([node_to_idx[src], node_to_idx[tgt]])\n",
    "            edge_descs.append(f\"{src} --{e.get('type', 'rel')}--> {tgt}\")\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() if edge_index else torch.zeros((2, 0), dtype=torch.long)\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    data.node_descs, data.edge_descs = node_texts, edge_descs\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_TO_IDX, RELATION_TO_IDX = {}, {}\n",
    "\n",
    "def run_llm_only(question):\n",
    "    return generate_response(question)\n",
    "\n",
    "def run_text_rag(question, references):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(references) if isinstance(references, str) else references\n",
    "        context = \"\\n\".join(parsed) if isinstance(parsed, list) else str(references)\n",
    "    except: context = str(references) if references else \"\"\n",
    "    return generate_response(question, context=context) if context else run_llm_only(question)\n",
    "\n",
    "def run_graph_rag_lpg(question, question_id, driver, gnn, device):\n",
    "    subgraph = load_subgraph_lpg(driver, question_id)\n",
    "    pyg_data = build_lpg_graph(subgraph, EMBEDDING_DIM)\n",
    "    if pyg_data is None or pyg_data.x.shape[0] == 0:\n",
    "        return run_llm_only(question), {\"nodes\": 0, \"edges\": 0}\n",
    "    pyg_data = pyg_data.to(device)\n",
    "    gnn.eval()\n",
    "    with torch.no_grad(): _ = gnn(pyg_data.x, pyg_data.edge_index)\n",
    "    context = \"=== [LPG] Entities ===\\n\" + \"\\n\".join(f\"- {d}\" for d in pyg_data.node_descs[:20])\n",
    "    context += \"\\n\\n=== Relationships ===\\n\" + \"\\n\".join(f\"- {d}\" for d in pyg_data.edge_descs[:30])\n",
    "    return generate_response(question, context), {\"nodes\": pyg_data.x.shape[0], \"edges\": pyg_data.edge_index.shape[1]}\n",
    "\n",
    "def run_graph_rag_rdf(question, question_id, driver, kge_model, device):\n",
    "    global NODE_TO_IDX, RELATION_TO_IDX\n",
    "    subgraph = load_subgraph_rdf(driver, question_id)\n",
    "    edges = subgraph.get('edges', [])\n",
    "    if not edges: return run_llm_only(question), {\"nodes\": 0, \"edges\": 0, \"relations\": 0}\n",
    "    head_indices, rel_types, tail_indices, edge_descs = [], [], [], []\n",
    "    for e in edges:\n",
    "        src, tgt, rel = e.get('source'), e.get('target'), e.get('type') or 'related'\n",
    "        if src and tgt:\n",
    "            if src not in NODE_TO_IDX: NODE_TO_IDX[src] = len(NODE_TO_IDX) % kge_model.transe.num_nodes\n",
    "            if tgt not in NODE_TO_IDX: NODE_TO_IDX[tgt] = len(NODE_TO_IDX) % kge_model.transe.num_nodes\n",
    "            if rel not in RELATION_TO_IDX: RELATION_TO_IDX[rel] = len(RELATION_TO_IDX) % kge_model.transe.num_relations\n",
    "            head_indices.append(NODE_TO_IDX[src]); rel_types.append(RELATION_TO_IDX[rel]); tail_indices.append(NODE_TO_IDX[tgt])\n",
    "            edge_descs.append(f\"{src} --{rel}--> {tgt}\")\n",
    "    if not head_indices: return run_llm_only(question), {\"nodes\": 0, \"edges\": 0, \"relations\": 0}\n",
    "    kge_model.eval()\n",
    "    with torch.no_grad(): _ = kge_model(torch.tensor(head_indices, device=device), torch.tensor(rel_types, device=device), torch.tensor(tail_indices, device=device))\n",
    "    context = \"=== [RDF/TransE] Triples ===\\n\" + \"\\n\".join(f\"- {d}\" for d in edge_descs[:40])\n",
    "    return generate_response(question, context), {\"nodes\": len(set(head_indices + tail_indices)), \"edges\": len(head_indices), \"relations\": len(set(rel_types))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Run Full Comparison (with Auto-Save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_comparison(df, sample_indices, driver, gnn_lpg, kge_rdf, device, save_every=5):\n",
    "    \"\"\"\n",
    "    4Í∞ÄÏßÄ Î∞©Î≤ï ÎπÑÍµê Ïã§Ìóò (Ï§ëÍ∞Ñ Ï†ÄÏû• Ìè¨Ìï®)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, idx in enumerate(tqdm(sample_indices, desc=\"Running experiments\")):\n",
    "        try:\n",
    "            row = df.iloc[idx]\n",
    "            question_id = row['_id']\n",
    "            question = row['text']\n",
    "            ground_truth = row['answer']\n",
    "            references = row.get('references', '')\n",
    "            category = row.get('category', '')\n",
    "            \n",
    "            result = {\n",
    "                'idx': idx, 'question_id': question_id, 'question': question,\n",
    "                'ground_truth': ground_truth, 'category': category,\n",
    "            }\n",
    "            \n",
    "            # [A] LLM Only\n",
    "            result['response_llm_only'] = run_llm_only(question)\n",
    "            \n",
    "            # [B] Text RAG\n",
    "            result['response_text_rag'] = run_text_rag(question, references)\n",
    "            result['has_references'] = bool(references)\n",
    "            \n",
    "            # [C] & [D] Graph RAG (Neo4j Ïó∞Í≤∞ÏãúÏóêÎßå)\n",
    "            if NEO4J_CONNECTED:\n",
    "                result['response_graph_lpg'], stats_lpg = run_graph_rag_lpg(question, question_id, driver, gnn_lpg, device)\n",
    "                result['lpg_nodes'], result['lpg_edges'] = stats_lpg['nodes'], stats_lpg['edges']\n",
    "                \n",
    "                result['response_graph_rdf'], stats_rdf = run_graph_rag_rdf(question, question_id, driver, kge_rdf, device)\n",
    "                result['rdf_nodes'], result['rdf_edges'] = stats_rdf['nodes'], stats_rdf['edges']\n",
    "                result['rdf_relations'] = stats_rdf.get('relations', 0)\n",
    "            else:\n",
    "                result['response_graph_lpg'] = \"[SKIPPED - Neo4j not connected]\"\n",
    "                result['response_graph_rdf'] = \"[SKIPPED - Neo4j not connected]\"\n",
    "                result['lpg_nodes'] = result['lpg_edges'] = 0\n",
    "                result['rdf_nodes'] = result['rdf_edges'] = result['rdf_relations'] = 0\n",
    "            \n",
    "            results.append(result)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Ï§ëÍ∞Ñ Ï†ÄÏû• (save_every ÏÉòÌîåÎßàÎã§)\n",
    "            if (i + 1) % save_every == 0:\n",
    "                save_checkpoint(results, f\"checkpoint_{i+1}\")\n",
    "                save_log(f\"Progress: {i+1}/{len(list(sample_indices))} samples completed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            save_log(f\"Error at sample {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Ïã§Ìóò ÏãúÏûë: 10Í∞ú ÏÉòÌîå ÌÖåÏä§Ìä∏\n",
    "# ============================================\n",
    "save_log(\"=\" * 50)\n",
    "save_log(\"Starting experiment: 10 samples test\")\n",
    "save_log(\"=\" * 50)\n",
    "\n",
    "results_10 = run_full_comparison(\n",
    "    df, sample_indices=range(10),\n",
    "    driver=driver if NEO4J_CONNECTED else None,\n",
    "    gnn_lpg=gnn_lpg, kge_rdf=kge_rdf, device=DEVICE,\n",
    "    save_every=5\n",
    ")\n",
    "\n",
    "save_checkpoint(results_10, \"results_10_final\")\n",
    "save_log(f\"‚úÖ Completed 10 samples test!\")\n",
    "display(results_10[['question', 'response_llm_only', 'response_text_rag']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Full Evaluation (50 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Î≥∏Í≤© Ïã§Ìóò: 50Í∞ú ÏÉòÌîå\n",
    "# ============================================\n",
    "EVAL_SAMPLES = 50\n",
    "\n",
    "save_log(\"=\" * 50)\n",
    "save_log(f\"Starting full evaluation: {EVAL_SAMPLES} samples\")\n",
    "save_log(\"=\" * 50)\n",
    "\n",
    "results_50 = run_full_comparison(\n",
    "    df, sample_indices=range(min(EVAL_SAMPLES, len(df))),\n",
    "    driver=driver if NEO4J_CONNECTED else None,\n",
    "    gnn_lpg=gnn_lpg, kge_rdf=kge_rdf, device=DEVICE,\n",
    "    save_every=10\n",
    ")\n",
    "\n",
    "save_checkpoint(results_50, \"results_50_final\")\n",
    "save_log(f\"‚úÖ Completed {len(results_50)} samples evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ÏµúÏ¢Ö Í≤∞Í≥º ÏÇ¨Ïö©\n",
    "final_results = results_50 if 'results_50' in dir() else results_10\n",
    "\n",
    "print(f\"\\nTotal samples: {len(final_results)}\")\n",
    "print(f\"\\nCategories:\")\n",
    "print(final_results['category'].value_counts())\n",
    "\n",
    "print(f\"\\n--- Response Lengths ---\")\n",
    "for method, name in [('response_llm_only', 'LLM Only'), ('response_text_rag', 'Text RAG'), \n",
    "                     ('response_graph_lpg', 'Graph LPG'), ('response_graph_rdf', 'Graph RDF')]:\n",
    "    if method in final_results.columns:\n",
    "        avg_len = final_results[method].str.len().mean()\n",
    "        print(f\"  {name}: {avg_len:.0f} chars\")\n",
    "\n",
    "if NEO4J_CONNECTED:\n",
    "    print(f\"\\n--- Graph Statistics ---\")\n",
    "    print(f\"  LPG - Avg nodes: {final_results['lpg_nodes'].mean():.1f}, Avg edges: {final_results['lpg_edges'].mean():.1f}\")\n",
    "    print(f\"  RDF - Avg nodes: {final_results['rdf_nodes'].mean():.1f}, Avg edges: {final_results['rdf_edges'].mean():.1f}\")\n",
    "\n",
    "# Summary Ï†ÄÏû•\n",
    "summary = {\n",
    "    'total_samples': len(final_results),\n",
    "    'avg_len_llm_only': final_results['response_llm_only'].str.len().mean(),\n",
    "    'avg_len_text_rag': final_results['response_text_rag'].str.len().mean(),\n",
    "    'neo4j_connected': NEO4J_CONNECTED,\n",
    "    'experiment_id': EXPERIMENT_ID,\n",
    "    'finish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(f\"{RESULT_DIR}/{EXPERIMENT_ID}_summary.csv\", index=False)\n",
    "save_log(f\"Summary saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞úÎ≥Ñ Í≤∞Í≥º ÎπÑÍµê\n",
    "def display_comparison(idx):\n",
    "    row = final_results.iloc[idx]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {idx}: {row['question'][:100]}...\")\n",
    "    print(f\"\\nCategory: {row['category']}\")\n",
    "    print(f\"\\n[Ground Truth] {str(row['ground_truth'])[:200]}...\")\n",
    "    print(f\"\\n[A] LLM Only: {row['response_llm_only'][:200]}...\")\n",
    "    print(f\"\\n[B] Text RAG: {row['response_text_rag'][:200]}...\")\n",
    "    if NEO4J_CONNECTED:\n",
    "        print(f\"\\n[C] Graph LPG ({row['lpg_nodes']} nodes): {row['response_graph_lpg'][:200]}...\")\n",
    "        print(f\"\\n[D] Graph RDF ({row['rdf_nodes']} nodes): {row['response_graph_rdf'][:200]}...\")\n",
    "\n",
    "display_comparison(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Cleanup & Final Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏµúÏ¢Ö Ï†ïÎ¶¨\n",
    "if NEO4J_CONNECTED:\n",
    "    driver.close()\n",
    "    save_log(\"Neo4j connection closed\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "save_log(\"GPU memory cleared\")\n",
    "\n",
    "save_log(f\"\\n{'='*50}\")\n",
    "save_log(f\"üéâ EXPERIMENT COMPLETED!\")\n",
    "save_log(f\"{'='*50}\")\n",
    "save_log(f\"Results saved to: {RESULT_DIR}\")\n",
    "save_log(f\"Experiment ID: {EXPERIMENT_ID}\")\n",
    "save_log(f\"Files:\")\n",
    "for f in os.listdir(RESULT_DIR):\n",
    "    if EXPERIMENT_ID in f:\n",
    "        save_log(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\n\\nüåô Good night! Check results in Google Drive tomorrow.\")\n",
    "print(f\"Path: {RESULT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}