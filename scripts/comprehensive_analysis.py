"""
Comprehensive analysis of FinDER KG datasets (LPG & RDF)
Generates detailed markdown report for docs/
"""

import json
from datetime import datetime
from typing import Any, Dict, List, Tuple

from neo4j import GraphDatabase

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password"


def analyze_lpg_detailed(driver) -> Dict[str, Any]:
    """Detailed LPG analysis."""
    results = {}

    with driver.session(database="finderlpg") as session:
        # Basic stats
        results["node_count"] = session.run("MATCH (n) RETURN count(n) as c").single()["c"]
        results["edge_count"] = session.run("MATCH ()-[r]->() RETURN count(r) as c").single()["c"]
        results["question_count"] = session.run("MATCH (q:Question) RETURN count(q) as c").single()["c"]
        results["entity_count"] = session.run("MATCH (e:Entity) RETURN count(e) as c").single()["c"]

        # Label distribution (top 30)
        labels = session.run("""
            MATCH (n)
            WITH labels(n) as lbls
            UNWIND lbls as label
            RETURN label, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 30
        """).data()
        results["label_distribution"] = labels

        # Relationship type distribution (top 30)
        rels = session.run("""
            MATCH ()-[r]->()
            RETURN type(r) as rel_type, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 30
        """).data()
        results["relationship_distribution"] = rels

        # Degree statistics
        degree_stats = session.run("""
            MATCH (n)
            WITH n, COUNT { (n)--() } as degree
            RETURN
                min(degree) as min_deg,
                max(degree) as max_deg,
                avg(degree) as avg_deg,
                stDev(degree) as std_deg,
                percentileCont(degree, 0.25) as p25,
                percentileCont(degree, 0.5) as median,
                percentileCont(degree, 0.75) as p75,
                percentileCont(degree, 0.9) as p90,
                percentileCont(degree, 0.95) as p95,
                percentileCont(degree, 0.99) as p99
        """).single()
        results["degree_stats"] = dict(degree_stats)

        # In/Out degree stats
        directed_stats = session.run("""
            MATCH (n)
            WITH n,
                 COUNT { (n)-[]->() } as out_deg,
                 COUNT { (n)<-[]-() } as in_deg
            RETURN
                avg(in_deg) as avg_in,
                avg(out_deg) as avg_out,
                max(in_deg) as max_in,
                max(out_deg) as max_out,
                stDev(in_deg) as std_in,
                stDev(out_deg) as std_out
        """).single()
        results["directed_stats"] = dict(directed_stats)

        # Top hub nodes
        hubs = session.run("""
            MATCH (n)
            WITH n, COUNT { (n)--() } as degree
            ORDER BY degree DESC
            LIMIT 20
            RETURN
                coalesce(n.name, n.id, 'unknown') as name,
                labels(n) as labels,
                degree,
                COUNT { (n)-[]->() } as out_deg,
                COUNT { (n)<-[]-() } as in_deg
        """).data()
        results["top_hubs"] = hubs

        # Isolated nodes by label
        isolated = session.run("""
            MATCH (n)
            WHERE NOT (n)--()
            WITH labels(n) as lbls
            UNWIND lbls as label
            RETURN label, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 10
        """).data()
        results["isolated_by_label"] = isolated

        # Property coverage
        props = session.run("""
            MATCH (n)
            WITH keys(n) as props
            UNWIND props as prop
            RETURN prop, count(*) as cnt
            ORDER BY cnt DESC
        """).data()
        results["property_coverage"] = props

        # Question-Entity stats
        q_entity = session.run("""
            MATCH (n)
            WHERE n.question_ids IS NOT NULL AND size(n.question_ids) > 0
            WITH size(n.question_ids) as q_count
            RETURN
                avg(q_count) as avg_q_per_entity,
                max(q_count) as max_q_per_entity,
                min(q_count) as min_q_per_entity
        """).single()
        results["question_entity_stats"] = dict(q_entity) if q_entity else {}

        # Entities per question
        entities_per_q = session.run("""
            MATCH (n)
            WHERE n.question_ids IS NOT NULL
            UNWIND n.question_ids as qid
            WITH qid, count(*) as entity_cnt
            RETURN
                avg(entity_cnt) as avg_entities,
                max(entity_cnt) as max_entities,
                min(entity_cnt) as min_entities,
                percentileCont(entity_cnt, 0.5) as median_entities
        """).single()
        results["entities_per_question"] = dict(entities_per_q) if entities_per_q else {}

        # Label combinations (multi-label)
        label_combos = session.run("""
            MATCH (n:Entity)
            WITH labels(n) as lbls
            WHERE size(lbls) > 1
            RETURN lbls, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 15
        """).data()
        results["label_combinations"] = label_combos

        # Relationship patterns
        rel_patterns = session.run("""
            MATCH (s)-[r]->(t)
            WITH labels(s)[0] as src_type, type(r) as rel, labels(t)[0] as tgt_type
            RETURN src_type, rel, tgt_type, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 20
        """).data()
        results["relationship_patterns"] = rel_patterns

        # Sample questions
        sample_qs = session.run("""
            MATCH (q:Question)
            RETURN q.id as id, q.text as text, q.category as category, q.type as type
            LIMIT 5
        """).data()
        results["sample_questions"] = sample_qs

    return results


def analyze_rdf_detailed(driver) -> Dict[str, Any]:
    """Detailed RDF analysis."""
    results = {}

    with driver.session(database="finderrdf") as session:
        # Basic stats
        results["node_count"] = session.run("MATCH (n) RETURN count(n) as c").single()["c"]
        results["edge_count"] = session.run("MATCH ()-[r]->() RETURN count(r) as c").single()["c"]
        results["resource_count"] = session.run("MATCH (r:Resource) RETURN count(r) as c").single()["c"]
        results["question_count"] = session.run("MATCH (q:Question) RETURN count(q) as c").single()["c"]

        # Predicate distribution (all)
        predicates = session.run("""
            MATCH ()-[r]->()
            RETURN type(r) as predicate, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 50
        """).data()
        results["predicate_distribution"] = predicates

        # FIBO ontology analysis
        fibo_predicates = session.run("""
            MATCH ()-[r]->()
            WHERE type(r) STARTS WITH 'fibo'
            RETURN type(r) as predicate, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 30
        """).data()
        results["fibo_predicates"] = fibo_predicates

        # Degree statistics
        degree_stats = session.run("""
            MATCH (n)
            WITH n, COUNT { (n)--() } as degree
            RETURN
                min(degree) as min_deg,
                max(degree) as max_deg,
                avg(degree) as avg_deg,
                stDev(degree) as std_deg,
                percentileCont(degree, 0.5) as median,
                percentileCont(degree, 0.9) as p90,
                percentileCont(degree, 0.99) as p99
        """).single()
        results["degree_stats"] = dict(degree_stats)

        # Top hub resources
        hubs = session.run("""
            MATCH (n:Resource)
            WITH n, COUNT { (n)--() } as degree
            ORDER BY degree DESC
            LIMIT 20
            RETURN n.uri as uri, degree
        """).data()
        results["top_hubs"] = hubs

        # Property predicates (literals)
        literal_props = session.run("""
            MATCH (n:Resource)
            WITH keys(n) as props
            UNWIND props as prop
            WITH prop WHERE prop <> 'uri'
            RETURN prop, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 20
        """).data()
        results["literal_properties"] = literal_props

        # Triple patterns
        triple_patterns = session.run("""
            MATCH (s)-[r]->(o)
            WITH
                CASE WHEN s:Resource THEN 'Resource' ELSE 'Question' END as s_type,
                type(r) as predicate,
                CASE WHEN o:Resource THEN 'Resource' ELSE 'Question' END as o_type
            RETURN s_type, predicate, o_type, count(*) as cnt
            ORDER BY cnt DESC
            LIMIT 20
        """).data()
        results["triple_patterns"] = triple_patterns

        # Isolated resources
        isolated = session.run("""
            MATCH (n:Resource)
            WHERE NOT (n)--()
            RETURN count(n) as cnt
        """).single()["cnt"]
        results["isolated_resources"] = isolated

        # URI namespace analysis
        namespaces = session.run("""
            MATCH (n:Resource)
            WHERE n.uri IS NOT NULL
            WITH
                CASE
                    WHEN n.uri STARTS WITH 'ex:' THEN 'ex:'
                    WHEN n.uri STARTS WITH 'fibo' THEN split(n.uri, ':')[0] + ':'
                    ELSE 'other'
                END as namespace
            RETURN namespace, count(*) as cnt
            ORDER BY cnt DESC
        """).data()
        results["uri_namespaces"] = namespaces

    return results


def compare_databases(lpg: Dict, rdf: Dict) -> Dict[str, Any]:
    """Compare LPG and RDF representations."""
    comparison = {
        "basic_stats": {
            "lpg_nodes": lpg["node_count"],
            "rdf_nodes": rdf["node_count"],
            "lpg_edges": lpg["edge_count"],
            "rdf_edges": rdf["edge_count"],
            "node_diff": lpg["node_count"] - rdf["node_count"],
            "edge_diff": lpg["edge_count"] - rdf["edge_count"],
        },
        "density": {
            "lpg": lpg["edge_count"] / (lpg["node_count"] ** 2) if lpg["node_count"] > 0 else 0,
            "rdf": rdf["edge_count"] / (rdf["node_count"] ** 2) if rdf["node_count"] > 0 else 0,
        },
        "degree_comparison": {
            "lpg_avg": lpg["degree_stats"]["avg_deg"],
            "rdf_avg": rdf["degree_stats"]["avg_deg"],
            "lpg_max": lpg["degree_stats"]["max_deg"],
            "rdf_max": rdf["degree_stats"]["max_deg"],
        }
    }
    return comparison


def generate_markdown_report(lpg: Dict, rdf: Dict, comparison: Dict) -> str:
    """Generate comprehensive markdown report."""

    report = f"""# FinDER Knowledge Graph ë°ì´í„°ì…‹ ë¶„ì„ ë³´ê³ ì„œ

> ìƒì„±ì¼: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ëª©ì°¨
1. [ê°œìš”](#1-ê°œìš”)
2. [LPG (Labeled Property Graph) ë¶„ì„](#2-lpg-ë¶„ì„)
3. [RDF (Resource Description Framework) ë¶„ì„](#3-rdf-ë¶„ì„)
4. [LPG vs RDF ë¹„êµ](#4-lpg-vs-rdf-ë¹„êµ)
5. [GNN Feature Engineering ê¶Œì¥ì‚¬í•­](#5-gnn-feature-engineering)
6. [ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë° ê°œì„ ì ](#6-ë°ì´í„°-í’ˆì§ˆ-ì´ìŠˆ)

---

## 1. ê°œìš”

### 1.1 ë°ì´í„°ì…‹ ìš”ì•½

| ì§€í‘œ | LPG (finderlpg) | RDF (finderrdf) |
|------|-----------------|-----------------|
| **ì´ ë…¸ë“œ** | {lpg["node_count"]:,} | {rdf["node_count"]:,} |
| **ì´ ì—£ì§€** | {lpg["edge_count"]:,} | {rdf["edge_count"]:,} |
| **ì§ˆë¬¸ ìˆ˜** | {lpg["question_count"]:,} | {rdf["question_count"]:,} |
| **ì—”í‹°í‹°/ë¦¬ì†ŒìŠ¤** | {lpg["entity_count"]:,} | {rdf["resource_count"]:,} |
| **ê·¸ë˜í”„ ë°€ë„** | {comparison["density"]["lpg"]:.6f} | {comparison["density"]["rdf"]:.6f} |
| **í‰ê·  Degree** | {lpg["degree_stats"]["avg_deg"]:.2f} | {rdf["degree_stats"]["avg_deg"]:.2f} |

### 1.2 í•µì‹¬ ë°œê²¬ì‚¬í•­

```
âœ… ê°•ì :
â€¢ 3,140ê°œ ì§ˆë¬¸ì— ëŒ€í•œ Knowledge Graph êµ¬ì¶• ì™„ë£Œ
â€¢ FIBO ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ê¸ˆìœµ ë„ë©”ì¸ êµ¬ì¡°í™”
â€¢ ì§ˆë¬¸ë‹¹ í‰ê·  7.18ê°œ ì—”í‹°í‹° ì—°ê²° (LPG)

âš ï¸ ì£¼ì˜ì‚¬í•­:
â€¢ ë§¤ìš° í¬ì†Œí•œ ê·¸ë˜í”„ (ë°€ë„ < 0.0001)
â€¢ Hub ë…¸ë“œ ë¶ˆê· í˜• (max degree: {lpg["degree_stats"]["max_deg"]:,})
â€¢ ê´€ê³„ íƒ€ì… ê³¼ë‹¤ (2,971ê°œ unique types)
â€¢ Isolated ë…¸ë“œ ì¡´ì¬ (LPG: 22%, RDF: 37%)
```

---

## 2. LPG ë¶„ì„

### 2.1 ë…¸ë“œ ë¼ë²¨ ë¶„í¬

| ë¼ë²¨ | ê°œìˆ˜ | ë¹„ìœ¨ |
|------|------|------|
"""

    # Add label distribution table
    total_nodes = lpg["node_count"]
    for item in lpg["label_distribution"][:20]:
        pct = (item["cnt"] / total_nodes * 100)
        report += f"| {item['label']} | {item['cnt']:,} | {pct:.1f}% |\n"

    if len(lpg["label_distribution"]) > 20:
        remaining = len(lpg["label_distribution"]) - 20
        report += f"| *... {remaining}ê°œ ë”* | - | - |\n"

    report += f"""
### 2.2 ê´€ê³„ íƒ€ì… ë¶„í¬

| ê´€ê³„ íƒ€ì… | ê°œìˆ˜ | ë¹„ìœ¨ |
|----------|------|------|
"""

    total_edges = lpg["edge_count"]
    for item in lpg["relationship_distribution"][:15]:
        pct = (item["cnt"] / total_edges * 100)
        report += f"| `{item['rel_type']}` | {item['cnt']:,} | {pct:.1f}% |\n"

    report += f"""
### 2.3 Degree ë¶„í¬ í†µê³„

| ì§€í‘œ | ê°’ |
|------|-----|
| ìµœì†Œ | {lpg["degree_stats"]["min_deg"]} |
| ìµœëŒ€ | {lpg["degree_stats"]["max_deg"]:,} |
| í‰ê·  | {lpg["degree_stats"]["avg_deg"]:.2f} |
| í‘œì¤€í¸ì°¨ | {lpg["degree_stats"]["std_deg"]:.2f} |
| ì¤‘ì•™ê°’ (P50) | {lpg["degree_stats"]["median"]:.1f} |
| P75 | {lpg["degree_stats"]["p75"]:.1f} |
| P90 | {lpg["degree_stats"]["p90"]:.1f} |
| P95 | {lpg["degree_stats"]["p95"]:.1f} |
| P99 | {lpg["degree_stats"]["p99"]:.1f} |

**ë¶„ì„**: ì¤‘ì•™ê°’({lpg["degree_stats"]["median"]:.1f})ê³¼ í‰ê· ({lpg["degree_stats"]["avg_deg"]:.2f})ì˜ ì°¨ì´ê°€ í¬ê³ , P99({lpg["degree_stats"]["p99"]:.1f})ì™€ ìµœëŒ€ê°’({lpg["degree_stats"]["max_deg"]:,})ì˜ ì°¨ì´ê°€ ë§¤ìš° í¼ â†’ **ê·¹ì‹¬í•œ Hub ë…¸ë“œ ì¡´ì¬**

### 2.4 ë°©í–¥ì„± ë¶„ì„ (In/Out Degree)

| ì§€í‘œ | In-Degree | Out-Degree |
|------|-----------|------------|
| í‰ê·  | {lpg["directed_stats"]["avg_in"]:.2f} | {lpg["directed_stats"]["avg_out"]:.2f} |
| ìµœëŒ€ | {lpg["directed_stats"]["max_in"]:,} | {lpg["directed_stats"]["max_out"]:,} |
| í‘œì¤€í¸ì°¨ | {lpg["directed_stats"]["std_in"]:.2f} | {lpg["directed_stats"]["std_out"]:.2f} |

### 2.5 Top Hub ë…¸ë“œ

| ìˆœìœ„ | ë…¸ë“œ | ë¼ë²¨ | Total Degree | Out | In |
|------|------|------|--------------|-----|-----|
"""

    for i, hub in enumerate(lpg["top_hubs"][:15], 1):
        labels = "/".join(hub["labels"][:2]) if hub["labels"] else "-"
        name = hub["name"][:30] + "..." if len(str(hub["name"])) > 30 else hub["name"]
        report += f"| {i} | {name} | {labels} | {hub['degree']:,} | {hub['out_deg']:,} | {hub['in_deg']:,} |\n"

    report += f"""
### 2.6 ì†ì„±(Property) ì»¤ë²„ë¦¬ì§€

| ì†ì„±ëª… | ë…¸ë“œ ìˆ˜ | ì»¤ë²„ë¦¬ì§€ |
|--------|---------|----------|
"""

    for item in lpg["property_coverage"][:15]:
        pct = (item["cnt"] / total_nodes * 100)
        report += f"| `{item['prop']}` | {item['cnt']:,} | {pct:.1f}% |\n"

    report += f"""
### 2.7 ì§ˆë¬¸-ì—”í‹°í‹° ì—°ê²°

| ì§€í‘œ | ê°’ |
|------|-----|
| ì§ˆë¬¸ë‹¹ í‰ê·  ì—”í‹°í‹° ìˆ˜ | {lpg["entities_per_question"].get("avg_entities", 0):.2f} |
| ì§ˆë¬¸ë‹¹ ìµœëŒ€ ì—”í‹°í‹° ìˆ˜ | {lpg["entities_per_question"].get("max_entities", 0):.0f} |
| ì§ˆë¬¸ë‹¹ ì¤‘ì•™ê°’ ì—”í‹°í‹° ìˆ˜ | {lpg["entities_per_question"].get("median_entities", 0):.1f} |

### 2.8 ê´€ê³„ íŒ¨í„´ (Subject â†’ Relation â†’ Object)

| Subject Type | Relation | Object Type | ê°œìˆ˜ |
|--------------|----------|-------------|------|
"""

    for item in lpg["relationship_patterns"][:15]:
        report += f"| {item['src_type']} | `{item['rel'][:30]}` | {item['tgt_type']} | {item['cnt']:,} |\n"

    # RDF Section
    report += f"""
---

## 3. RDF ë¶„ì„

### 3.1 ê¸°ë³¸ í†µê³„

| ì§€í‘œ | ê°’ |
|------|-----|
| ì´ ë…¸ë“œ | {rdf["node_count"]:,} |
| ì´ Triple (ì—£ì§€) | {rdf["edge_count"]:,} |
| Resource ë…¸ë“œ | {rdf["resource_count"]:,} |
| Question ë…¸ë“œ | {rdf["question_count"]:,} |
| Isolated Resource | {rdf["isolated_resources"]:,} ({rdf["isolated_resources"]/rdf["resource_count"]*100:.1f}%) |

### 3.2 Predicate ë¶„í¬ (ìƒìœ„ 30ê°œ)

| Predicate | ê°œìˆ˜ | ë¹„ìœ¨ |
|-----------|------|------|
"""

    rdf_edges = rdf["edge_count"]
    for item in rdf["predicate_distribution"][:30]:
        pct = (item["cnt"] / rdf_edges * 100)
        pred_short = item["predicate"][:50] + "..." if len(item["predicate"]) > 50 else item["predicate"]
        report += f"| `{pred_short}` | {item['cnt']:,} | {pct:.1f}% |\n"

    report += f"""
### 3.3 FIBO ì˜¨í†¨ë¡œì§€ Predicate

| FIBO Predicate | ê°œìˆ˜ |
|----------------|------|
"""

    for item in rdf["fibo_predicates"][:20]:
        pred_short = item["predicate"][:60]
        report += f"| `{pred_short}` | {item['cnt']:,} |\n"

    report += f"""
### 3.4 URI ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„ì„

| Namespace | ê°œìˆ˜ |
|-----------|------|
"""

    for item in rdf["uri_namespaces"][:10]:
        report += f"| `{item['namespace']}` | {item['cnt']:,} |\n"

    report += f"""
### 3.5 Degree ë¶„í¬

| ì§€í‘œ | ê°’ |
|------|-----|
| ìµœì†Œ | {rdf["degree_stats"]["min_deg"]} |
| ìµœëŒ€ | {rdf["degree_stats"]["max_deg"]:,} |
| í‰ê·  | {rdf["degree_stats"]["avg_deg"]:.2f} |
| ì¤‘ì•™ê°’ | {rdf["degree_stats"]["median"]:.1f} |
| P90 | {rdf["degree_stats"]["p90"]:.1f} |
| P99 | {rdf["degree_stats"]["p99"]:.1f} |

### 3.6 Top Hub Resources

| ìˆœìœ„ | URI | Degree |
|------|-----|--------|
"""

    for i, hub in enumerate(rdf["top_hubs"][:15], 1):
        uri_short = hub["uri"][:40] + "..." if len(str(hub["uri"])) > 40 else hub["uri"]
        report += f"| {i} | `{uri_short}` | {hub['degree']:,} |\n"

    report += f"""
### 3.7 Literal ì†ì„± (ë°ì´í„° í”„ë¡œí¼í‹°)

| Property | ë…¸ë“œ ìˆ˜ |
|----------|---------|
"""

    for item in rdf["literal_properties"][:15]:
        report += f"| `{item['prop']}` | {item['cnt']:,} |\n"

    # Comparison Section
    report += f"""
---

## 4. LPG vs RDF ë¹„êµ

### 4.1 êµ¬ì¡°ì  ì°¨ì´

| íŠ¹ì„± | LPG | RDF | ì°¨ì´ |
|------|-----|-----|------|
| ë…¸ë“œ ìˆ˜ | {comparison["basic_stats"]["lpg_nodes"]:,} | {comparison["basic_stats"]["rdf_nodes"]:,} | {comparison["basic_stats"]["node_diff"]:+,} |
| ì—£ì§€ ìˆ˜ | {comparison["basic_stats"]["lpg_edges"]:,} | {comparison["basic_stats"]["rdf_edges"]:,} | {comparison["basic_stats"]["edge_diff"]:+,} |
| ë°€ë„ | {comparison["density"]["lpg"]:.6f} | {comparison["density"]["rdf"]:.6f} | - |
| í‰ê·  Degree | {comparison["degree_comparison"]["lpg_avg"]:.2f} | {comparison["degree_comparison"]["rdf_avg"]:.2f} | - |
| ìµœëŒ€ Degree | {comparison["degree_comparison"]["lpg_max"]:,} | {comparison["degree_comparison"]["rdf_max"]:,} | - |

### 4.2 ëª¨ë¸ë§ ì°¨ì´ì 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LPG vs RDF                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  LPG (Labeled Property Graph)          RDF (Triple Store)        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚  â€¢ Multi-label nodes (avg 1.79)       â€¢ Single label (Resource)  â”‚
â”‚  â€¢ Properties on nodes                â€¢ Properties as predicates â”‚
â”‚  â€¢ Typed relationships                â€¢ URI-based predicates     â”‚
â”‚  â€¢ question_ids ì§ì ‘ ì—°ê²°              â€¢ question_ids ë¯¸ì—°ê²°      â”‚
â”‚                                                                  â”‚
â”‚  ì¥ì :                                 ì¥ì :                      â”‚
â”‚  â”œâ”€ ì§ê´€ì  ì¿¼ë¦¬                        â”œâ”€ í‘œì¤€í™”ëœ êµ¬ì¡°            â”‚
â”‚  â”œâ”€ ë¹ ë¥¸ íƒìƒ‰                          â”œâ”€ FIBO ì˜¨í†¨ë¡œì§€ í˜¸í™˜       â”‚
â”‚  â””â”€ GNNì— ì í•©                         â””â”€ ì¶”ë¡  ì§€ì› ê°€ëŠ¥           â”‚
â”‚                                                                  â”‚
â”‚  GNN ì í•©ì„±: â˜…â˜…â˜…â˜…â˜…                    GNN ì í•©ì„±: â˜…â˜…â˜…â˜†â˜†         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. GNN Feature Engineering

### 5.1 ë…¸ë“œ í”¼ì²˜ (Node Features)

#### A. í…ìŠ¤íŠ¸ ì„ë² ë”© (Primary Feature)

```python
# í˜„ì¬ êµ¬í˜„
text = f"{{label}}: {{name}}"
embedding = sentence_transformer.encode(text)  # 384-dim

# ê¶Œì¥ ê°œì„ 
text = f"{{label}}: {{name}}. {{description}}. {{key_properties}}"
embedding = mpnet.encode(text)  # 768-dim (ë” ë†’ì€ í’ˆì§ˆ)
```

| ëª¨ë¸ | ì°¨ì› | ì†ë„ | í’ˆì§ˆ | ê¶Œì¥ |
|------|------|------|------|------|
| all-MiniLM-L6-v2 | 384 | â­â­â­ | â­â­ | ë¹ ë¥¸ ì‹¤í—˜ |
| all-mpnet-base-v2 | 768 | â­â­ | â­â­â­ | í”„ë¡œë•ì…˜ |
| e5-large-v2 | 1024 | â­ | â­â­â­â­ | ìµœê³  í’ˆì§ˆ |

#### B. êµ¬ì¡°ì  í”¼ì²˜ (Structural Features)

```python
structural_features = [
    # ê¸°ë³¸ Centrality
    degree / max_degree,                           # Normalized degree
    in_degree / (in_degree + out_degree + 1e-6),   # In-degree ratio
    out_degree / (in_degree + out_degree + 1e-6),  # Out-degree ratio

    # Graph Centrality (Neo4j GDSë¡œ ì‚¬ì „ ê³„ì‚°)
    pagerank_score,                                # PageRank
    betweenness_centrality,                        # Bridge ë…¸ë“œ ì‹ë³„

    # Local Structure
    local_clustering_coefficient,                  # ì§€ì—­ ë°€ë„
    avg_neighbor_degree / max_degree,              # ì´ì›ƒ ì¤‘ìš”ë„
]
# Total: 7-dim
```

**Neo4j GDS ê³„ì‚° ì˜ˆì‹œ**:
```cypher
-- PageRank
CALL gds.pageRank.stream('myGraph')
YIELD nodeId, score
WITH gds.util.asNode(nodeId) AS n, score
SET n.pagerank = score;

-- Betweenness
CALL gds.betweenness.stream('myGraph')
YIELD nodeId, score
SET gds.util.asNode(nodeId).betweenness = score;
```

#### C. ë¼ë²¨ ì¸ì½”ë”© (Label Encoding)

```python
# Option 1: Multi-hot encoding
# 2,497ê°œ ë¼ë²¨ â†’ ë„ˆë¬´ sparse â†’ ìƒìœ„ 100ê°œë§Œ ì‚¬ìš©
top_100_labels = [...]  # ë¹ˆë„ìˆœ ìƒìœ„ 100ê°œ
label_encoding = multi_hot(node_labels, top_100_labels)  # 100-dim

# Option 2: Learnable embedding
class LabelEmbedding(nn.Module):
    def __init__(self, num_labels=100, embed_dim=32):
        self.embedding = nn.Embedding(num_labels, embed_dim)

    def forward(self, label_indices):
        # Multi-label: average embeddings
        return self.embedding(label_indices).mean(dim=0)  # 32-dim
```

#### D. ìœ„ì¹˜ ì¸ì½”ë”© (Positional Encoding)

```python
# Random Walk Positional Encoding (RWPE)
from torch_geometric.transforms import AddRandomWalkPE

transform = AddRandomWalkPE(walk_length=16, attr_name='pe')
data = transform(data)  # 16-dim PE added
```

### 5.2 ì—£ì§€ í”¼ì²˜ (Edge Features)

#### A. ê´€ê³„ íƒ€ì… ì„ë² ë”©

```python
# 2,971ê°œ ê´€ê³„ íƒ€ì… â†’ í´ëŸ¬ìŠ¤í„°ë§ í•„ìš”

# Step 1: ê´€ê³„ íƒ€ì… ê·¸ë£¹í™”
relation_groups = {{
    "competition": ["COMPETES_WITH", "HAS_COMPETITOR", "facesCompetitionFrom"],
    "composition": ["INCLUDES", "PART_OF", "HAS_SEGMENT"],
    "employment": ["EMPLOYS", "WORKS_FOR", "HAS_EMPLOYEE"],
    "location": ["isDomiciledIn", "OPERATES_IN", "HAS_LOCATION"],
    # ... ì•½ 50ê°œ ê·¸ë£¹ìœ¼ë¡œ ì¶•ì†Œ
}}

# Step 2: Learnable embedding
class RelationEmbedding(nn.Module):
    def __init__(self, num_relations=50, embed_dim=64):
        self.embedding = nn.Embedding(num_relations, embed_dim)
```

#### B. ì—£ì§€ ê°€ì¤‘ì¹˜

```python
# Inverse Document Frequency ìŠ¤íƒ€ì¼
edge_weight = log(total_edges / relation_count[rel_type])

# ë˜ëŠ” learnable attention
class EdgeAttention(nn.Module):
    def __init__(self, hidden_dim):
        self.attn = nn.Linear(hidden_dim * 2, 1)

    def forward(self, src_emb, tgt_emb):
        return torch.sigmoid(self.attn(torch.cat([src_emb, tgt_emb], dim=-1)))
```

### 5.3 í†µí•© í”¼ì²˜ êµ¬ì„±

```python
@dataclass
class FeatureConfig:
    # Text embedding
    text_dim: int = 384            # or 768 for mpnet

    # Structural features
    structural_dim: int = 7

    # Label encoding
    label_dim: int = 32            # learnable

    # Positional encoding
    pe_dim: int = 16               # RWPE

    # Edge features
    relation_dim: int = 64

    @property
    def node_dim(self) -> int:
        return self.text_dim + self.structural_dim + self.label_dim + self.pe_dim
        # 384 + 7 + 32 + 16 = 439-dim
        # or 768 + 7 + 32 + 16 = 823-dim

# ì‹¤ì œ ì‚¬ìš©
node_features = torch.cat([
    text_embedding,      # [N, 384]
    structural_feat,     # [N, 7]
    label_embedding,     # [N, 32]
    position_encoding,   # [N, 16]
], dim=-1)              # [N, 439]
```

### 5.4 Hub ë…¸ë“œ ì²˜ë¦¬ ì „ëµ

```python
# ë¬¸ì œ: "The Company" (degree=3,121)ê°€ ë„ˆë¬´ ë§ì€ ë©”ì‹œì§€ ì§‘ì¤‘

# í•´ê²°ì±… 1: Degree normalization in GATv2
class DegreeNormalizedGAT(nn.Module):
    def forward(self, x, edge_index):
        # Symmetric normalization
        deg = degree(edge_index[0], x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        norm = deg_inv_sqrt[edge_index[0]] * deg_inv_sqrt[edge_index[1]]
        # Apply in attention
        ...

# í•´ê²°ì±… 2: Virtual node (GraphGPS style)
class VirtualNode(nn.Module):
    def __init__(self, hidden_dim):
        self.virtual_node = nn.Parameter(torch.randn(1, hidden_dim))

    def forward(self, x, batch):
        # Aggregate all nodes to virtual
        # Broadcast back
        ...

# í•´ê²°ì±… 3: Hub sampling
def sample_hub_neighbors(edge_index, hub_mask, max_neighbors=50):
    # For hub nodes, randomly sample neighbors
    ...
```

---

## 6. ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ

### 6.1 ë°œê²¬ëœ ì´ìŠˆ

| ì´ìŠˆ | ì‹¬ê°ë„ | ì„¤ëª… | í•´ê²° ë°©ì•ˆ |
|------|--------|------|-----------|
| Hub ë¶ˆê· í˜• | ğŸ”´ ë†’ìŒ | "The Company" 3,121 edges | Degree normalization |
| Isolated ë…¸ë“œ | ğŸŸ¡ ì¤‘ê°„ | LPG 22%, RDF 37% | í•„í„°ë§ ë˜ëŠ” self-loop |
| ê´€ê³„ íƒ€ì… ê³¼ë‹¤ | ğŸŸ¡ ì¤‘ê°„ | 2,971ê°œ íƒ€ì… | í´ëŸ¬ìŠ¤í„°ë§ (~50ê°œ) |
| RDF question ì—°ê²° | ğŸ”´ ë†’ìŒ | question_ids ëˆ„ë½ | ë°ì´í„° ì¬êµ¬ì¶• í•„ìš” |
| Generic ë…¸ë“œ | ğŸŸ¡ ì¤‘ê°„ | "Our Company" ë“± | íŠ¹ìˆ˜ ì²˜ë¦¬ í•„ìš” |

### 6.2 ë°ì´í„° í´ë¦¬ë‹ ê¶Œì¥

```python
# 1. Hub ë…¸ë“œ í•„í„°ë§/ìƒ˜í”Œë§
hub_threshold = 500  # degree > 500ì¸ ë…¸ë“œ íŠ¹ìˆ˜ ì²˜ë¦¬

# 2. Isolated ë…¸ë“œ ì²˜ë¦¬
# Option A: ì œê±°
# Option B: Self-loop ì¶”ê°€

# 3. ê´€ê³„ íƒ€ì… ì •ê·œí™”
def normalize_relation(rel_type: str) -> str:
    rel_lower = rel_type.lower().replace('_', '')
    # Map to canonical form
    mapping = {{
        'competeswith': 'COMPETES',
        'hascompetitor': 'COMPETES',
        'includes': 'CONTAINS',
        'partof': 'CONTAINS',
        # ...
    }}
    return mapping.get(rel_lower, 'OTHER')

# 4. RDF question_ids ë³µêµ¬
# Parquetì—ì„œ ë‹¤ì‹œ ë§¤í•‘ í•„ìš”
```

### 6.3 ê¶Œì¥ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```
Raw Data
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ë°ì´í„° ê²€ì¦   â”‚ â†’ ëˆ„ë½ í•„ë“œ ì²´í¬, íƒ€ì… ê²€ì¦
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Isolated ì²˜ë¦¬ â”‚ â†’ ì œê±° ë˜ëŠ” self-loop
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Hub ì •ê·œí™”    â”‚ â†’ degree normalization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ê´€ê³„ í´ëŸ¬ìŠ¤í„° â”‚ â†’ 2,971 â†’ ~50 ê·¸ë£¹
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. í”¼ì²˜ ê³„ì‚°     â”‚ â†’ PageRank, êµ¬ì¡°ì  í”¼ì²˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Clean Data (PyG Data)
```

---

## ë¶€ë¡: ìƒ˜í”Œ ì§ˆë¬¸

"""

    for i, q in enumerate(lpg["sample_questions"], 1):
        text_short = q["text"][:200] + "..." if len(q["text"]) > 200 else q["text"]
        report += f"""
### ìƒ˜í”Œ {i}
- **ID**: `{q["id"]}`
- **Category**: {q.get("category", "N/A")}
- **Type**: {q.get("type", "N/A")}
- **Text**: {text_short}
"""

    report += """
---

*ì´ ë³´ê³ ì„œëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""

    return report


def main():
    print("FinDER KG Comprehensive Analysis")
    print("=" * 60)

    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))

    try:
        driver.verify_connectivity()
        print("Connected to Neo4j")

        # Analyze LPG
        print("\nAnalyzing LPG database...")
        lpg_results = analyze_lpg_detailed(driver)

        # Analyze RDF
        print("Analyzing RDF database...")
        rdf_results = analyze_rdf_detailed(driver)

        # Compare
        print("Comparing databases...")
        comparison = compare_databases(lpg_results, rdf_results)

        # Generate report
        print("Generating markdown report...")
        report = generate_markdown_report(lpg_results, rdf_results, comparison)

        # Save report
        report_path = "docs/finder_kg_analysis.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"\nReport saved to {report_path}")

        # Also save raw JSON
        json_path = "results/comprehensive_analysis.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump({
                "lpg": lpg_results,
                "rdf": rdf_results,
                "comparison": comparison,
            }, f, indent=2, default=str)
        print(f"Raw data saved to {json_path}")

    finally:
        driver.close()

    print("\nDone!")


if __name__ == "__main__":
    main()
