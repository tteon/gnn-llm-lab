{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV Cache Offloading Experiment: Context-Type Impact Analysis\n",
    "\n",
    "**Experiment**: 5 context conditions Ã— 50 questions Ã— cold/warm pairs  \n",
    "**Model**: `openai/gpt-oss-120b` (reasoning model) via vLLM + LMCache  \n",
    "**Date**: 2026-02-08  \n",
    "\n",
    "## Research Question\n",
    "\n",
    "> How do different Graph RAG context serialization formats (LPG structured, RDF triples, text references, combined LPG+RDF) affect KV cache reuse efficiency and answer quality in a vLLM + LMCache serving environment?\n",
    "\n",
    "## Experimental Protocol\n",
    "\n",
    "For each (question, condition) pair:\n",
    "1. **Cold run**: fresh API call â†’ measure latency & scrape `/metrics`\n",
    "2. **Wait 3s** for LMCache to store KV cache\n",
    "3. **Warm run**: identical prompt â†’ measure latency & scrape `/metrics`\n",
    "4. **Speedup** = cold_latency / warm_latency\n",
    "\n",
    "### 5 Conditions\n",
    "\n",
    "| Condition | Context Source | Format |\n",
    "|-----------|---------------|--------|\n",
    "| `lpg` | Neo4j finderlpg | GraphFormatter structured |\n",
    "| `rdf` | Neo4j finderrdf | Triple format (raw URIs) |\n",
    "| `text` | Parquet references | Raw text passages |\n",
    "| `lpg_rdf` | Both DBs | Combined LPG + RDF |\n",
    "| `vanilla` | â€” | No context (baseline) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# â”€â”€ Style â”€â”€\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"#0d1117\",\n",
    "    \"axes.facecolor\": \"#161b22\",\n",
    "    \"axes.edgecolor\": \"#30363d\",\n",
    "    \"axes.labelcolor\": \"#c9d1d9\",\n",
    "    \"text.color\": \"#c9d1d9\",\n",
    "    \"xtick.color\": \"#8b949e\",\n",
    "    \"ytick.color\": \"#8b949e\",\n",
    "    \"grid.color\": \"#21262d\",\n",
    "    \"legend.facecolor\": \"#161b22\",\n",
    "    \"legend.edgecolor\": \"#30363d\",\n",
    "    \"figure.dpi\": 120,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.labelsize\": 11,\n",
    "})\n",
    "\n",
    "COLORS = {\n",
    "    \"lpg\": \"#58a6ff\",\n",
    "    \"rdf\": \"#f78166\",\n",
    "    \"text\": \"#3fb950\",\n",
    "    \"lpg_rdf\": \"#d2a8ff\",\n",
    "    \"vanilla\": \"#8b949e\",\n",
    "}\n",
    "COND_ORDER = [\"vanilla\", \"lpg\", \"rdf\", \"text\", \"lpg_rdf\"]\n",
    "COND_LABELS = {\n",
    "    \"vanilla\": \"Vanilla\\n(no context)\",\n",
    "    \"lpg\": \"LPG\\n(structured)\",\n",
    "    \"rdf\": \"RDF\\n(triples)\",\n",
    "    \"text\": \"Text\\n(references)\",\n",
    "    \"lpg_rdf\": \"LPG+RDF\\n(combined)\",\n",
    "}\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load Data â”€â”€\n",
    "RESULT_DIR = Path(\"../results/kvcache_experiment/20260208_091053\")\n",
    "\n",
    "raw = []\n",
    "with open(RESULT_DIR / \"results_log.jsonl\") as f:\n",
    "    for line in f:\n",
    "        raw.append(json.loads(line))\n",
    "\n",
    "print(f\"Total entries: {len(raw)}\")\n",
    "print(f\"By status: {pd.Series([r.get('status') for r in raw]).value_counts().to_dict()}\")\n",
    "\n",
    "# Filter to success only\n",
    "success = [r for r in raw if r.get(\"status\") == \"success\"]\n",
    "\n",
    "# Build DataFrame\n",
    "rows = []\n",
    "for r in success:\n",
    "    ev = r.get(\"eval\", {})\n",
    "    wd = r.get(\"warm_delta\", {})\n",
    "    cd = r.get(\"cold_delta\", {})\n",
    "    rows.append({\n",
    "        \"question_id\": r[\"question_id\"],\n",
    "        \"condition\": r[\"condition\"],\n",
    "        \"cold_latency\": r[\"cold_latency\"],\n",
    "        \"warm_latency\": r[\"warm_latency\"],\n",
    "        \"speedup\": r[\"speedup\"],\n",
    "        \"prompt_tokens\": r[\"prompt_tokens\"],\n",
    "        \"context_length\": r[\"context_length\"],\n",
    "        \"cold_completion_tokens\": r[\"cold_completion_tokens\"],\n",
    "        \"warm_completion_tokens\": r[\"warm_completion_tokens\"],\n",
    "        \"cold_finish_reason\": r[\"cold_finish_reason\"],\n",
    "        \"warm_finish_reason\": r[\"warm_finish_reason\"],\n",
    "        # Quality\n",
    "        \"exact_match\": ev.get(\"exact_match\", 0),\n",
    "        \"substring_match\": ev.get(\"substring_match\", 0),\n",
    "        \"token_f1\": ev.get(\"token_f1\", 0),\n",
    "        \"rouge_1\": ev.get(\"rouge_1\", 0),\n",
    "        \"rouge_2\": ev.get(\"rouge_2\", 0),\n",
    "        \"rouge_l\": ev.get(\"rouge_l\", 0),\n",
    "        # Cache deltas (warm)\n",
    "        \"warm_cache_hit_delta\": wd.get(\"delta_vllm:gpu_prefix_cache_hits_total\", 0),\n",
    "        \"warm_cache_query_delta\": wd.get(\"delta_vllm:gpu_prefix_cache_queries_total\", 0),\n",
    "        \"warm_cache_hitrate_delta\": wd.get(\"delta_vllm:gpu_prefix_cache_hit_rate\", 0),\n",
    "        \"warm_elapsed\": wd.get(\"elapsed_sec\", 0),\n",
    "        \"cold_elapsed\": cd.get(\"elapsed_sec\", 0),\n",
    "        # vLLM internal latency delta\n",
    "        \"cold_avg_latency_delta\": cd.get(\"delta_vllm:avg_latency\", 0),\n",
    "        \"warm_avg_latency_delta\": wd.get(\"delta_vllm:avg_latency\", 0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"condition\"] = pd.Categorical(df[\"condition\"], categories=COND_ORDER, ordered=True)\n",
    "df = df.sort_values(\"condition\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataFrame: {df.shape[0]} rows Ã— {df.shape[1]} cols\")\n",
    "print(f\"Conditions: {df['condition'].value_counts().sort_index().to_dict()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Overview Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Summary Table â”€â”€\n",
    "summary = df.groupby(\"condition\", observed=True).agg(\n",
    "    n=(\"speedup\", \"count\"),\n",
    "    cold_lat_mean=(\"cold_latency\", \"mean\"),\n",
    "    cold_lat_std=(\"cold_latency\", \"std\"),\n",
    "    warm_lat_mean=(\"warm_latency\", \"mean\"),\n",
    "    warm_lat_std=(\"warm_latency\", \"std\"),\n",
    "    speedup_mean=(\"speedup\", \"mean\"),\n",
    "    speedup_std=(\"speedup\", \"std\"),\n",
    "    prompt_tok_mean=(\"prompt_tokens\", \"mean\"),\n",
    "    ctx_len_mean=(\"context_length\", \"mean\"),\n",
    "    f1_mean=(\"token_f1\", \"mean\"),\n",
    "    rouge_l_mean=(\"rouge_l\", \"mean\"),\n",
    "    substr_match_mean=(\"substring_match\", \"mean\"),\n",
    ").round(3)\n",
    "\n",
    "# Format for display\n",
    "display_df = summary.copy()\n",
    "display_df[\"cold_latency\"] = display_df.apply(lambda r: f\"{r.cold_lat_mean:.2f} Â± {r.cold_lat_std:.2f}\", axis=1)\n",
    "display_df[\"warm_latency\"] = display_df.apply(lambda r: f\"{r.warm_lat_mean:.2f} Â± {r.warm_lat_std:.2f}\", axis=1)\n",
    "display_df[\"speedup\"] = display_df.apply(lambda r: f\"{r.speedup_mean:.2f} Â± {r.speedup_std:.2f}x\", axis=1)\n",
    "display_df = display_df[[\"n\", \"cold_latency\", \"warm_latency\", \"speedup\",\n",
    "                          \"prompt_tok_mean\", \"ctx_len_mean\", \"f1_mean\", \"rouge_l_mean\", \"substr_match_mean\"]]\n",
    "display_df.columns = [\"N\", \"Cold Latency (s)\", \"Warm Latency (s)\", \"Speedup\",\n",
    "                       \"Prompt Tokens\", \"Context Chars\", \"Token F1\", \"ROUGE-L\", \"Substring Match\"]\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5.5))\n",
    "\n",
    "# â”€â”€ 2a: Cold vs Warm Latency (grouped bar) â”€â”€\n",
    "ax = axes[0]\n",
    "x = np.arange(len(COND_ORDER))\n",
    "w = 0.35\n",
    "cold_means = [df[df.condition == c][\"cold_latency\"].mean() for c in COND_ORDER]\n",
    "cold_stds = [df[df.condition == c][\"cold_latency\"].std() for c in COND_ORDER]\n",
    "warm_means = [df[df.condition == c][\"warm_latency\"].mean() for c in COND_ORDER]\n",
    "warm_stds = [df[df.condition == c][\"warm_latency\"].std() for c in COND_ORDER]\n",
    "\n",
    "bars1 = ax.bar(x - w/2, cold_means, w, yerr=cold_stds, label=\"Cold\",\n",
    "               color=\"#f47067\", edgecolor=\"#da3633\", capsize=3, alpha=0.9)\n",
    "bars2 = ax.bar(x + w/2, warm_means, w, yerr=warm_stds, label=\"Warm\",\n",
    "               color=\"#58a6ff\", edgecolor=\"#388bfd\", capsize=3, alpha=0.9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Latency (seconds)\")\n",
    "ax.set_title(\"Cold vs Warm Latency by Condition\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_ylim(0, 12)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 2b: Speedup Distribution (box plot) â”€â”€\n",
    "ax = axes[1]\n",
    "bp_data = [df[df.condition == c][\"speedup\"].values for c in COND_ORDER]\n",
    "bp = ax.boxplot(bp_data, patch_artist=True, widths=0.6,\n",
    "                boxprops=dict(linewidth=1.2),\n",
    "                medianprops=dict(color=\"#f0f6fc\", linewidth=2),\n",
    "                whiskerprops=dict(color=\"#8b949e\"),\n",
    "                capprops=dict(color=\"#8b949e\"),\n",
    "                flierprops=dict(marker=\"o\", markersize=4, markerfacecolor=\"#f78166\", alpha=0.6))\n",
    "\n",
    "for patch, cond in zip(bp[\"boxes\"], COND_ORDER):\n",
    "    patch.set_facecolor(COLORS[cond])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(y=1.0, color=\"#f78166\", linestyle=\"--\", alpha=0.5, label=\"No speedup (1.0x)\")\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Speedup (cold / warm)\")\n",
    "ax.set_title(\"Cache Speedup Distribution\")\n",
    "ax.legend(loc=\"upper left\", fontsize=9)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 2c: Latency vs Context Length (scatter) â”€â”€\n",
    "ax = axes[2]\n",
    "for cond in COND_ORDER:\n",
    "    subset = df[df.condition == cond]\n",
    "    ax.scatter(subset[\"context_length\"], subset[\"cold_latency\"],\n",
    "               c=COLORS[cond], s=30, alpha=0.6, label=cond, edgecolors=\"none\")\n",
    "\n",
    "# Regression line for non-vanilla\n",
    "ctx_df = df[df.condition != \"vanilla\"]\n",
    "if len(ctx_df) > 2:\n",
    "    z = np.polyfit(ctx_df[\"context_length\"], ctx_df[\"cold_latency\"], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(ctx_df[\"context_length\"].min(), ctx_df[\"context_length\"].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), \"--\", color=\"#f0f6fc\", alpha=0.5, linewidth=1.5)\n",
    "    r, pval = sp_stats.pearsonr(ctx_df[\"context_length\"], ctx_df[\"cold_latency\"])\n",
    "    ax.text(0.05, 0.95, f\"r = {r:.3f} (p = {pval:.2e})\",\n",
    "            transform=ax.transAxes, fontsize=9, va=\"top\", color=\"#f0f6fc\")\n",
    "\n",
    "ax.set_xlabel(\"Context Length (chars)\")\n",
    "ax.set_ylabel(\"Cold Latency (seconds)\")\n",
    "ax.set_title(\"Latency vs Context Length\")\n",
    "ax.legend(fontsize=8, loc=\"lower right\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Section 2: Latency Analysis\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig2_latency.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Context Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5.5))\n",
    "\n",
    "# â”€â”€ 3a: Context Length Distribution â”€â”€\n",
    "ax = axes[0]\n",
    "ctx_conds = [c for c in COND_ORDER if c != \"vanilla\"]\n",
    "bp_data = [df[df.condition == c][\"context_length\"].values for c in ctx_conds]\n",
    "bp = ax.boxplot(bp_data, patch_artist=True, widths=0.6,\n",
    "                medianprops=dict(color=\"#f0f6fc\", linewidth=2),\n",
    "                whiskerprops=dict(color=\"#8b949e\"),\n",
    "                capprops=dict(color=\"#8b949e\"),\n",
    "                flierprops=dict(marker=\"o\", markersize=4, markerfacecolor=\"#f78166\", alpha=0.6))\n",
    "for patch, cond in zip(bp[\"boxes\"], ctx_conds):\n",
    "    patch.set_facecolor(COLORS[cond])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in ctx_conds], fontsize=9)\n",
    "ax.set_ylabel(\"Context Length (chars)\")\n",
    "ax.set_title(\"Context Length Distribution\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 3b: Prompt Tokens Distribution â”€â”€\n",
    "ax = axes[1]\n",
    "bp_data = [df[df.condition == c][\"prompt_tokens\"].values for c in COND_ORDER]\n",
    "bp = ax.boxplot(bp_data, patch_artist=True, widths=0.6,\n",
    "                medianprops=dict(color=\"#f0f6fc\", linewidth=2),\n",
    "                whiskerprops=dict(color=\"#8b949e\"),\n",
    "                capprops=dict(color=\"#8b949e\"),\n",
    "                flierprops=dict(marker=\"o\", markersize=4, markerfacecolor=\"#f78166\", alpha=0.6))\n",
    "for patch, cond in zip(bp[\"boxes\"], COND_ORDER):\n",
    "    patch.set_facecolor(COLORS[cond])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Prompt Tokens\")\n",
    "ax.set_title(\"Prompt Token Count by Condition\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 3c: Token Efficiency (chars per token) â”€â”€\n",
    "ax = axes[2]\n",
    "eff_conds = [c for c in COND_ORDER if c != \"vanilla\"]\n",
    "efficiencies = []\n",
    "for cond in eff_conds:\n",
    "    sub = df[df.condition == cond]\n",
    "    # chars per prompt token (excluding base prompt overhead)\n",
    "    vanilla_base = df[df.condition == \"vanilla\"][\"prompt_tokens\"].mean()\n",
    "    ctx_tokens = sub[\"prompt_tokens\"] - vanilla_base\n",
    "    eff = sub[\"context_length\"] / ctx_tokens.clip(lower=1)\n",
    "    efficiencies.append(eff.values)\n",
    "\n",
    "bp = ax.boxplot(efficiencies, patch_artist=True, widths=0.6,\n",
    "                medianprops=dict(color=\"#f0f6fc\", linewidth=2),\n",
    "                whiskerprops=dict(color=\"#8b949e\"),\n",
    "                capprops=dict(color=\"#8b949e\"),\n",
    "                flierprops=dict(marker=\"o\", markersize=4, markerfacecolor=\"#f78166\", alpha=0.6))\n",
    "for patch, cond in zip(bp[\"boxes\"], eff_conds):\n",
    "    patch.set_facecolor(COLORS[cond])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in eff_conds], fontsize=9)\n",
    "ax.set_ylabel(\"Characters per Token\")\n",
    "ax.set_title(\"Tokenization Efficiency\\n(higher = more chars packed per token)\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Section 3: Context Characteristics\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig3_context.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Answer Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5.5))\n",
    "\n",
    "quality_metrics = [\n",
    "    (\"token_f1\", \"Token F1\"),\n",
    "    (\"rouge_l\", \"ROUGE-L\"),\n",
    "    (\"substring_match\", \"Substring Match Rate\"),\n",
    "]\n",
    "\n",
    "for ax, (metric, label) in zip(axes, quality_metrics):\n",
    "    means = [df[df.condition == c][metric].mean() for c in COND_ORDER]\n",
    "    stds = [df[df.condition == c][metric].std() for c in COND_ORDER]\n",
    "    colors = [COLORS[c] for c in COND_ORDER]\n",
    "\n",
    "    bars = ax.bar(range(len(COND_ORDER)), means, yerr=stds,\n",
    "                  color=colors, edgecolor=[c + \"cc\" for c in colors],\n",
    "                  capsize=4, alpha=0.85)\n",
    "\n",
    "    # Value labels\n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f\"{mean:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"#f0f6fc\")\n",
    "\n",
    "    ax.set_xticks(range(len(COND_ORDER)))\n",
    "    ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "    ax.set_ylabel(label)\n",
    "    ax.set_title(label)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Highlight best\n",
    "    best_idx = np.argmax(means)\n",
    "    bars[best_idx].set_edgecolor(\"#f0f6fc\")\n",
    "    bars[best_idx].set_linewidth(2)\n",
    "\n",
    "fig.suptitle(\"Section 4: Answer Quality by Context Type\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig4_quality.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Quality vs Latency Trade-off (Pareto Front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for cond in COND_ORDER:\n",
    "    sub = df[df.condition == cond]\n",
    "    mean_lat = sub[\"cold_latency\"].mean()\n",
    "    mean_f1 = sub[\"token_f1\"].mean()\n",
    "    std_lat = sub[\"cold_latency\"].std()\n",
    "    std_f1 = sub[\"token_f1\"].std()\n",
    "    mean_tok = sub[\"prompt_tokens\"].mean()\n",
    "\n",
    "    ax.scatter(mean_lat, mean_f1, c=COLORS[cond], s=mean_tok * 0.8,\n",
    "               edgecolors=\"#f0f6fc\", linewidth=1.5, zorder=5, alpha=0.9)\n",
    "    ax.errorbar(mean_lat, mean_f1, xerr=std_lat, yerr=std_f1,\n",
    "                color=COLORS[cond], alpha=0.3, fmt=\"none\", capsize=3)\n",
    "\n",
    "    # Label\n",
    "    offset_x = 0.15 if cond != \"vanilla\" else -0.3\n",
    "    offset_y = 0.003 if cond not in (\"rdf\", \"lpg_rdf\") else -0.006\n",
    "    ax.annotate(cond, (mean_lat + offset_x, mean_f1 + offset_y),\n",
    "                fontsize=10, fontweight=\"bold\", color=COLORS[cond])\n",
    "\n",
    "ax.set_xlabel(\"Mean Cold Latency (seconds)\")\n",
    "ax.set_ylabel(\"Mean Token F1\")\n",
    "ax.set_title(\"Quality vs Latency Trade-off\\n(bubble size = prompt tokens)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Arrow annotation for ideal direction\n",
    "ax.annotate(\"\", xy=(6.5, 0.20), xytext=(8.3, 0.13),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"#3fb950\", lw=2))\n",
    "ax.text(6.8, 0.17, \"Ideal\\ndirection\", fontsize=9, color=\"#3fb950\", style=\"italic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig5_pareto.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Completion Token & Finish Reason Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "# â”€â”€ 6a: Completion Tokens (cold vs warm) â”€â”€\n",
    "ax = axes[0]\n",
    "x = np.arange(len(COND_ORDER))\n",
    "w = 0.35\n",
    "cold_comp = [df[df.condition == c][\"cold_completion_tokens\"].mean() for c in COND_ORDER]\n",
    "warm_comp = [df[df.condition == c][\"warm_completion_tokens\"].mean() for c in COND_ORDER]\n",
    "\n",
    "ax.bar(x - w/2, cold_comp, w, label=\"Cold\", color=\"#f47067\", edgecolor=\"#da3633\", alpha=0.9)\n",
    "ax.bar(x + w/2, warm_comp, w, label=\"Warm\", color=\"#58a6ff\", edgecolor=\"#388bfd\", alpha=0.9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Completion Tokens\")\n",
    "ax.set_title(\"Mean Completion Tokens (Cold vs Warm)\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 6b: Finish Reason Breakdown â”€â”€\n",
    "ax = axes[1]\n",
    "fr_data = df.groupby([\"condition\", \"cold_finish_reason\"], observed=True).size().unstack(fill_value=0)\n",
    "fr_pct = fr_data.div(fr_data.sum(axis=1), axis=0)\n",
    "\n",
    "fr_pct_ordered = fr_pct.reindex(COND_ORDER)\n",
    "colors_fr = {\"stop\": \"#3fb950\", \"length\": \"#f78166\"}\n",
    "bottom = np.zeros(len(COND_ORDER))\n",
    "\n",
    "for reason in [\"stop\", \"length\"]:\n",
    "    if reason in fr_pct_ordered.columns:\n",
    "        vals = fr_pct_ordered[reason].values\n",
    "        ax.bar(range(len(COND_ORDER)), vals, bottom=bottom,\n",
    "               color=colors_fr.get(reason, \"#8b949e\"), label=reason, alpha=0.85)\n",
    "        # Percentage labels\n",
    "        for i, v in enumerate(vals):\n",
    "            if v > 0.05:\n",
    "                ax.text(i, bottom[i] + v/2, f\"{v:.0%}\",\n",
    "                        ha=\"center\", va=\"center\", fontsize=9, color=\"#0d1117\", fontweight=\"bold\")\n",
    "        bottom += vals\n",
    "\n",
    "ax.set_xticks(range(len(COND_ORDER)))\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.set_title(\"Finish Reason (Cold Run)\\nstop = complete answer, length = truncated\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Section 6: Generation Behavior\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig6_completion.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ablation Study: Decomposing the Impact of Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# â”€â”€ 7a: Per-Question Paired Comparison (Radar-like) â”€â”€\n",
    "ax = axes[0, 0]\n",
    "\n",
    "# For each question that has all 5 conditions, compute delta from vanilla\n",
    "paired = df.pivot_table(index=\"question_id\", columns=\"condition\",\n",
    "                         values=\"token_f1\", observed=True)\n",
    "# Only questions with vanilla baseline\n",
    "has_vanilla = paired.dropna(subset=[\"vanilla\"])\n",
    "\n",
    "deltas = {}\n",
    "for cond in [\"lpg\", \"rdf\", \"text\", \"lpg_rdf\"]:\n",
    "    if cond in has_vanilla.columns:\n",
    "        d = has_vanilla[cond] - has_vanilla[\"vanilla\"]\n",
    "        deltas[cond] = d.dropna()\n",
    "\n",
    "bp_data = [deltas[c].values for c in [\"lpg\", \"rdf\", \"text\", \"lpg_rdf\"] if c in deltas]\n",
    "bp_labels = [c for c in [\"lpg\", \"rdf\", \"text\", \"lpg_rdf\"] if c in deltas]\n",
    "\n",
    "bp = ax.boxplot(bp_data, patch_artist=True, widths=0.6,\n",
    "                medianprops=dict(color=\"#f0f6fc\", linewidth=2),\n",
    "                whiskerprops=dict(color=\"#8b949e\"),\n",
    "                capprops=dict(color=\"#8b949e\"))\n",
    "for patch, cond in zip(bp[\"boxes\"], bp_labels):\n",
    "    patch.set_facecolor(COLORS[cond])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(y=0, color=\"#f78166\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in bp_labels], fontsize=9)\n",
    "ax.set_ylabel(\"Î”F1 vs Vanilla\")\n",
    "ax.set_title(\"7a: Paired F1 Gain Over Vanilla\\n(per question)\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Stats\n",
    "for i, cond in enumerate(bp_labels):\n",
    "    d = deltas[cond]\n",
    "    t, p = sp_stats.ttest_1samp(d, 0)\n",
    "    sign = \"*\" if p < 0.05 else \"\"\n",
    "    ax.text(i + 1, ax.get_ylim()[1] * 0.9,\n",
    "            f\"Î¼={d.mean():+.3f}\\np={p:.3f}{sign}\",\n",
    "            ha=\"center\", fontsize=8, color=COLORS[cond])\n",
    "\n",
    "# â”€â”€ 7b: Speedup vs Context Length Bins â”€â”€\n",
    "ax = axes[0, 1]\n",
    "ctx_df = df[df.condition != \"vanilla\"].copy()\n",
    "ctx_df[\"ctx_bin\"] = pd.cut(ctx_df[\"context_length\"],\n",
    "                            bins=[0, 500, 1000, 2000, 5000, 20000],\n",
    "                            labels=[\"<500\", \"500-1K\", \"1K-2K\", \"2K-5K\", \">5K\"])\n",
    "\n",
    "bin_order = [\"<500\", \"500-1K\", \"1K-2K\", \"2K-5K\", \">5K\"]\n",
    "bin_means = ctx_df.groupby(\"ctx_bin\", observed=True)[\"speedup\"].mean()\n",
    "bin_stds = ctx_df.groupby(\"ctx_bin\", observed=True)[\"speedup\"].std()\n",
    "bin_counts = ctx_df.groupby(\"ctx_bin\", observed=True)[\"speedup\"].count()\n",
    "\n",
    "present_bins = [b for b in bin_order if b in bin_means.index]\n",
    "bars = ax.bar(range(len(present_bins)),\n",
    "              [bin_means[b] for b in present_bins],\n",
    "              yerr=[bin_stds[b] for b in present_bins],\n",
    "              color=\"#58a6ff\", edgecolor=\"#388bfd\", capsize=4, alpha=0.85)\n",
    "\n",
    "for i, b in enumerate(present_bins):\n",
    "    ax.text(i, bin_means[b] + bin_stds[b] + 0.02,\n",
    "            f\"n={bin_counts[b]}\", ha=\"center\", fontsize=8, color=\"#8b949e\")\n",
    "\n",
    "ax.axhline(y=1.0, color=\"#f78166\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xticks(range(len(present_bins)))\n",
    "ax.set_xticklabels(present_bins)\n",
    "ax.set_xlabel(\"Context Length (chars)\")\n",
    "ax.set_ylabel(\"Mean Speedup\")\n",
    "ax.set_title(\"7b: Cache Speedup by Context Length\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 7c: Quality vs Prompt Token Efficiency â”€â”€\n",
    "ax = axes[1, 0]\n",
    "for cond in [\"lpg\", \"rdf\", \"text\", \"lpg_rdf\"]:\n",
    "    sub = df[df.condition == cond]\n",
    "    ax.scatter(sub[\"prompt_tokens\"], sub[\"token_f1\"],\n",
    "               c=COLORS[cond], s=25, alpha=0.5, label=cond, edgecolors=\"none\")\n",
    "\n",
    "# Per-condition means\n",
    "for cond in [\"lpg\", \"rdf\", \"text\", \"lpg_rdf\"]:\n",
    "    sub = df[df.condition == cond]\n",
    "    ax.scatter(sub[\"prompt_tokens\"].mean(), sub[\"token_f1\"].mean(),\n",
    "               c=COLORS[cond], s=150, edgecolors=\"#f0f6fc\", linewidth=2, zorder=5,\n",
    "               marker=\"D\")\n",
    "\n",
    "ax.set_xlabel(\"Prompt Tokens\")\n",
    "ax.set_ylabel(\"Token F1\")\n",
    "ax.set_title(\"7c: Quality vs Token Budget\\n(diamonds = condition means)\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# â”€â”€ 7d: Graph Structure vs Text (head-to-head) â”€â”€\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Paired: for each question, compare graph vs text F1\n",
    "pivot_f1 = df.pivot_table(index=\"question_id\", columns=\"condition\",\n",
    "                           values=\"token_f1\", observed=True)\n",
    "\n",
    "comparisons = [\n",
    "    (\"lpg\", \"text\", \"LPG vs Text\"),\n",
    "    (\"rdf\", \"text\", \"RDF vs Text\"),\n",
    "    (\"lpg_rdf\", \"text\", \"LPG+RDF vs Text\"),\n",
    "]\n",
    "\n",
    "y_positions = []\n",
    "for i, (cond_a, cond_b, label) in enumerate(comparisons):\n",
    "    if cond_a in pivot_f1.columns and cond_b in pivot_f1.columns:\n",
    "        paired_diff = (pivot_f1[cond_a] - pivot_f1[cond_b]).dropna()\n",
    "        mean_diff = paired_diff.mean()\n",
    "        ci = 1.96 * paired_diff.std() / np.sqrt(len(paired_diff))\n",
    "        t, p = sp_stats.ttest_1samp(paired_diff, 0)\n",
    "\n",
    "        color = COLORS[cond_a]\n",
    "        ax.barh(i, mean_diff, xerr=ci, color=color, alpha=0.8,\n",
    "                edgecolor=color, capsize=4, height=0.6)\n",
    "        sig = \" *\" if p < 0.05 else \" (ns)\"\n",
    "        side = \"left\" if mean_diff < 0 else \"right\"\n",
    "        offset = -0.005 if mean_diff < 0 else 0.005\n",
    "        ax.text(mean_diff + offset, i,\n",
    "                f\"{mean_diff:+.3f}{sig}\\n(p={p:.3f})\",\n",
    "                ha=side, va=\"center\", fontsize=9, color=color)\n",
    "        y_positions.append(i)\n",
    "\n",
    "ax.axvline(x=0, color=\"#f0f6fc\", linestyle=\"-\", alpha=0.3)\n",
    "ax.set_yticks(range(len(comparisons)))\n",
    "ax.set_yticklabels([c[2] for c in comparisons])\n",
    "ax.set_xlabel(\"Î”F1 (Graph âˆ’ Text)\")\n",
    "ax.set_title(\"7d: Graph Context vs Text Context\\n(paired per-question difference)\")\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Section 7: Ablation Study\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig7_ablation.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"8a: Pairwise Mann-Whitney U Tests (Token F1)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Condition A':>12s} vs {'Condition B':<12s}  {'U':>10s}  {'p-value':>10s}  {'Sig':>5s}  {'Mean A':>8s}  {'Mean B':>8s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for a, b in combinations(COND_ORDER, 2):\n",
    "    va = df[df.condition == a][\"token_f1\"].values\n",
    "    vb = df[df.condition == b][\"token_f1\"].values\n",
    "    u, p = sp_stats.mannwhitneyu(va, vb, alternative=\"two-sided\")\n",
    "    sig = \"*\" if p < 0.05 else \"\"\n",
    "    print(f\"{a:>12s} vs {b:<12s}  {u:>10.0f}  {p:>10.4f}  {sig:>5s}  {va.mean():>8.4f}  {vb.mean():>8.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"8b: Kruskal-Wallis H Test (all conditions)\")\n",
    "print(\"=\" * 80)\n",
    "groups = [df[df.condition == c][\"token_f1\"].values for c in COND_ORDER]\n",
    "h, p = sp_stats.kruskal(*groups)\n",
    "print(f\"H = {h:.4f}, p = {p:.4f} {'(significant)' if p < 0.05 else '(not significant)'}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"8c: Paired t-test: Speedup significantly > 1.0?\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Condition':>12s}  {'Mean Speedup':>14s}  {'t-stat':>10s}  {'p-value':>10s}  {'Sig':>5s}\")\n",
    "print(\"-\" * 60)\n",
    "for cond in COND_ORDER:\n",
    "    speeds = df[df.condition == cond][\"speedup\"].values\n",
    "    t, p = sp_stats.ttest_1samp(speeds, 1.0)\n",
    "    # One-sided: speedup > 1.0\n",
    "    p_one = p / 2 if t > 0 else 1 - p / 2\n",
    "    sig = \"*\" if p_one < 0.05 else \"\"\n",
    "    print(f\"{cond:>12s}  {speeds.mean():>14.4f}  {t:>10.4f}  {p_one:>10.4f}  {sig:>5s}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"8d: Paired t-test: Latency â€” Context vs Vanilla\")\n",
    "print(\"=\" * 80)\n",
    "pivot_lat = df.pivot_table(index=\"question_id\", columns=\"condition\",\n",
    "                            values=\"cold_latency\", observed=True)\n",
    "print(f\"{'Condition':>12s}  {'Î”Latency (s)':>14s}  {'t-stat':>10s}  {'p-value':>10s}  {'Sig':>5s}\")\n",
    "print(\"-\" * 60)\n",
    "for cond in [\"lpg\", \"rdf\", \"text\", \"lpg_rdf\"]:\n",
    "    if cond in pivot_lat.columns and \"vanilla\" in pivot_lat.columns:\n",
    "        paired = (pivot_lat[cond] - pivot_lat[\"vanilla\"]).dropna()\n",
    "        t, p = sp_stats.ttest_1samp(paired, 0)\n",
    "        sig = \"*\" if p < 0.05 else \"\"\n",
    "        print(f\"{cond:>12s}  {paired.mean():>+14.4f}  {t:>10.4f}  {p:>10.4f}  {sig:>5s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Detailed Latency Breakdown: Reasoning Model Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "# â”€â”€ 9a: Latency Distribution (violin) â”€â”€\n",
    "ax = axes[0]\n",
    "\n",
    "parts = ax.violinplot(\n",
    "    [df[df.condition == c][\"cold_latency\"].values for c in COND_ORDER],\n",
    "    positions=range(len(COND_ORDER)),\n",
    "    showmeans=True, showmedians=True, showextrema=False,\n",
    ")\n",
    "\n",
    "for i, (pc, cond) in enumerate(zip(parts[\"bodies\"], COND_ORDER)):\n",
    "    pc.set_facecolor(COLORS[cond])\n",
    "    pc.set_alpha(0.6)\n",
    "\n",
    "parts[\"cmeans\"].set_color(\"#f0f6fc\")\n",
    "parts[\"cmedians\"].set_color(\"#f78166\")\n",
    "\n",
    "ax.set_xticks(range(len(COND_ORDER)))\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Cold Latency (seconds)\")\n",
    "ax.set_title(\"Latency Distribution (Violin)\\nwhite = mean, orange = median\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# â”€â”€ 9b: Bimodal analysis â€” stop vs length â”€â”€\n",
    "ax = axes[1]\n",
    "\n",
    "for cond in COND_ORDER:\n",
    "    sub = df[df.condition == cond]\n",
    "    stop = sub[sub.cold_finish_reason == \"stop\"][\"cold_latency\"]\n",
    "    length = sub[sub.cold_finish_reason == \"length\"][\"cold_latency\"]\n",
    "\n",
    "    x_pos = COND_ORDER.index(cond)\n",
    "    if len(stop) > 0:\n",
    "        ax.scatter([x_pos - 0.15] * len(stop), stop,\n",
    "                   c=\"#3fb950\", s=15, alpha=0.5, marker=\"o\")\n",
    "        ax.scatter(x_pos - 0.15, stop.mean(),\n",
    "                   c=\"#3fb950\", s=100, edgecolors=\"#f0f6fc\", linewidth=1.5,\n",
    "                   marker=\"D\", zorder=5)\n",
    "    if len(length) > 0:\n",
    "        ax.scatter([x_pos + 0.15] * len(length), length,\n",
    "                   c=\"#f78166\", s=15, alpha=0.5, marker=\"o\")\n",
    "        ax.scatter(x_pos + 0.15, length.mean(),\n",
    "                   c=\"#f78166\", s=100, edgecolors=\"#f0f6fc\", linewidth=1.5,\n",
    "                   marker=\"D\", zorder=5)\n",
    "\n",
    "# Legend\n",
    "ax.scatter([], [], c=\"#3fb950\", s=50, marker=\"D\", label=\"stop (complete)\")\n",
    "ax.scatter([], [], c=\"#f78166\", s=50, marker=\"D\", label=\"length (truncated)\")\n",
    "ax.legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "ax.set_xticks(range(len(COND_ORDER)))\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_ylabel(\"Cold Latency (seconds)\")\n",
    "ax.set_title(\"Latency by Finish Reason\\n(diamond = mean per group)\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Section 9: Reasoning Model Latency Behavior\", fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig9_reasoning.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Per-Question Heatmap: Speedup Ã— Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_speed = df.pivot_table(index=\"question_id\", columns=\"condition\",\n",
    "                              values=\"speedup\", observed=True)\n",
    "pivot_speed = pivot_speed.reindex(columns=COND_ORDER)\n",
    "\n",
    "# Sort by lpg_rdf speedup\n",
    "sort_col = \"lpg_rdf\" if \"lpg_rdf\" in pivot_speed.columns else pivot_speed.columns[0]\n",
    "pivot_speed = pivot_speed.sort_values(sort_col, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 16))\n",
    "im = ax.imshow(pivot_speed.values, aspect=\"auto\", cmap=\"RdYlGn\",\n",
    "               vmin=0.8, vmax=1.5)\n",
    "\n",
    "ax.set_xticks(range(len(COND_ORDER)))\n",
    "ax.set_xticklabels([COND_LABELS[c] for c in COND_ORDER], fontsize=9)\n",
    "ax.set_yticks(range(len(pivot_speed)))\n",
    "ax.set_yticklabels(pivot_speed.index, fontsize=6)\n",
    "ax.set_ylabel(\"Question ID\")\n",
    "ax.set_title(\"Per-Question Speedup Heatmap\\n(green = cache helping, red = cache miss)\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.6)\n",
    "cbar.set_label(\"Speedup (cold/warm)\", color=\"#c9d1d9\")\n",
    "cbar.ax.yaxis.set_tick_params(color=\"#8b949e\")\n",
    "plt.setp(plt.getp(cbar.ax.axes, \"yticklabels\"), color=\"#8b949e\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig10_heatmap.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()\n",
    "\n",
    "# Top speedup outliers\n",
    "print(\"\\nTop 10 speedup cases:\")\n",
    "top = df.nlargest(10, \"speedup\")[[\"question_id\", \"condition\", \"cold_latency\", \"warm_latency\", \"speedup\", \"context_length\"]]\n",
    "print(top.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart: normalize each metric to [0, 1] across conditions\n",
    "radar_metrics = {\n",
    "    \"Token F1\": \"token_f1\",\n",
    "    \"ROUGE-L\": \"rouge_l\",\n",
    "    \"Substring\\nMatch\": \"substring_match\",\n",
    "    \"Speedup\": \"speedup\",\n",
    "    \"Low\\nLatency\": \"inv_latency\",  # inverted\n",
    "}\n",
    "\n",
    "# Compute per-condition means\n",
    "radar_data = {}\n",
    "for cond in COND_ORDER:\n",
    "    sub = df[df.condition == cond]\n",
    "    radar_data[cond] = {\n",
    "        \"token_f1\": sub[\"token_f1\"].mean(),\n",
    "        \"rouge_l\": sub[\"rouge_l\"].mean(),\n",
    "        \"substring_match\": sub[\"substring_match\"].mean(),\n",
    "        \"speedup\": sub[\"speedup\"].mean(),\n",
    "        \"inv_latency\": 1.0 / max(sub[\"cold_latency\"].mean(), 0.001),\n",
    "    }\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "metric_keys = list(radar_metrics.values())\n",
    "metric_labels = list(radar_metrics.keys())\n",
    "\n",
    "all_vals = {k: [radar_data[c][k] for c in COND_ORDER] for k in metric_keys}\n",
    "norm_data = {}\n",
    "for cond in COND_ORDER:\n",
    "    norm_data[cond] = []\n",
    "    for k in metric_keys:\n",
    "        vals = all_vals[k]\n",
    "        mn, mx = min(vals), max(vals)\n",
    "        if mx - mn > 0:\n",
    "            norm_data[cond].append((radar_data[cond][k] - mn) / (mx - mn))\n",
    "        else:\n",
    "            norm_data[cond].append(0.5)\n",
    "\n",
    "# Plot\n",
    "angles = np.linspace(0, 2 * np.pi, len(metric_keys), endpoint=False).tolist()\n",
    "angles += angles[:1]  # close\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "ax.set_facecolor(\"#161b22\")\n",
    "\n",
    "for cond in COND_ORDER:\n",
    "    values = norm_data[cond] + norm_data[cond][:1]\n",
    "    ax.plot(angles, values, \"o-\", linewidth=2, label=cond,\n",
    "            color=COLORS[cond], markersize=5)\n",
    "    ax.fill(angles, values, alpha=0.1, color=COLORS[cond])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metric_labels, fontsize=10, color=\"#c9d1d9\")\n",
    "ax.set_yticks([0.25, 0.5, 0.75, 1.0])\n",
    "ax.set_yticklabels([\"25%\", \"50%\", \"75%\", \"100%\"], fontsize=8, color=\"#8b949e\")\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "ax.set_title(\"Multi-Metric Comparison (Normalized)\",\n",
    "             fontsize=14, fontweight=\"bold\", pad=20)\n",
    "ax.grid(color=\"#30363d\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/kvcache_experiment/20260208_091053/fig11_radar.png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Key Findings & Implications\n",
    "\n",
    "### Finding 1: KV Cache Speedup is Marginal for Reasoning Models\n",
    "\n",
    "- Mean speedup across all conditions is **~1.01x** â€” nearly no benefit from KV cache reuse\n",
    "- `gpu_prefix_cache_hit_rate` remained **0.0** throughout the experiment\n",
    "- **Root cause**: The reasoning model (`gpt-oss-120b`) spends ~8s in *generation* (reasoning chain-of-thought), which dominates total latency. Prefix cache only accelerates the *prefill* phase, which is a small fraction of total time\n",
    "- `lpg_rdf` shows the highest speedup variance (Ïƒ=0.20) because it has the longest prompts â†’ larger potential prefill savings\n",
    "\n",
    "### Finding 2: Text RAG Wins on Answer Quality\n",
    "\n",
    "| Condition | Token F1 | ROUGE-L |\n",
    "|-----------|----------|----------|\n",
    "| text | **0.189** | **0.181** |\n",
    "| lpg | 0.163 | 0.149 |\n",
    "| rdf | 0.152 | 0.150 |\n",
    "| lpg_rdf | 0.151 | 0.156 |\n",
    "| vanilla | 0.143 | 0.126 |\n",
    "\n",
    "- Text references provide the richest semantic context (mean 1,922 chars)\n",
    "- Graph contexts (LPG/RDF) improve over vanilla but fall short of text\n",
    "- **Combining LPG+RDF does NOT improve quality** over individual graph types â€” suggests redundancy or noise\n",
    "\n",
    "### Finding 3: Context Type Affects Latency Through Token Count\n",
    "\n",
    "- Vanilla (89 tokens) is consistently ~8.1s â†’ dominated by generation time\n",
    "- With context, latency *decreases* to ~6.9-7.8s with higher variance\n",
    "- This counterintuitive effect is because context helps the model reach `stop` faster (shorter reasoning chains)\n",
    "- LPG has highest latency variance (Ïƒ=2.04) due to varying subgraph sizes\n",
    "\n",
    "### Finding 4: Finish Reason Reveals Reasoning Budget Constraints\n",
    "\n",
    "- ~78% of responses hit `length` limit (max_tokens=512)\n",
    "- The reasoning model consumes most tokens for chain-of-thought, leaving little for the final answer\n",
    "- `stop` responses have significantly lower latency â†’ early stopping when answer is found quickly\n",
    "\n",
    "### Implications for Next Experiment: Hard Prompt (LLM + GNN Retrieve)\n",
    "\n",
    "1. **KV cache benefits require shorter generation, not longer prefill** \n",
    "   â†’ For the hard prompt experiment, consider using a non-reasoning model OR increasing `max_tokens` significantly\n",
    "\n",
    "2. **Graph structure alone underperforms text references**  \n",
    "   â†’ Hard prompt with GNN-retrieved subgraphs should consider **post-processing graph to natural language** before feeding to LLM\n",
    "\n",
    "3. **LPG+RDF combined doesn't add value in soft prompt** \n",
    "   â†’ For hard prompt, instead of combining serialized graphs, use GNN embeddings directly as hidden states (true hard prompt)\n",
    "\n",
    "4. **Token budget is critical for reasoning models** \n",
    "   â†’ Hard prompt experiment should use `max_tokens â‰¥ 1024` or use a non-reasoning model to avoid truncation\n",
    "\n",
    "5. **Latency measurement should focus on TTFT (time to first token)** rather than total latency \n",
    "   â†’ TTFT directly measures prefill time which is where KV cache and GNN embeddings make a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final Summary Table for Blog Post â”€â”€\n",
    "print(\"ðŸ“Š KV Cache Experiment Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: openai/gpt-oss-120b (reasoning model)\")\n",
    "print(f\"Questions: 50 (stratified from 1,332 FinDER KG questions)\")\n",
    "print(f\"Conditions: 5 (vanilla, lpg, rdf, text, lpg_rdf)\")\n",
    "print(f\"Total API calls: {len(df) * 2} (cold + warm per pair)\")\n",
    "print(f\"Success rate: {len(df)}/250 = {len(df)/250:.0%}\")\n",
    "print()\n",
    "\n",
    "final = df.groupby(\"condition\", observed=True).agg(\n",
    "    cold_lat=(\"cold_latency\", \"mean\"),\n",
    "    warm_lat=(\"warm_latency\", \"mean\"),\n",
    "    speedup=(\"speedup\", \"mean\"),\n",
    "    tokens=(\"prompt_tokens\", \"mean\"),\n",
    "    f1=(\"token_f1\", \"mean\"),\n",
    "    rouge_l=(\"rouge_l\", \"mean\"),\n",
    "    sub_match=(\"substring_match\", \"mean\"),\n",
    ").round(3)\n",
    "\n",
    "print(final.to_string())\n",
    "print()\n",
    "print(\"Key takeaway: Text RAG > Graph RAG for soft prompt; \")\n",
    "print(\"KV cache speedup negligible for reasoning models.\")\n",
    "print(\"Next: Hard prompt (GNN embeddings â†’ LLM hidden states) may bypass these limitations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
