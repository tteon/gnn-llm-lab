{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# G-Retrieval Comparison: LPG (GAT) vs RDF (TransE / DistMult)\n\nEnd-to-end experiment notebook with two tracks:\n\n## Track A: Representation Quality (Cells 1–5, 14–20)\nTrain graph encoders on FinDER dual-graph PyG dataset. Evaluate via **graph→answer cosine retrieval**.\n\n| Model | Type | Input | Training |\n|-------|------|-------|----------|\n| **GAT** | GNN (2-layer, 4-head) | LPG subgraphs (384d SentenceTransformer node features) | Link prediction (BCE) |\n| **TransE** | KGE (translation) | RDF triples (h + r ≈ t) | Margin ranking loss |\n| **DistMult** | KGE (bilinear) | RDF triples (h · diag(r) · t) | Negative sampling loss |\n\n## Track B: G-Retrieval E2E Profiling (Cells 6–13)\nFull **Graph Encoder → Projection → LLM** pipeline at D=256.\n\n- Stage-wise profiling with `torch.profiler` + `record_function` labels\n- CPU-store baseline: embedding tables forced to CPU, explicit lookup → H2D → GPU compute\n- Attention map analysis: how graph soft tokens influence LLM self-attention\n- LLM: Llama 3.1 8B Instruct (bfloat16, A100 80GB)\n\n## Data\n- **FinDER KG**: 2,542 samples (train 2,030 / val 251 / test 261)\n- **LPG**: 13,920 nodes, 18,892 edges | **RDF**: 17,534 entities, 4,340 relations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup & Install\n# Uncomment for Colab:\n# !pip install torch torch-geometric sentence-transformers rouge-score\n# !pip install torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n\nimport sys, os, json, time, warnings\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 11, 'figure.dpi': 120})\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom torch_geometric.nn import GATConv, global_mean_pool\nfrom torch_geometric.nn.kge import TransE, DistMult\nfrom torch_geometric.utils import negative_sampling, scatter\n\n# Project imports — adjust path for Colab\nPROJECT_ROOT = Path('..').resolve()\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom src.data import FinDERGraphQADataset, DualGraphBatch, dual_graph_collate_fn, VocabularyBuilder\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif device.type == 'cuda':\n    print(f'  GPU: {torch.cuda.get_device_name()}')\n    print(f'  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading\n",
    "DATA_ROOT = PROJECT_ROOT / 'data' / 'processed' / 'finder_pyg'\n",
    "\n",
    "train_ds = FinDERGraphQADataset(root=str(DATA_ROOT), split='train')\n",
    "val_ds   = FinDERGraphQADataset(root=str(DATA_ROOT), split='val')\n",
    "test_ds  = FinDERGraphQADataset(root=str(DATA_ROOT), split='test')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=dual_graph_collate_fn, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=dual_graph_collate_fn, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=dual_graph_collate_fn, num_workers=0)\n",
    "\n",
    "# Load vocabularies\n",
    "vocabs = FinDERGraphQADataset.get_vocab(root=str(DATA_ROOT))\n",
    "metadata = json.loads((DATA_ROOT / 'processed' / 'metadata.json').read_text())\n",
    "\n",
    "NUM_RDF_ENTITIES  = metadata['vocab_sizes']['rdf_entities']   # 17,534\n",
    "NUM_RDF_RELATIONS = metadata['vocab_sizes']['rdf_relations']  # 4,340\n",
    "LPG_FEATURE_DIM   = metadata['lpg_feature_dim']               # 384\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset Statistics\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Train: {len(train_ds):,} | Val: {len(val_ds):,} | Test: {len(test_ds):,}\")\n",
    "print(f\"  LPG feature dim: {LPG_FEATURE_DIM}\")\n",
    "print(f\"  RDF entities: {NUM_RDF_ENTITIES:,} | relations: {NUM_RDF_RELATIONS:,}\")\n",
    "\n",
    "# Category distribution\n",
    "categories = [train_ds[i].category for i in range(len(train_ds))]\n",
    "cat_counts = pd.Series(categories).value_counts()\n",
    "print(f\"\\nCategory distribution (train):\")\n",
    "for cat, count in cat_counts.items():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Average graph sizes\n",
    "lpg_nodes = [train_ds[i].lpg_num_nodes.item() for i in range(min(200, len(train_ds)))]\n",
    "rdf_edges = [train_ds[i].rdf_edge_index.shape[1] for i in range(min(200, len(train_ds)))]\n",
    "print(f\"\\nAvg LPG nodes/sample: {np.mean(lpg_nodes):.1f} (±{np.std(lpg_nodes):.1f})\")\n",
    "print(f\"Avg RDF triples/sample: {np.mean(rdf_edges):.1f} (±{np.std(rdf_edges):.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Model Definitions\n\n# --- GAT for LPG (batched) ---\n\nclass BatchedGAT(nn.Module):\n    \"\"\"GAT encoder for LPG subgraphs with batched graph-level pooling.\n\n    Based on MessagePassingGNN from src/_legacy/models.py but uses\n    global_mean_pool for proper mini-batch support.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 384,\n        hidden_dim: int = 256,\n        output_dim: int = 384,\n        num_layers: int = 2,\n        heads: int = 4,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.input_proj = nn.Linear(input_dim, hidden_dim)\n        self.convs = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        for i in range(num_layers):\n            in_ch = hidden_dim if i == 0 else hidden_dim * heads\n            self.convs.append(GATConv(in_ch, hidden_dim, heads=heads, dropout=dropout))\n            self.norms.append(nn.LayerNorm(hidden_dim * heads))\n        self.output_proj = nn.Linear(hidden_dim * heads, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, edge_index, batch):\n        \"\"\"Forward pass with batched graph-level pooling.\n\n        Args:\n            x: [sum(N_i), input_dim] node features\n            edge_index: [2, sum(E_i)] COO edges\n            batch: [sum(N_i)] graph membership index\n\n        Returns:\n            [B, output_dim] graph-level embeddings\n        \"\"\"\n        x = torch.relu(self.input_proj(x))\n        for conv, norm in zip(self.convs, self.norms):\n            x = conv(x, edge_index)\n            x = norm(x)\n            x = torch.relu(x)\n            x = self.dropout(x)\n        node_emb = self.output_proj(x)  # [sum(N_i), output_dim]\n        return global_mean_pool(node_emb, batch)  # [B, output_dim]\n\n    def get_node_embeddings(self, x, edge_index):\n        \"\"\"Get per-node embeddings (no pooling). For link prediction decoding.\"\"\"\n        x = torch.relu(self.input_proj(x))\n        for conv, norm in zip(self.convs, self.norms):\n            x = conv(x, edge_index)\n            x = norm(x)\n            x = torch.relu(x)\n            x = self.dropout(x)\n        return self.output_proj(x)  # [sum(N_i), output_dim]\n\n\n# --- KGE for RDF (TransE / DistMult) with per-graph aggregation ---\n\nclass BatchedKGE(nn.Module):\n    \"\"\"KGE encoder for RDF triples with per-graph embedding aggregation.\n\n    Learns global entity and relation embeddings, then for each question's\n    RDF subgraph, aggregates triple representations via scatter mean.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type: str,  # 'transe' or 'distmult'\n        num_entities: int,\n        num_relations: int,\n        hidden_dim: int = 256,\n        output_dim: int = 384,\n    ):\n        super().__init__()\n        self.model_type = model_type\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        if model_type == 'transe':\n            self.kge = TransE(\n                num_nodes=num_entities,\n                num_relations=num_relations,\n                hidden_channels=hidden_dim,\n                margin=1.0,\n                p_norm=1.0,\n            )\n        elif model_type == 'distmult':\n            self.kge = DistMult(\n                num_nodes=num_entities,\n                num_relations=num_relations,\n                hidden_channels=hidden_dim,\n            )\n        else:\n            raise ValueError(f\"Unknown model_type: {model_type}\")\n\n        self.output_proj = nn.Linear(hidden_dim, output_dim)\n\n    def loss(self, head_index, rel_type, tail_index):\n        \"\"\"KGE training loss with built-in negative sampling.\"\"\"\n        return self.kge.loss(head_index, rel_type, tail_index)\n\n    def forward(self, batch: 'DualGraphBatch'):\n        \"\"\"Compute per-graph embeddings from RDF triples.\n\n        For each triple (h, r, t) in the batch, computes h_emb + r_emb,\n        then aggregates per-graph via scatter mean on the head node's\n        graph membership.\n\n        Returns:\n            [B, output_dim] graph-level embeddings\n        \"\"\"\n        edge_index = batch.rdf_edge_index  # [2, sum(T_i)]\n        edge_type = batch.rdf_edge_type    # [sum(T_i)]\n        rdf_batch = batch.rdf_batch        # [sum(N_rdf_i)]\n        global_idx = batch.rdf_global_node_idx  # [sum(N_rdf_i)]\n\n        if edge_index.shape[1] == 0:\n            return torch.zeros(batch.batch_size, self.output_dim, device=edge_index.device)\n\n        head_local = edge_index[0]  # local node indices\n        tail_local = edge_index[1]\n\n        # Map local indices to global for embedding lookup\n        head_global = global_idx[head_local]\n        tail_global = global_idx[tail_local]\n\n        head_emb = self.kge.node_emb(head_global)  # [sum(T_i), hidden_dim]\n        rel_emb = self.kge.rel_emb(edge_type)       # [sum(T_i), hidden_dim]\n\n        # Triple representation: h + r (translation-style, works for both)\n        triple_emb = head_emb + rel_emb  # [sum(T_i), hidden_dim]\n\n        # Determine graph membership for each triple (from head node)\n        triple_graph = rdf_batch[head_local]  # [sum(T_i)]\n\n        # Aggregate triples per graph (using PyG's scatter with reduce='mean')\n        graph_emb = scatter(triple_emb, triple_graph, dim=0,\n                            dim_size=batch.batch_size, reduce='mean')  # [B, hidden_dim]\n\n        return self.output_proj(graph_emb)  # [B, output_dim]\n\n    def get_entity_embeddings(self):\n        \"\"\"Export projected entity embeddings [num_entities, output_dim].\"\"\"\n        with torch.no_grad():\n            return self.output_proj(self.kge.node_emb.weight)\n\n\n# Quick sanity check\nprint('BatchedGAT params:', sum(p.numel() for p in BatchedGAT().parameters()) / 1e3, 'K')\nprint('BatchedKGE (TransE) params:',\n      sum(p.numel() for p in BatchedKGE('transe', NUM_RDF_ENTITIES, NUM_RDF_RELATIONS).parameters()) / 1e6, 'M')\nprint('BatchedKGE (DistMult) params:',\n      sum(p.numel() for p in BatchedKGE('distmult', NUM_RDF_ENTITIES, NUM_RDF_RELATIONS).parameters()) / 1e6, 'M')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Training — GAT (LPG) via Link Prediction\n",
    "\n",
    "def train_gat_epoch(model, loader, optimizer):\n",
    "    \"\"\"Train GAT via link prediction with negative sampling.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get node embeddings (no pooling)\n",
    "        z = model.get_node_embeddings(batch.lpg_x, batch.lpg_edge_index)\n",
    "\n",
    "        # Positive edges\n",
    "        pos_edge = batch.lpg_edge_index\n",
    "        num_nodes = batch.lpg_x.shape[0]\n",
    "\n",
    "        if pos_edge.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        # Negative sampling\n",
    "        neg_edge = negative_sampling(\n",
    "            pos_edge, num_nodes=num_nodes,\n",
    "            num_neg_samples=pos_edge.shape[1],\n",
    "        )\n",
    "\n",
    "        # Score positive and negative edges via dot product\n",
    "        pos_score = (z[pos_edge[0]] * z[pos_edge[1]]).sum(dim=-1)\n",
    "        neg_score = (z[neg_edge[0]] * z[neg_edge[1]]).sum(dim=-1)\n",
    "\n",
    "        # BCE loss\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(pos_score, torch.ones_like(pos_score))\n",
    "        neg_loss = F.binary_cross_entropy_with_logits(neg_score, torch.zeros_like(neg_score))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_gat_link_prediction(model, loader):\n",
    "    \"\"\"Evaluate GAT link prediction: MRR and Hits@10.\"\"\"\n",
    "    model.eval()\n",
    "    mrr_sum, hits10_sum, count = 0.0, 0.0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        z = model.get_node_embeddings(batch.lpg_x, batch.lpg_edge_index)\n",
    "\n",
    "        pos_edge = batch.lpg_edge_index\n",
    "        if pos_edge.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        num_nodes = z.shape[0]\n",
    "        # Score a sample of edges (full ranking is too slow)\n",
    "        sample_size = min(500, pos_edge.shape[1])\n",
    "        idx = torch.randperm(pos_edge.shape[1])[:sample_size]\n",
    "        src, dst = pos_edge[0, idx], pos_edge[1, idx]\n",
    "\n",
    "        for s, d in zip(src, dst):\n",
    "            # Score true tail vs all nodes\n",
    "            scores = (z[s].unsqueeze(0) * z).sum(dim=-1)  # [num_nodes]\n",
    "            rank = (scores >= scores[d]).sum().item()\n",
    "            mrr_sum += 1.0 / rank\n",
    "            hits10_sum += 1.0 if rank <= 10 else 0.0\n",
    "            count += 1\n",
    "\n",
    "    mrr = mrr_sum / max(count, 1)\n",
    "    hits10 = hits10_sum / max(count, 1)\n",
    "    return {'mrr': mrr, 'hits@10': hits10}\n",
    "\n",
    "\n",
    "# Train GAT\n",
    "GAT_EPOCHS = 50\n",
    "GAT_LR = 1e-3\n",
    "\n",
    "gat_model = BatchedGAT(input_dim=LPG_FEATURE_DIM).to(device)\n",
    "gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=GAT_LR, weight_decay=1e-5)\n",
    "\n",
    "gat_history = {'train_loss': [], 'val_mrr': [], 'val_hits10': []}\n",
    "best_val_mrr = 0.0\n",
    "best_gat_state = None\n",
    "\n",
    "print(f'Training GAT for {GAT_EPOCHS} epochs...')\n",
    "for epoch in range(1, GAT_EPOCHS + 1):\n",
    "    loss = train_gat_epoch(gat_model, train_loader, gat_optimizer)\n",
    "    gat_history['train_loss'].append(loss)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        val_metrics = eval_gat_link_prediction(gat_model, val_loader)\n",
    "        gat_history['val_mrr'].append(val_metrics['mrr'])\n",
    "        gat_history['val_hits10'].append(val_metrics['hits@10'])\n",
    "        print(f'  Epoch {epoch:3d} | Loss: {loss:.4f} | Val MRR: {val_metrics[\"mrr\"]:.4f} | Val Hits@10: {val_metrics[\"hits@10\"]:.3f}')\n",
    "\n",
    "        if val_metrics['mrr'] > best_val_mrr:\n",
    "            best_val_mrr = val_metrics['mrr']\n",
    "            best_gat_state = {k: v.cpu().clone() for k, v in gat_model.state_dict().items()}\n",
    "\n",
    "# Restore best model\n",
    "if best_gat_state:\n",
    "    gat_model.load_state_dict(best_gat_state)\n",
    "    gat_model.to(device)\n",
    "print(f'\\nBest GAT Val MRR: {best_val_mrr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training — TransE & DistMult (RDF)\n",
    "\n",
    "def collect_all_rdf_triples(loader):\n",
    "    \"\"\"Collect all (head_global, rel, tail_global) triples from the dataset.\"\"\"\n",
    "    heads, rels, tails = [], [], []\n",
    "    for batch in loader:\n",
    "        ei = batch.rdf_edge_index\n",
    "        et = batch.rdf_edge_type\n",
    "        gi = batch.rdf_global_node_idx\n",
    "        if ei.shape[1] == 0:\n",
    "            continue\n",
    "        heads.append(gi[ei[0]])\n",
    "        tails.append(gi[ei[1]])\n",
    "        rels.append(et)\n",
    "    return torch.cat(heads), torch.cat(rels), torch.cat(tails)\n",
    "\n",
    "\n",
    "def train_kge_epoch(model, head, rel, tail, optimizer, batch_size=512):\n",
    "    \"\"\"Train KGE model for one epoch over all triples.\"\"\"\n",
    "    model.train()\n",
    "    perm = torch.randperm(head.shape[0], device=head.device)\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i in range(0, head.shape[0], batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head[idx], rel[idx], tail[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_kge(model, head, rel, tail, sample_size=1000, k=10):\n",
    "    \"\"\"Evaluate KGE with sampled ranking: MRR and Hits@K.\"\"\"\n",
    "    model.eval()\n",
    "    n = min(sample_size, head.shape[0])\n",
    "    idx = torch.randperm(head.shape[0])[:n]\n",
    "    h, r, t = head[idx], rel[idx], tail[idx]\n",
    "\n",
    "    node_emb = model.kge.node_emb.weight  # [num_entities, hidden_dim]\n",
    "    rel_emb = model.kge.rel_emb(r)        # [n, hidden_dim]\n",
    "    h_emb = model.kge.node_emb(h)          # [n, hidden_dim]\n",
    "\n",
    "    mrr_sum, hits_sum = 0.0, 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        if model.model_type == 'transe':\n",
    "            # score = -||h + r - t||  (higher = better)\n",
    "            pred = h_emb[i] + rel_emb[i]  # [hidden_dim]\n",
    "            scores = -torch.norm(node_emb - pred.unsqueeze(0), p=1, dim=-1)  # [num_entities]\n",
    "        else:  # distmult\n",
    "            # score = sum(h * r * t)\n",
    "            pred = h_emb[i] * rel_emb[i]  # [hidden_dim]\n",
    "            scores = (node_emb * pred.unsqueeze(0)).sum(dim=-1)  # [num_entities]\n",
    "\n",
    "        rank = (scores >= scores[t[i]]).sum().item()\n",
    "        mrr_sum += 1.0 / max(rank, 1)\n",
    "        hits_sum += 1.0 if rank <= k else 0.0\n",
    "\n",
    "    return {'mrr': mrr_sum / n, f'hits@{k}': hits_sum / n}\n",
    "\n",
    "\n",
    "# Collect triples\n",
    "print('Collecting RDF triples...')\n",
    "train_h, train_r, train_t = collect_all_rdf_triples(train_loader)\n",
    "val_h, val_r, val_t = collect_all_rdf_triples(val_loader)\n",
    "train_h, train_r, train_t = train_h.to(device), train_r.to(device), train_t.to(device)\n",
    "val_h, val_r, val_t = val_h.to(device), val_r.to(device), val_t.to(device)\n",
    "print(f'  Train triples: {train_h.shape[0]:,} | Val triples: {val_h.shape[0]:,}')\n",
    "\n",
    "# Train both models\n",
    "KGE_EPOCHS = 100\n",
    "KGE_LR = 1e-2\n",
    "KGE_BATCH = 512\n",
    "\n",
    "kge_models = {}\n",
    "kge_histories = {}\n",
    "\n",
    "for model_type in ['transe', 'distmult']:\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Training {model_type.upper()} for {KGE_EPOCHS} epochs...')\n",
    "    print(f'{\"=\"*50}')\n",
    "\n",
    "    model = BatchedKGE(model_type, NUM_RDF_ENTITIES, NUM_RDF_RELATIONS).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=KGE_LR)\n",
    "    history = {'train_loss': [], 'val_mrr': [], 'val_hits10': []}\n",
    "    best_mrr = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, KGE_EPOCHS + 1):\n",
    "        loss = train_kge_epoch(model, train_h, train_r, train_t, optimizer, KGE_BATCH)\n",
    "        history['train_loss'].append(loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            val_metrics = eval_kge(model, val_h, val_r, val_t)\n",
    "            history['val_mrr'].append(val_metrics['mrr'])\n",
    "            history['val_hits10'].append(val_metrics['hits@10'])\n",
    "            print(f'  Epoch {epoch:3d} | Loss: {loss:.4f} | Val MRR: {val_metrics[\"mrr\"]:.4f} | Val Hits@10: {val_metrics[\"hits@10\"]:.3f}')\n",
    "\n",
    "            if val_metrics['mrr'] > best_mrr:\n",
    "                best_mrr = val_metrics['mrr']\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "        model.to(device)\n",
    "    print(f'Best {model_type.upper()} Val MRR: {best_mrr:.4f}')\n",
    "\n",
    "    kge_models[model_type] = model\n",
    "    kge_histories[model_type] = history\n",
    "\n",
    "# Loss curve comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for mt, h in kge_histories.items():\n",
    "    axes[0].plot(h['train_loss'], label=mt.upper())\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('KGE Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Also plot GAT loss\n",
    "axes[1].plot(gat_history['train_loss'], label='GAT', color='tab:green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('GAT Training Loss')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Track B: G-Retrieval E2E Profiling (D=256)\n\n**Architecture:** Graph Encoder → Projection MLP → LLM (Llama 3.1 8B, 4-bit)\n\n**D=256**: LPG features PCA-projected from 384→256. KGE embeddings natively 256d.\n\n**CPU-store baseline**: Embedding tables forced to CPU. Explicit stage separation:\n\n| # | Label | LPG (GAT) | RDF (DistMult) |\n|---|-------|-----------|----------------|\n| 1 | `SUBGRAPH_LOAD` | Extract index tensors from batch | Extract edge_index, edge_type, global_idx |\n| 2 | `LPG_CPU_GATHER_X` / `RDF_KGE_CPU_LOOKUP_ENT_REL` | CPU feature store indexing | CPU embedding table lookup |\n| 3 | `LPG_H2D_COPY` / `RDF_KGE_H2D_COPY` | Sync transfer to GPU | Sync transfer to GPU |\n| 4 | `LPG_GAT_FWD` / `RDF_KGE_FWD_DISTMULT` | GAT layers + global_mean_pool | h*r (DistMult) + scatter + proj |\n| 5 | `PROJECTION` | Linear(256 → LLM hidden) | Linear(256 → LLM hidden) |\n| 6 | `LLM_PREFILL` | LLM forward with graph soft tokens | Same |\n\n**Two profiling passes:**\n1. Custom CUDA Event timers → quantitative summary tables\n2. `torch.profiler` + `record_function` → Chrome trace (`chrome://tracing`)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 6: Profiling Utilities + CPU Store Simulators\n\n**Objective:** Set up timing infrastructure and CPU-resident feature/embedding stores for G-Retrieval profiling.\n\n**Key components:**\n\n| Component | Purpose |\n|-----------|---------|\n| `GRAPH_DIM = 256` | Unified embedding dimension (PCA-projected from 384) |\n| `StageTiming` | Dataclass holding CPU ms, CUDA ms, bytes transferred per stage |\n| `CUDATimer` | Context manager using `torch.cuda.Event` pairs for GPU timing |\n| `CPUTimer` | Context manager using `time.perf_counter` for CPU-only stages |\n| `LPGCPUStore` | CPU-resident node features [13920, 256] — forces explicit `gather()` + H2D |\n| `RDFCPUStore` | CPU-resident KGE embeddings (entity [17534, 256] + relation [4340, 256]) — forces explicit `lookup()` + H2D |\n\n**PCA Projection:** `torch.pca_lowrank(384→256)` applied offline to global LPG node features. Reports explained variance ratio.\n\n**Design decision:** `non_blocking=False` for all H2D copies to ensure stage isolation in profiling. `pin_memory=False` on DataLoader for baseline measurement."
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Profiling Utilities + CPU Store Simulators (D=256)\n\nimport time\nfrom torch.profiler import profile as torch_profile, ProfilerActivity, record_function\n\n# ── Constants ──\nWARMUP_BATCHES = 3\nPROFILE_BATCHES = 20\nPROFILE_BATCH_SIZE = 128  # A100 80GB can handle large batches\nGRAPH_DIM = 256  # Unified graph embedding dim (PCA-projected from 384)\n\nHAS_CUDA = device.type == 'cuda'\n\n# ── Timing Infrastructure ──\n\n@dataclass\nclass StageTiming:\n    stage: str\n    cpu_ms: float\n    cuda_ms: float = float('nan')\n    bytes_transferred: int = 0\n\n\nclass CUDATimer:\n    def __init__(self, stage: str):\n        self.stage = stage\n        if HAS_CUDA:\n            self.start_evt = torch.cuda.Event(enable_timing=True)\n            self.end_evt = torch.cuda.Event(enable_timing=True)\n\n    def __enter__(self):\n        if HAS_CUDA:\n            torch.cuda.synchronize()\n        self.cpu_start = time.perf_counter()\n        if HAS_CUDA:\n            self.start_evt.record()\n        return self\n\n    def __exit__(self, *args):\n        if HAS_CUDA:\n            self.end_evt.record()\n            torch.cuda.synchronize()\n        self.cpu_end = time.perf_counter()\n        cpu_ms = (self.cpu_end - self.cpu_start) * 1000\n        cuda_ms = self.start_evt.elapsed_time(self.end_evt) if HAS_CUDA else float('nan')\n        self._result = StageTiming(stage=self.stage, cpu_ms=cpu_ms, cuda_ms=cuda_ms)\n\n    def result(self):\n        return self._result\n\n\nclass CPUTimer:\n    def __init__(self, stage: str):\n        self.stage = stage\n\n    def __enter__(self):\n        self.cpu_start = time.perf_counter()\n        return self\n\n    def __exit__(self, *args):\n        self.cpu_end = time.perf_counter()\n        cpu_ms = (self.cpu_end - self.cpu_start) * 1000\n        self._result = StageTiming(stage=self.stage, cpu_ms=cpu_ms)\n\n    def result(self):\n        return self._result\n\n\n# ── Result Aggregation ──\n\ndef aggregate_timings(all_timings, batch_sizes, node_counts, edge_counts):\n    stages = list(all_timings[0].keys())\n    rows = []\n    for stage in stages:\n        cpu_vals = [t[stage].cpu_ms for t in all_timings]\n        cuda_vals = [t[stage].cuda_ms for t in all_timings]\n        bytes_vals = [t[stage].bytes_transferred for t in all_timings]\n        row = {\n            'stage': stage,\n            'cpu_mean': np.mean(cpu_vals),\n            'cpu_std': np.std(cpu_vals),\n            'cpu_p95': np.percentile(cpu_vals, 95),\n            'cuda_mean': np.nanmean(cuda_vals) if not all(np.isnan(cuda_vals)) else float('nan'),\n            'cuda_std': np.nanstd(cuda_vals) if not all(np.isnan(cuda_vals)) else float('nan'),\n            'cuda_p95': np.nanpercentile(cuda_vals, 95) if not all(np.isnan(cuda_vals)) else float('nan'),\n            'bytes_mean': np.mean(bytes_vals),\n        }\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\ndef print_profiling_table(df, model_name, n_batches):\n    total_cpu = df['cpu_mean'].sum()\n    print(f'\\n{\"=\"*70}')\n    print(f'{model_name} Profiling -- {n_batches} batches, BS={PROFILE_BATCH_SIZE}, D={GRAPH_DIM}')\n    print(f'{\"=\"*70}')\n    print(f'{\"Stage\":<30} {\"CPU ms\":>10} {\"CUDA ms\":>10} {\"% total\":>8} {\"BW\":>10}')\n    print(f'{\"-\"*70}')\n    for _, r in df.iterrows():\n        pct = r['cpu_mean'] / total_cpu * 100 if total_cpu > 0 else 0\n        cuda_str = f'{r[\"cuda_mean\"]:.2f}' if not np.isnan(r['cuda_mean']) else '--'\n        bw_str = ''\n        if r['bytes_mean'] > 0 and not np.isnan(r['cuda_mean']) and r['cuda_mean'] > 0:\n            bw_gbps = (r['bytes_mean'] / 1e9) / (r['cuda_mean'] / 1e3)\n            bw_str = f'{bw_gbps:.1f} GB/s'\n        print(f'{r[\"stage\"]:<30} {r[\"cpu_mean\"]:>8.2f}+/-{r[\"cpu_std\"]:<4.1f} '\n              f'{cuda_str:>8} {pct:>7.1f}%  {bw_str}')\n    print(f'{\"-\"*70}')\n    print(f'{\"TOTAL\":<30} {total_cpu:>10.2f}')\n\n\n# ══════════════════════════════════════════════════\n# Offline PCA Projection: 384 -> 256\n# ══════════════════════════════════════════════════\nprint(f'Loading & projecting LPG features (384d -> {GRAPH_DIM}d via PCA)...')\n_lpg_cache_path = PROJECT_ROOT / 'data' / 'processed' / 'lpg_full_graph.pt'\n\nif _lpg_cache_path.exists():\n    _lpg_cache = torch.load(str(_lpg_cache_path), weights_only=False, map_location='cpu')\n    _feat_384 = _lpg_cache['data'].x  # [13920, 384]\n    del _lpg_cache\nelse:\n    print('  lpg_full_graph.pt not found -- reconstructing from dataset...')\n    _max_idx = 0\n    for ds in [train_ds, val_ds, test_ds]:\n        for i in range(len(ds)):\n            gi = ds[i].lpg_global_node_idx\n            if gi.numel() > 0:\n                _max_idx = max(_max_idx, gi.max().item())\n    _feat_384 = torch.zeros(_max_idx + 1, LPG_FEATURE_DIM)\n    for ds in [train_ds, val_ds, test_ds]:\n        for i in range(len(ds)):\n            sample = ds[i]\n            gi = sample.lpg_global_node_idx\n            if gi.numel() > 0:\n                _feat_384[gi] = sample.lpg_x\n    print(f'  Reconstructed from {len(train_ds)+len(val_ds)+len(test_ds)} samples')\n\n_centered = _feat_384 - _feat_384.mean(dim=0)\nU, S, V = torch.pca_lowrank(_centered, q=GRAPH_DIM)\nglobal_node_features = (_centered @ V[:, :GRAPH_DIM]).float()  # [N, 256]\n_var_ratio = (S[:GRAPH_DIM]**2).sum() / (S**2).sum()\nprint(f'  PCA: {_feat_384.shape} -> {global_node_features.shape}')\nprint(f'  Explained variance ratio: {_var_ratio:.3f}')\ndel _feat_384, _centered, U, S, V\n\n\n# ══════════════════════════════════════════════════\n# CPU Store Simulators (forced CPU residence)\n# ══════════════════════════════════════════════════\nprint('\\nInitializing CPU stores...')\n\n\nclass LPGCPUStore:\n    \"\"\"CPU-resident node feature store for LPG (D=256).\n    Forces gather on CPU before explicit H2D copy.\"\"\"\n    def __init__(self, features):\n        self.features = features.cpu().clone()\n        mb = self.features.nelement() * self.features.element_size() / 1e6\n        print(f'  LPGCPUStore: {self.features.shape}, {mb:.1f} MB on CPU')\n\n    def gather(self, global_node_idx):\n        return self.features[global_node_idx.cpu()]\n\n\nclass RDFCPUStore:\n    \"\"\"CPU-resident KGE embedding tables (D=256).\n    Entity + relation embeddings forced to CPU.\n    lookup() returns CPU tensors for explicit H2D.\"\"\"\n    def __init__(self, kge_model):\n        self.node_emb_weight = kge_model.kge.node_emb.weight.detach().cpu().clone()\n        self.rel_emb_weight = kge_model.kge.rel_emb.weight.detach().cpu().clone()\n        node_mb = self.node_emb_weight.nelement() * self.node_emb_weight.element_size() / 1e6\n        rel_mb = self.rel_emb_weight.nelement() * self.rel_emb_weight.element_size() / 1e6\n        print(f'  RDFCPUStore: node_emb {self.node_emb_weight.shape} ({node_mb:.1f} MB)')\n        print(f'               rel_emb  {self.rel_emb_weight.shape} ({rel_mb:.1f} MB)')\n\n    def lookup(self, head_global, edge_type):\n        h_emb = self.node_emb_weight[head_global.cpu()]\n        r_emb = self.rel_emb_weight[edge_type.cpu()]\n        return h_emb, r_emb\n\n\nlpg_cpu_store = LPGCPUStore(global_node_features)\nrdf_cpu_store = RDFCPUStore(kge_models['distmult'])\n\n# Profiling DataLoader (no pin_memory for baseline)\nprofile_loader = DataLoader(train_ds, batch_size=PROFILE_BATCH_SIZE, shuffle=False,\n                            collate_fn=dual_graph_collate_fn, num_workers=0,\n                            pin_memory=False)\nprint(f'\\nProfile loader: {len(train_ds)} samples, ~{len(train_ds)//PROFILE_BATCH_SIZE} batches')\nprint(f'Config: {WARMUP_BATCHES} warmup + {PROFILE_BATCHES} measure, D={GRAPH_DIM}, sync H2D')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 7: Profiling Models (D=256) + Llama 3 Loading\n\n**Objective:** Create profiling-specific 256d encoders and load the LLM.\n\n**Models created (separate from Track A's 384d trained models):**\n- `prof_gat`: `BatchedGAT(input_dim=256, output_dim=256)` — random init (latency is weight-independent)\n- `prof_kge_proj`: `Linear(256→256)` — replaces Track A's `256→384` output projection\n- `GraphLLM`: Projection MLP `256 → LLM_hidden × T` + LLM forward with `inputs_embeds`\n\n**LLM:** `meta-llama/Llama-3.1-8B-Instruct` loaded in **bfloat16** (no quantization) to eliminate quant/dequant overhead from profiling. ~16GB VRAM on A100 80GB.\n\n**Graph tokens:** `NUM_GRAPH_TOKENS = 4` soft tokens prepended to text input embeddings."
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Profiling Models (D=256) + Llama 3 Loading\n#\n# Profiling-specific 256d encoders (separate from Track A's 384d trained models).\n# Untrained weights are fine for latency profiling (compute cost is identical).\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# ── Profiling Encoders at D=256 ──\nprint(f'Creating profiling encoders at D={GRAPH_DIM}...')\n\n# GAT encoder (256d input from PCA-projected features)\nprof_gat = BatchedGAT(\n    input_dim=GRAPH_DIM, hidden_dim=256, output_dim=GRAPH_DIM,\n    num_layers=2, heads=4, dropout=0.1,\n).to(device)\nprof_gat.eval()\nprint(f'  prof_gat: {sum(p.numel() for p in prof_gat.parameters())/1e3:.1f}K params')\n\n# KGE output projection (256 -> 256, replaces Track A's 256->384)\nprof_kge_proj = nn.Linear(256, GRAPH_DIM).to(device)\nprof_kge_proj.eval()\nprint(f'  prof_kge_proj: Linear(256 -> {GRAPH_DIM})')\n\n\n# ── GraphLLM: Graph Encoder -> Projection -> LLM ──\n\nclass GraphLLM(nn.Module):\n    def __init__(self, graph_output_dim, llm, num_graph_tokens=1):\n        super().__init__()\n        self.llm = llm\n        self.num_graph_tokens = num_graph_tokens\n        self.llm_hidden = llm.config.hidden_size\n\n        self.projector = nn.Sequential(\n            nn.Linear(graph_output_dim, self.llm_hidden),\n            nn.GELU(),\n            nn.Linear(self.llm_hidden, self.llm_hidden * num_graph_tokens),\n        )\n\n    def project(self, graph_emb):\n        proj = self.projector(graph_emb)\n        B = proj.shape[0]\n        proj = proj.to(self.llm.dtype)  # float32 -> bfloat16\n        return proj.view(B, self.num_graph_tokens, self.llm_hidden)\n\n    @torch.no_grad()\n    def forward_prefill(self, graph_tokens, input_ids, attention_mask):\n        text_emb = self.llm.get_input_embeddings()(input_ids)\n        combined = torch.cat([graph_tokens, text_emb], dim=1)\n        graph_mask = torch.ones(\n            graph_tokens.shape[0], graph_tokens.shape[1],\n            device=attention_mask.device, dtype=attention_mask.dtype,\n        )\n        combined_mask = torch.cat([graph_mask, attention_mask], dim=1)\n        return self.llm(inputs_embeds=combined, attention_mask=combined_mask).logits\n\n\n# ── Load Llama 3.1 8B (4-bit NF4) ──\nLLM_MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\nNUM_GRAPH_TOKENS = 4  # More graph context tokens (A100 80GB has headroom)\n\nprint(f'\\nLoading LLM: {LLM_MODEL_ID} (bfloat16, no quantization)')\nprint(f'  A100 80GB: loading full bfloat16 for clean profiling (no quant overhead)')\n\ntokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nllm = AutoModelForCausalLM.from_pretrained(\n    LLM_MODEL_ID,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\nllm.eval()\nfor p in llm.parameters():\n    p.requires_grad = False\n\nllm_mem_gb = sum(p.nelement() * p.element_size() for p in llm.parameters()) / 1e9\nprint(f'  LLM: {llm_mem_gb:.1f} GB, hidden_size={llm.config.hidden_size}')\n\n# ── GraphLLM instance (shared LLM, D=256 projector) ──\ngraph_llm = GraphLLM(GRAPH_DIM, llm, NUM_GRAPH_TOKENS)\ngraph_llm.projector.to(device)\nproj_params = sum(p.numel() for p in graph_llm.projector.parameters())\nprint(f'  Projector: {GRAPH_DIM} -> {llm.config.hidden_size} x {NUM_GRAPH_TOKENS}, {proj_params/1e3:.1f}K params')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 8: E2E Stage-wise Profiling\n\n**Objective:** Profile the full G-Retrieval inference pipeline with two complementary methods.\n\n**Pass 1 — Custom CUDA Event Timers** (20 batches):\nQuantitative per-stage timing with mean/std/p95 and H2D bandwidth.\n\n**Pass 2 — `torch.profiler` + `record_function`** (20 batches):\nChrome-trace-compatible profiling. Export to `gat_profile_trace.json` and `rdf_distmult_profile_trace.json` for visualization at `chrome://tracing` or `perfetto.dev`.\n\n**Stage labels (6 stages per pipeline):**\n\n| # | GAT (LPG) | DistMult (RDF) |\n|---|-----------|----------------|\n| 1 | `SUBGRAPH_LOAD` | `SUBGRAPH_LOAD` |\n| 2 | `LPG_CPU_GATHER_X` | `RDF_KGE_CPU_LOOKUP_ENT_REL` |\n| 3 | `LPG_H2D_COPY` | `RDF_KGE_H2D_COPY` |\n| 4 | `LPG_GAT_FWD` | `RDF_KGE_FWD_DISTMULT` |\n| 5 | `PROJECTION` | `PROJECTION` |\n| 6 | `LLM_PREFILL` | `LLM_PREFILL` |"
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: E2E Stage-wise Profiling\n",
    "#\n",
    "# Pass 1: Custom CUDA Event timers -> quantitative summary tables\n",
    "# Pass 2: torch.profiler + record_function -> Chrome trace\n",
    "#\n",
    "# Labels:\n",
    "#   LPG:  SUBGRAPH_LOAD, LPG_CPU_GATHER_X, LPG_H2D_COPY, LPG_GAT_FWD, PROJECTION, LLM_PREFILL\n",
    "#   RDF:  SUBGRAPH_LOAD, RDF_KGE_CPU_LOOKUP_ENT_REL, RDF_KGE_H2D_COPY, RDF_KGE_FWD_DISTMULT, PROJECTION, LLM_PREFILL\n",
    "\n",
    "from itertools import islice, cycle\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Based on the graph context, answer the following question concisely.\\n\"\n",
    "    \"Question: {question}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "def tokenize_questions(questions, max_length=128):\n",
    "    prompts = [PROMPT_TEMPLATE.format(question=q) for q in questions]\n",
    "    enc = tokenizer(prompts, return_tensors='pt', padding=True,\n",
    "                    truncation=True, max_length=max_length)\n",
    "    return enc['input_ids'].to(device), enc['attention_mask'].to(device)\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Pass 1: Custom Timers -- GAT (LPG)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def profile_lpg(gat_model, lpg_store, gllm, loader, n_warmup, n_measure):\n",
    "    gat_model.eval()\n",
    "    timings_list, batch_sizes, node_counts, edge_counts = [], [], [], []\n",
    "\n",
    "    for i, batch in enumerate(islice(cycle(loader), n_warmup + n_measure)):\n",
    "\n",
    "        # Warmup (same ops, no timing)\n",
    "        if i < n_warmup:\n",
    "            with torch.no_grad():\n",
    "                x = lpg_store.gather(batch.lpg_global_node_idx).to(device)\n",
    "                g = gat_model(x, batch.lpg_edge_index.to(device), batch.lpg_batch.to(device))\n",
    "                gt = gllm.project(g)\n",
    "                ids, mask = tokenize_questions(batch.questions)\n",
    "                _ = gllm.forward_prefill(gt, ids, mask)\n",
    "            continue\n",
    "\n",
    "        timings = {}\n",
    "\n",
    "        # Stage 1: SUBGRAPH_LOAD\n",
    "        with CPUTimer('SUBGRAPH_LOAD') as t:\n",
    "            global_idx = batch.lpg_global_node_idx\n",
    "            edge_index = batch.lpg_edge_index\n",
    "            batch_vec = batch.lpg_batch\n",
    "        timings['SUBGRAPH_LOAD'] = t.result()\n",
    "\n",
    "        # Stage 2: LPG_CPU_GATHER_X -- CPU feature store indexing\n",
    "        with CPUTimer('LPG_CPU_GATHER_X') as t:\n",
    "            x_cpu = lpg_store.gather(global_idx)  # CPU tensor [sum(N_i), 256]\n",
    "        timings['LPG_CPU_GATHER_X'] = t.result()\n",
    "\n",
    "        # Stage 3: LPG_H2D_COPY -- sync transfer to GPU\n",
    "        with CUDATimer('LPG_H2D_COPY') as t:\n",
    "            x_gpu = x_cpu.to(device, non_blocking=False)\n",
    "            ei_gpu = edge_index.to(device, non_blocking=False)\n",
    "            b_gpu = batch_vec.to(device, non_blocking=False)\n",
    "        timings['LPG_H2D_COPY'] = t.result()\n",
    "        timings['LPG_H2D_COPY'].bytes_transferred = (\n",
    "            x_cpu.nelement() * x_cpu.element_size() +\n",
    "            edge_index.nelement() * edge_index.element_size() +\n",
    "            batch_vec.nelement() * batch_vec.element_size()\n",
    "        )\n",
    "\n",
    "        # Stage 4: LPG_GAT_FWD -- GAT layers + global_mean_pool\n",
    "        with torch.no_grad():\n",
    "            with CUDATimer('LPG_GAT_FWD') as t:\n",
    "                graph_emb = gat_model(x_gpu, ei_gpu, b_gpu)  # [B, 256]\n",
    "        timings['LPG_GAT_FWD'] = t.result()\n",
    "\n",
    "        # Stage 5: PROJECTION -- Linear(256 -> LLM_hidden)\n",
    "        with torch.no_grad():\n",
    "            with CUDATimer('PROJECTION') as t:\n",
    "                graph_tokens = gllm.project(graph_emb)\n",
    "        timings['PROJECTION'] = t.result()\n",
    "\n",
    "        # Stage 6: LLM_PREFILL -- LLM forward with graph soft tokens\n",
    "        with torch.no_grad():\n",
    "            with CUDATimer('LLM_PREFILL') as t:\n",
    "                input_ids, attn_mask = tokenize_questions(batch.questions)\n",
    "                _ = gllm.forward_prefill(graph_tokens, input_ids, attn_mask)\n",
    "        timings['LLM_PREFILL'] = t.result()\n",
    "\n",
    "        timings_list.append(timings)\n",
    "        batch_sizes.append(batch.batch_size)\n",
    "        node_counts.append(x_cpu.shape[0])\n",
    "        edge_counts.append(edge_index.shape[1])\n",
    "\n",
    "    return timings_list, batch_sizes, node_counts, edge_counts\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Pass 1: Custom Timers -- DistMult (RDF)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "def profile_rdf_distmult(rdf_store, kge_proj, gllm, loader, n_warmup, n_measure):\n",
    "    kge_proj.eval()\n",
    "    timings_list, batch_sizes, node_counts, edge_counts = [], [], [], []\n",
    "\n",
    "    for i, batch in enumerate(islice(cycle(loader), n_warmup + n_measure)):\n",
    "        ei = batch.rdf_edge_index\n",
    "        if ei.shape[1] == 0:\n",
    "            continue\n",
    "\n",
    "        if i < n_warmup:\n",
    "            with torch.no_grad():\n",
    "                hg = batch.rdf_global_node_idx[ei[0]]\n",
    "                h, r = rdf_store.lookup(hg, batch.rdf_edge_type)\n",
    "                tg = batch.rdf_batch[ei[0]].to(device)\n",
    "                te = h.to(device) * r.to(device)\n",
    "                ge = scatter(te, tg, dim=0, dim_size=batch.batch_size, reduce='mean')\n",
    "                ge = kge_proj(ge)\n",
    "                gt = gllm.project(ge)\n",
    "                ids, mask = tokenize_questions(batch.questions)\n",
    "                _ = gllm.forward_prefill(gt, ids, mask)\n",
    "            continue\n",
    "\n",
    "        timings = {}\n",
    "\n",
    "        # Stage 1: SUBGRAPH_LOAD\n",
    "        with CPUTimer('SUBGRAPH_LOAD') as t:\n",
    "            head_local = ei[0]\n",
    "            global_idx = batch.rdf_global_node_idx\n",
    "            head_global = global_idx[head_local]\n",
    "            edge_type = batch.rdf_edge_type\n",
    "            rdf_batch_vec = batch.rdf_batch\n",
    "        timings['SUBGRAPH_LOAD'] = t.result()\n",
    "\n",
    "        # Stage 2: RDF_KGE_CPU_LOOKUP_ENT_REL -- CPU embedding table lookup\n",
    "        with CPUTimer('RDF_KGE_CPU_LOOKUP_ENT_REL') as t:\n",
    "            h_emb_cpu, r_emb_cpu = rdf_store.lookup(head_global, edge_type)\n",
    "        timings['RDF_KGE_CPU_LOOKUP_ENT_REL'] = t.result()\n",
    "\n",
    "        # Stage 3: RDF_KGE_H2D_COPY -- sync transfer to GPU\n",
    "        with CUDATimer('RDF_KGE_H2D_COPY') as t:\n",
    "            h_emb_gpu = h_emb_cpu.to(device, non_blocking=False)\n",
    "            r_emb_gpu = r_emb_cpu.to(device, non_blocking=False)\n",
    "            triple_graph = rdf_batch_vec[head_local].to(device, non_blocking=False)\n",
    "        timings['RDF_KGE_H2D_COPY'] = t.result()\n",
    "        timings['RDF_KGE_H2D_COPY'].bytes_transferred = (\n",
    "            h_emb_cpu.nelement() * h_emb_cpu.element_size() +\n",
    "            r_emb_cpu.nelement() * r_emb_cpu.element_size() +\n",
    "            head_local.nelement() * 8\n",
    "        )\n",
    "\n",
    "        # Stage 4: RDF_KGE_FWD_DISTMULT -- h*r (DistMult) + scatter + proj\n",
    "        with torch.no_grad():\n",
    "            with CUDATimer('RDF_KGE_FWD_DISTMULT') as t:\n",
    "                triple_emb = h_emb_gpu * r_emb_gpu  # DistMult: element-wise product\n",
    "                graph_emb_raw = scatter(triple_emb, triple_graph, dim=0,\n",
    "                                        dim_size=batch.batch_size, reduce='mean')\n",
    "                graph_emb = kge_proj(graph_emb_raw)  # [B, 256]\n",
    "        timings['RDF_KGE_FWD_DISTMULT'] = t.result()\n",
    "\n",
    "        # Stage 5: PROJECTION\n",
    "        with torch.no_grad():\n",
    "            with CUDATimer('PROJECTION') as t:\n",
    "                graph_tokens = gllm.project(graph_emb)\n",
    "        timings['PROJECTION'] = t.result()\n",
    "\n",
    "        # Stage 6: LLM_PREFILL\n",
    "        with torch.no_grad():\n",
    "            with CUDATimer('LLM_PREFILL') as t:\n",
    "                input_ids, attn_mask = tokenize_questions(batch.questions)\n",
    "                _ = gllm.forward_prefill(graph_tokens, input_ids, attn_mask)\n",
    "        timings['LLM_PREFILL'] = t.result()\n",
    "\n",
    "        timings_list.append(timings)\n",
    "        batch_sizes.append(batch.batch_size)\n",
    "        node_counts.append(global_idx.shape[0])\n",
    "        edge_counts.append(ei.shape[1])\n",
    "\n",
    "    return timings_list, batch_sizes, node_counts, edge_counts\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Run Pass 1: Custom Timer Profiling\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "all_profile_results = {}\n",
    "\n",
    "print('=' * 60)\n",
    "print('Pass 1: Custom CUDA Event Timer Profiling')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\n[GAT (LPG)]')\n",
    "tl, bs, nc, ec = profile_lpg(\n",
    "    prof_gat, lpg_cpu_store, graph_llm, profile_loader, WARMUP_BATCHES, PROFILE_BATCHES)\n",
    "gat_df = aggregate_timings(tl, bs, nc, ec)\n",
    "all_profile_results['gat'] = {'df': gat_df, 'raw': tl, 'bs': bs, 'nc': nc, 'ec': ec}\n",
    "print_profiling_table(gat_df, 'GAT (LPG)', len(tl))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n[DistMult (RDF)]')\n",
    "tl, bs, nc, ec = profile_rdf_distmult(\n",
    "    rdf_cpu_store, prof_kge_proj, graph_llm, profile_loader, WARMUP_BATCHES, PROFILE_BATCHES)\n",
    "dm_df = aggregate_timings(tl, bs, nc, ec)\n",
    "all_profile_results['distmult'] = {'df': dm_df, 'raw': tl, 'bs': bs, 'nc': nc, 'ec': ec}\n",
    "print_profiling_table(dm_df, 'DistMult (RDF)', len(tl))\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "# Pass 2: torch.profiler + record_function -> Chrome Trace\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "\n",
    "TRACE_BATCHES = 20  # More batches for meaningful Chrome trace\n",
    "print(f'\\n\\n{\"=\" * 60}')\n",
    "print(f'Pass 2: torch.profiler trace ({TRACE_BATCHES} batches)')\n",
    "print(f'{\"=\" * 60}')\n",
    "\n",
    "\n",
    "def profiler_lpg_pass(gat_model, lpg_store, gllm, loader, n_batches):\n",
    "    gat_model.eval()\n",
    "    batch_iter = islice(cycle(loader), n_batches)\n",
    "    activities = [ProfilerActivity.CPU] + ([ProfilerActivity.CUDA] if HAS_CUDA else [])\n",
    "\n",
    "    with torch_profile(activities=activities, record_shapes=True, profile_memory=True) as prof:\n",
    "        for batch in batch_iter:\n",
    "            with torch.no_grad():\n",
    "                with record_function(\"SUBGRAPH_LOAD\"):\n",
    "                    global_idx = batch.lpg_global_node_idx\n",
    "                    edge_index = batch.lpg_edge_index\n",
    "                    batch_vec = batch.lpg_batch\n",
    "\n",
    "                with record_function(\"LPG_CPU_GATHER_X\"):\n",
    "                    x_cpu = lpg_store.gather(global_idx)\n",
    "\n",
    "                with record_function(\"LPG_H2D_COPY\"):\n",
    "                    x_gpu = x_cpu.to(device, non_blocking=False)\n",
    "                    ei_gpu = edge_index.to(device, non_blocking=False)\n",
    "                    b_gpu = batch_vec.to(device, non_blocking=False)\n",
    "\n",
    "                with record_function(\"LPG_GAT_FWD\"):\n",
    "                    graph_emb = gat_model(x_gpu, ei_gpu, b_gpu)\n",
    "\n",
    "                with record_function(\"PROJECTION\"):\n",
    "                    graph_tokens = gllm.project(graph_emb)\n",
    "\n",
    "                with record_function(\"LLM_PREFILL\"):\n",
    "                    ids, mask = tokenize_questions(batch.questions)\n",
    "                    _ = gllm.forward_prefill(graph_tokens, ids, mask)\n",
    "    return prof\n",
    "\n",
    "\n",
    "def profiler_rdf_pass(rdf_store, kge_proj, gllm, loader, n_batches):\n",
    "    kge_proj.eval()\n",
    "    batch_iter = islice(cycle(loader), n_batches)\n",
    "    activities = [ProfilerActivity.CPU] + ([ProfilerActivity.CUDA] if HAS_CUDA else [])\n",
    "\n",
    "    with torch_profile(activities=activities, record_shapes=True, profile_memory=True) as prof:\n",
    "        for batch in batch_iter:\n",
    "            ei = batch.rdf_edge_index\n",
    "            if ei.shape[1] == 0:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                with record_function(\"SUBGRAPH_LOAD\"):\n",
    "                    head_local = ei[0]\n",
    "                    head_global = batch.rdf_global_node_idx[head_local]\n",
    "                    edge_type = batch.rdf_edge_type\n",
    "                    rdf_batch_vec = batch.rdf_batch\n",
    "\n",
    "                with record_function(\"RDF_KGE_CPU_LOOKUP_ENT_REL\"):\n",
    "                    h_emb_cpu, r_emb_cpu = rdf_store.lookup(head_global, edge_type)\n",
    "\n",
    "                with record_function(\"RDF_KGE_H2D_COPY\"):\n",
    "                    h_emb_gpu = h_emb_cpu.to(device, non_blocking=False)\n",
    "                    r_emb_gpu = r_emb_cpu.to(device, non_blocking=False)\n",
    "                    triple_graph = rdf_batch_vec[head_local].to(device, non_blocking=False)\n",
    "\n",
    "                with record_function(\"RDF_KGE_FWD_DISTMULT\"):\n",
    "                    triple_emb = h_emb_gpu * r_emb_gpu\n",
    "                    graph_emb_raw = scatter(triple_emb, triple_graph, dim=0,\n",
    "                                            dim_size=batch.batch_size, reduce='mean')\n",
    "                    graph_emb = kge_proj(graph_emb_raw)\n",
    "\n",
    "                with record_function(\"PROJECTION\"):\n",
    "                    graph_tokens = gllm.project(graph_emb)\n",
    "\n",
    "                with record_function(\"LLM_PREFILL\"):\n",
    "                    ids, mask = tokenize_questions(batch.questions)\n",
    "                    _ = gllm.forward_prefill(graph_tokens, ids, mask)\n",
    "    return prof\n",
    "\n",
    "\n",
    "# Run torch.profiler passes\n",
    "n_loader_batches = len(profile_loader)\n",
    "print(f'\\nDataLoader has {n_loader_batches} batches (BS={PROFILE_BATCH_SIZE}), '\n",
    "      f'TRACE_BATCHES={TRACE_BATCHES} (cycling if needed)')\n",
    "\n",
    "print('\\n[GAT (LPG)] torch.profiler trace...')\n",
    "gat_prof = profiler_lpg_pass(prof_gat, lpg_cpu_store, graph_llm, profile_loader, TRACE_BATCHES)\n",
    "\n",
    "print('[DistMult (RDF)] torch.profiler trace...')\n",
    "rdf_prof = profiler_rdf_pass(rdf_cpu_store, prof_kge_proj, graph_llm, profile_loader, TRACE_BATCHES)\n",
    "\n",
    "# Print key_averages summary\n",
    "print('\\n--- GAT (LPG) torch.profiler key_averages ---')\n",
    "print(gat_prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
    "\n",
    "print('\\n--- DistMult (RDF) torch.profiler key_averages ---')\n",
    "print(rdf_prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=15))\n",
    "\n",
    "# Export Chrome traces\n",
    "gat_prof.export_chrome_trace('gat_profile_trace.json')\n",
    "rdf_prof.export_chrome_trace('rdf_distmult_profile_trace.json')\n",
    "print('\\nChrome traces saved:')\n",
    "print('  gat_profile_trace.json')\n",
    "print('  rdf_distmult_profile_trace.json')\n",
    "print('  View at: chrome://tracing or https://ui.perfetto.dev/')\n",
    "torch.cuda.empty_cache()\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 9: Combined Comparison Table + Bottleneck Analysis\n\n**Objective:** Side-by-side comparison of GAT vs DistMult profiling results.\n\n**Outputs:**\n1. **Combined table** — Unified stage names × 2 models (mean ms, p95 ms, % of total)\n2. **Bottleneck identification** — Which stage dominates each model's latency?\n3. **Encoder vs LLM ratio** — How much time is graph encoding vs LLM prefill?\n4. **H2D bandwidth** — Measured vs theoretical PCIe bandwidth (T4: 12 GB/s, A100: 32 GB/s)\n5. **Graph size → latency correlation** — Spearman rho with p-value"
  },
  {
   "cell_type": "code",
   "source": "# Cell 9: Combined Comparison Table + Bottleneck Analysis\n\nMODEL_LABELS = {'gat': 'GAT (LPG)', 'distmult': 'DistMult (RDF)'}\nMODEL_KEYS = ['gat', 'distmult']\n\n# Unified stage names for cross-model comparison\nUNIFIED_STAGES = [\n    'SUBGRAPH_LOAD',\n    'CPU_GATHER/LOOKUP',  # LPG_CPU_GATHER_X or RDF_KGE_CPU_LOOKUP_ENT_REL\n    'H2D_COPY',           # LPG_H2D_COPY or RDF_KGE_H2D_COPY\n    'ENCODER_FWD',        # LPG_GAT_FWD or RDF_KGE_FWD_DISTMULT\n    'PROJECTION',\n    'LLM_PREFILL',\n]\n\n# Map unified names to model-specific stage names\nSTAGE_MAP = {\n    'gat': {\n        'SUBGRAPH_LOAD': 'SUBGRAPH_LOAD',\n        'CPU_GATHER/LOOKUP': 'LPG_CPU_GATHER_X',\n        'H2D_COPY': 'LPG_H2D_COPY',\n        'ENCODER_FWD': 'LPG_GAT_FWD',\n        'PROJECTION': 'PROJECTION',\n        'LLM_PREFILL': 'LLM_PREFILL',\n    },\n    'distmult': {\n        'SUBGRAPH_LOAD': 'SUBGRAPH_LOAD',\n        'CPU_GATHER/LOOKUP': 'RDF_KGE_CPU_LOOKUP_ENT_REL',\n        'H2D_COPY': 'RDF_KGE_H2D_COPY',\n        'ENCODER_FWD': 'RDF_KGE_FWD_DISTMULT',\n        'PROJECTION': 'PROJECTION',\n        'LLM_PREFILL': 'LLM_PREFILL',\n    },\n}\n\n\ndef get_stage_val(df, stage_name, col='cpu_mean'):\n    row = df[df['stage'] == stage_name]\n    return row[col].values[0] if len(row) > 0 else 0.0\n\n\n# ── Combined Table ──\nprint(f'\\n{\"=\"*75}')\nprint(f'G-Retrieval E2E Bottleneck Analysis -- {PROFILE_BATCHES} batches, BS={PROFILE_BATCH_SIZE}, D={GRAPH_DIM}')\nprint(f'{\"=\"*75}')\n\n# Header\nprint(f'{\"Stage\":<25}', end='')\nfor mk in MODEL_KEYS:\n    print(f'  {\"mean\":>6}  {\"p95\":>6}  {\"%\":>5}', end='')\nprint()\nprint(f'{\"\":<25}', end='')\nfor mk in MODEL_KEYS:\n    print(f'  {MODEL_LABELS[mk]:^20}', end='')\nprint()\nprint('-' * 75)\n\n# Per-model totals\nmodel_totals = {}\nfor mk in MODEL_KEYS:\n    df = all_profile_results[mk]['df']\n    total = sum(get_stage_val(df, STAGE_MAP[mk][s]) for s in UNIFIED_STAGES)\n    model_totals[mk] = total\n\n# Print rows\nbottleneck = {}\nfor stage in UNIFIED_STAGES:\n    print(f'{stage:<25}', end='')\n    for mk in MODEL_KEYS:\n        df = all_profile_results[mk]['df']\n        actual_stage = STAGE_MAP[mk][stage]\n        mean_ms = get_stage_val(df, actual_stage, 'cpu_mean')\n        p95_ms = get_stage_val(df, actual_stage, 'cpu_p95')\n        pct = mean_ms / model_totals[mk] * 100 if model_totals[mk] > 0 else 0\n        print(f'  {mean_ms:6.2f}  {p95_ms:6.2f}  {pct:4.1f}%', end='')\n        if mk not in bottleneck or pct > bottleneck[mk][1]:\n            bottleneck[mk] = (stage, pct)\n    print()\n\nprint('-' * 75)\nprint(f'{\"TOTAL\":<25}', end='')\nfor mk in MODEL_KEYS:\n    print(f'  {model_totals[mk]:6.2f}  {\"\":>6}  {\"\":>5}', end='')\nprint()\n\n# ── Bottleneck ID ──\nprint(f'\\n{\"=\"*75}')\nprint('Bottleneck Identification:')\nfor mk in MODEL_KEYS:\n    stage, pct = bottleneck[mk]\n    print(f'  {MODEL_LABELS[mk]:20s}: {stage} ({pct:.1f}%)')\n\n# ── Encoder vs LLM ──\nprint(f'\\nEncoder vs LLM Time Ratio:')\nfor mk in MODEL_KEYS:\n    df = all_profile_results[mk]['df']\n    enc_ms = get_stage_val(df, STAGE_MAP[mk]['ENCODER_FWD'])\n    proj_ms = get_stage_val(df, STAGE_MAP[mk]['PROJECTION'])\n    llm_ms = get_stage_val(df, STAGE_MAP[mk]['LLM_PREFILL'])\n    enc_total = enc_ms + proj_ms\n    ratio = enc_total / llm_ms if llm_ms > 0 else float('inf')\n    print(f'  {MODEL_LABELS[mk]:20s}: Encoder+Proj={enc_total:.2f}ms, LLM={llm_ms:.2f}ms, ratio={ratio:.3f}x')\n\n# ── H2D Bandwidth ──\nprint(f'\\nH2D Transfer Bandwidth:')\nif HAS_CUDA:\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    theoretical_bw = 32.0 if any(x in gpu_name for x in ['a100', 'l4']) else 16.0 if 'v100' in gpu_name else 12.0\n    print(f'  Theoretical PCIe BW: ~{theoretical_bw:.0f} GB/s ({gpu_name})')\nelse:\n    theoretical_bw = None\n\nfor mk in MODEL_KEYS:\n    df = all_profile_results[mk]['df']\n    h2d_stage = STAGE_MAP[mk]['H2D_COPY']\n    h2d_row = df[df['stage'] == h2d_stage]\n    if len(h2d_row) > 0:\n        bytes_mean = h2d_row['bytes_mean'].values[0]\n        cuda_mean = h2d_row['cuda_mean'].values[0]\n        if not np.isnan(cuda_mean) and cuda_mean > 0:\n            bw_gbps = (bytes_mean / 1e9) / (cuda_mean / 1e3)\n            eff = f' ({bw_gbps/theoretical_bw*100:.0f}% eff)' if theoretical_bw else ''\n            print(f'  {MODEL_LABELS[mk]:20s}: {bytes_mean/1e6:.2f} MB, {cuda_mean:.2f} ms -> {bw_gbps:.1f} GB/s{eff}')\n\n# ── Graph Size -> Latency Correlation ──\nprint(f'\\nGraph Size -> Total Latency Correlation (Spearman):')\nfrom scipy.stats import spearmanr as _spearmanr\nfor mk in MODEL_KEYS:\n    res = all_profile_results[mk]\n    sizes = np.array(res['nc'] if mk == 'gat' else res['ec'])\n    total_ms = np.array([sum(t[s].cpu_ms for s in t) for t in res['raw']])\n    if len(sizes) > 3:\n        corr, pval = _spearmanr(sizes, total_ms)\n        label = \"nodes\" if mk == 'gat' else \"triples\"\n        print(f'  {MODEL_LABELS[mk]:20s}: rho={corr:.3f}, p={pval:.3e} ({label})')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 10: Profiling Visualization\n\n**Objective:** Visual summary of the profiling results in 4 charts.\n\n**Charts:**\n1. **Stacked bar** — Stage-wise latency breakdown (ms) per model, with total labels\n2. **Scatter + trend** — Graph size (nodes/triples) vs total E2E latency\n3. **Pie charts** — Time distribution per model (% of total)\n4. **Encoder vs LLM** — Horizontal bar comparing graph encoding time vs LLM prefill time\n5. **Label reference** — `record_function` label names + Chrome trace file paths"
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Profiling Visualization\n#\n# 4 charts: stacked bar, scatter, pie charts, encoder-vs-LLM\n\nfig = plt.figure(figsize=(16, 12))\n\n# Stage colors\nstage_colors = {\n    'SUBGRAPH_LOAD': '#2196F3',\n    'CPU_GATHER/LOOKUP': '#FF9800',\n    'H2D_COPY': '#F44336',\n    'ENCODER_FWD': '#4CAF50',\n    'PROJECTION': '#9C27B0',\n    'LLM_PREFILL': '#795548',\n}\n\n\n# ── Chart 1: Stacked Bar ──\nax1 = fig.add_subplot(2, 2, 1)\nx_pos = np.arange(len(MODEL_KEYS))\nbar_width = 0.4\nbottom = np.zeros(len(MODEL_KEYS))\n\nfor stage in UNIFIED_STAGES:\n    vals = []\n    for mk in MODEL_KEYS:\n        df = all_profile_results[mk]['df']\n        actual = STAGE_MAP[mk][stage]\n        vals.append(get_stage_val(df, actual))\n    vals = np.array(vals)\n    ax1.bar(x_pos, vals, bar_width, bottom=bottom,\n            label=stage, color=stage_colors[stage], edgecolor='white', linewidth=0.5)\n    bottom += vals\n\nax1.set_xticks(x_pos)\nax1.set_xticklabels([MODEL_LABELS[mk] for mk in MODEL_KEYS], fontsize=10)\nax1.set_ylabel('Time (ms)')\nax1.set_title(f'E2E Latency Breakdown (D={GRAPH_DIM})')\nax1.legend(fontsize=7, loc='upper left')\nfor i, mk in enumerate(MODEL_KEYS):\n    ax1.text(i, bottom[i] + 0.5, f'{bottom[i]:.1f}ms', ha='center', fontsize=9, fontweight='bold')\n\n\n# ── Chart 2: Scatter -- Graph Size vs Latency ──\nax2 = fig.add_subplot(2, 2, 2)\nscatter_colors = {'gat': '#4CAF50', 'distmult': '#FF9800'}\n\nfor mk in MODEL_KEYS:\n    res = all_profile_results[mk]\n    sizes = np.array(res['nc'] if mk == 'gat' else res['ec'])\n    total_ms = np.array([sum(t[s].cpu_ms for s in t) for t in res['raw']])\n    ax2.scatter(sizes, total_ms, alpha=0.5, s=25, color=scatter_colors[mk], label=MODEL_LABELS[mk])\n    if len(sizes) > 3:\n        z = np.polyfit(sizes, total_ms, 1)\n        p = np.poly1d(z)\n        x_line = np.linspace(sizes.min(), sizes.max(), 50)\n        ax2.plot(x_line, p(x_line), '--', color=scatter_colors[mk], alpha=0.7, linewidth=1.5)\n\nax2.set_xlabel('Graph Size (nodes for GAT, triples for DistMult)')\nax2.set_ylabel('Total Latency (ms)')\nax2.set_title('Graph Size vs E2E Latency')\nax2.legend(fontsize=9)\n\n\n# ── Chart 3: Pie Charts ──\nfor idx, mk in enumerate(MODEL_KEYS):\n    ax = fig.add_subplot(2, 4, 5 + idx)\n    df = all_profile_results[mk]['df']\n    sizes_pie, colors_pie, labels_pie = [], [], []\n    for stage in UNIFIED_STAGES:\n        actual = STAGE_MAP[mk][stage]\n        val = get_stage_val(df, actual)\n        if val > 0:\n            sizes_pie.append(val)\n            colors_pie.append(stage_colors[stage])\n            labels_pie.append(stage)\n    ax.pie(sizes_pie, labels=None, colors=colors_pie,\n           autopct=lambda p: f'{p:.0f}%' if p > 5 else '',\n           pctdistance=0.75, startangle=90, textprops={'fontsize': 7})\n    ax.set_title(MODEL_LABELS[mk], fontsize=10, fontweight='bold')\n\n\n# ── Chart 4: Encoder vs LLM ──\nax4 = fig.add_subplot(2, 4, 7)\nenc_times, llm_times = [], []\nfor mk in MODEL_KEYS:\n    df = all_profile_results[mk]['df']\n    enc = get_stage_val(df, STAGE_MAP[mk]['ENCODER_FWD']) + get_stage_val(df, STAGE_MAP[mk]['PROJECTION'])\n    llm = get_stage_val(df, STAGE_MAP[mk]['LLM_PREFILL'])\n    enc_times.append(enc)\n    llm_times.append(llm)\n\nx_bar = np.arange(len(MODEL_KEYS))\nbar_w = 0.35\nax4.barh(x_bar - bar_w/2, enc_times, bar_w, label='Encoder+Proj', color='#4CAF50')\nax4.barh(x_bar + bar_w/2, llm_times, bar_w, label='LLM Prefill', color='#795548')\nax4.set_yticks(x_bar)\nax4.set_yticklabels([mk.upper() for mk in MODEL_KEYS], fontsize=9)\nax4.set_xlabel('Time (ms)')\nax4.set_title('Encoder vs LLM', fontsize=10)\nax4.legend(fontsize=8)\n\n# ── Chart 5 (4th position): record_function label reference ──\nax5 = fig.add_subplot(2, 4, 8)\nax5.axis('off')\nlabel_text = 'record_function labels:\\n\\n'\nlabel_text += 'GAT (LPG):\\n'\nfor s in STAGE_MAP['gat'].values():\n    label_text += f'  {s}\\n'\nlabel_text += '\\nDistMult (RDF):\\n'\nfor s in STAGE_MAP['distmult'].values():\n    label_text += f'  {s}\\n'\nlabel_text += '\\nChrome traces:\\n  gat_profile_trace.json\\n  rdf_distmult_profile_trace.json'\nax5.text(0.05, 0.95, label_text, transform=ax5.transAxes, fontsize=7,\n         verticalalignment='top', fontfamily='monospace',\n         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\nplt.suptitle(f'G-Retrieval E2E Profiling (D={GRAPH_DIM})', fontsize=14, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.show()\n\nprint('Track B profiling complete. Proceeding to Track A (retrieval evaluation)...')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Track B-2: Attention Map Analysis\n\nHow do graph soft tokens influence the LLM's self-attention?\n\n- **Graph tokens** are prepended at positions `0..NUM_GRAPH_TOKENS-1`\n- **Text tokens** follow at positions `NUM_GRAPH_TOKENS..`\n- We extract attention weights from every layer/head and measure how much text tokens \"look at\" graph tokens\n- Compare GAT vs DistMult: does the graph encoder affect attention patterns?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 11: Attention Map Analysis — Graph Token Influence\n\n**Objective:** Visualize how the LLM's self-attention interacts with prepended graph soft tokens.\n\n**Method:**\n1. Run LLM forward with `output_attentions=True`\n2. Extract attention weights from all 32 layers × 32 heads\n3. Measure: `attention[text_positions → graph_positions]` per layer/head\n4. Aggregate across 8 samples for stability\n\n**Visualizations (4 charts):**\n1. **Layer × Head heatmap** — GAT and DistMult side-by-side; top-5 (layer, head) pairs marked with ★\n2. **Per-layer attention curve** — Mean graph attention across layers (early/mid/late pattern)\n3. **Per-position attention** — Which text tokens attend most to graph? (mid-layer average)\n4. Uniform baseline reference line: `T / seq_len`\n\n**Key questions:**\n- Do early layers (feature extraction) or late layers (reasoning) attend more to graph tokens?\n- Does graph encoder choice (GAT vs DistMult) change the attention pattern?\n- Are there specialized \"graph-reading\" attention heads?"
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: Attention Map Analysis — Graph Token Influence\n#\n# Extract attention weights from every LLM layer/head and measure\n# how much text tokens attend to the prepended graph soft tokens.\n\nN_ATTN_SAMPLES = 8  # Number of samples to analyze (memory-intensive)\nNUM_LAYERS = llm.config.num_hidden_layers  # 32 for Llama 3.1 8B\nNUM_HEADS = llm.config.num_attention_heads  # 32 for Llama 3.1 8B\n\nprint(f'Attention analysis: {NUM_LAYERS} layers x {NUM_HEADS} heads, '\n      f'{NUM_GRAPH_TOKENS} graph tokens, {N_ATTN_SAMPLES} samples')\n\n\n@torch.no_grad()\ndef extract_attention_to_graph(gllm, graph_emb, questions, n_graph_tokens):\n    \"\"\"Run LLM with output_attentions=True, extract attention to graph tokens.\n\n    Returns:\n        attn_to_graph: [num_layers, num_heads] mean attention from text -> graph tokens\n        token_attn:    [num_layers, seq_len] per-position attention to graph (head-averaged)\n        tokens:        list of token strings for labeling\n    \"\"\"\n    graph_tokens = gllm.project(graph_emb[:1])  # [1, T, H]\n\n    enc = tokenizer(\n        PROMPT_TEMPLATE.format(question=questions[0]),\n        return_tensors='pt', truncation=True, max_length=128,\n    )\n    input_ids = enc['input_ids'].to(device)\n    attention_mask = enc['attention_mask'].to(device)\n\n    text_emb = gllm.llm.get_input_embeddings()(input_ids)\n    combined = torch.cat([graph_tokens, text_emb], dim=1)\n    graph_mask = torch.ones(1, n_graph_tokens, device=device, dtype=attention_mask.dtype)\n    combined_mask = torch.cat([graph_mask, attention_mask], dim=1)\n\n    outputs = gllm.llm(\n        inputs_embeds=combined,\n        attention_mask=combined_mask,\n        output_attentions=True,\n    )\n\n    # outputs.attentions: tuple of [1, num_heads, seq_len, seq_len] per layer\n    num_layers = len(outputs.attentions)\n    seq_len = outputs.attentions[0].shape[2]\n\n    # Mean attention from text tokens -> graph tokens, per layer x head\n    attn_to_graph = torch.zeros(num_layers, NUM_HEADS)\n    # Per-position attention to graph tokens (averaged across heads)\n    token_attn = torch.zeros(num_layers, seq_len)\n\n    for layer_idx, attn in enumerate(outputs.attentions):\n        # attn: [1, heads, seq, seq]\n        a = attn[0]  # [heads, seq, seq]\n\n        # Attention from text positions to graph positions\n        # text positions: n_graph_tokens..seq_len\n        # graph positions: 0..n_graph_tokens\n        text_to_graph = a[:, n_graph_tokens:, :n_graph_tokens]  # [heads, text_len, T]\n        attn_to_graph[layer_idx] = text_to_graph.sum(dim=-1).mean(dim=-1).cpu()  # [heads]\n\n        # Per-position: how much each position attends to graph (head-averaged)\n        pos_to_graph = a[:, :, :n_graph_tokens].sum(dim=-1).mean(dim=0).cpu()  # [seq]\n        token_attn[layer_idx] = pos_to_graph\n\n    # Token labels for x-axis\n    token_ids = input_ids[0].cpu().tolist()\n    token_strs = ['[G]'] * n_graph_tokens + [\n        tokenizer.decode([t]).strip()[:10] for t in token_ids\n    ]\n\n    return attn_to_graph, token_attn, token_strs\n\n\n# ── Extract attention for both encoders ──\nattn_results = {}\nsample_batch = next(iter(profile_loader))\n\nfor enc_name in ['gat', 'distmult']:\n    print(f'\\n[{enc_name.upper()}] Extracting attention maps...')\n\n    if enc_name == 'gat':\n        x_cpu = lpg_cpu_store.gather(sample_batch.lpg_global_node_idx)\n        graph_emb = prof_gat(\n            x_cpu.to(device),\n            sample_batch.lpg_edge_index.to(device),\n            sample_batch.lpg_batch.to(device),\n        )\n    else:\n        ei = sample_batch.rdf_edge_index\n        head_local = ei[0]\n        head_global = sample_batch.rdf_global_node_idx[head_local]\n        h_emb, r_emb = rdf_cpu_store.lookup(head_global, sample_batch.rdf_edge_type)\n        triple_graph = sample_batch.rdf_batch[head_local].to(device)\n        triple_emb = h_emb.to(device) * r_emb.to(device)\n        graph_emb_raw = scatter(triple_emb, triple_graph, dim=0,\n                                dim_size=sample_batch.batch_size, reduce='mean')\n        graph_emb = prof_kge_proj(graph_emb_raw)\n\n    # Aggregate across multiple samples\n    all_attn_to_graph = []\n    n = min(N_ATTN_SAMPLES, sample_batch.batch_size)\n    for s in range(n):\n        a2g, tok_a, tok_s = extract_attention_to_graph(\n            graph_llm, graph_emb[s:s+1], [sample_batch.questions[s]], NUM_GRAPH_TOKENS)\n        all_attn_to_graph.append(a2g)\n\n    mean_attn = torch.stack(all_attn_to_graph).mean(dim=0)  # [layers, heads]\n    attn_results[enc_name] = {\n        'attn_to_graph': mean_attn,\n        'last_token_attn': tok_a,    # from last sample\n        'token_strs': tok_s,\n    }\n    print(f'  Mean attention to graph tokens: {mean_attn.mean():.4f}')\n    print(f'  Max (layer, head): layer={mean_attn.max(dim=1).values.argmax().item()}, '\n          f'head={mean_attn.max(dim=0).values.argmax().item()}, val={mean_attn.max():.4f}')\n\ntorch.cuda.empty_cache()\n\n\n# ══════════════════════════════════════════════════════\n# Visualization\n# ══════════════════════════════════════════════════════\n\nfig = plt.figure(figsize=(20, 16))\n\n# ── Chart 1 & 2: Layer x Head heatmaps (GAT vs DistMult) ──\nfor idx, enc_name in enumerate(['gat', 'distmult']):\n    ax = fig.add_subplot(2, 2, idx + 1)\n    a2g = attn_results[enc_name]['attn_to_graph'].numpy()\n    im = ax.imshow(a2g, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n    ax.set_xlabel('Head')\n    ax.set_ylabel('Layer')\n    ax.set_title(f'{enc_name.upper()}: Text -> Graph Token Attention')\n    plt.colorbar(im, ax=ax, shrink=0.8, label='Attention weight')\n\n    # Mark top-5 (layer, head) pairs\n    flat_idx = np.argsort(a2g.ravel())[-5:]\n    for fi in flat_idx:\n        ly, hd = np.unravel_index(fi, a2g.shape)\n        ax.plot(hd, ly, 'k*', markersize=8)\n\n# ── Chart 3: Per-layer mean attention to graph (both models) ──\nax3 = fig.add_subplot(2, 2, 3)\nfor enc_name, color in [('gat', '#4CAF50'), ('distmult', '#FF9800')]:\n    a2g = attn_results[enc_name]['attn_to_graph']\n    layer_mean = a2g.mean(dim=1).numpy()  # [layers]\n    ax3.plot(range(NUM_LAYERS), layer_mean, '-o', color=color,\n             markersize=3, label=f'{enc_name.upper()}', linewidth=1.5)\n\nax3.set_xlabel('Layer')\nax3.set_ylabel('Mean Attention to Graph Tokens')\nax3.set_title('Graph Token Attention Across Layers')\nax3.legend()\nax3.axhline(y=NUM_GRAPH_TOKENS / 128, color='gray', linestyle='--', alpha=0.5,\n            label=f'Uniform baseline ({NUM_GRAPH_TOKENS}/128)')\nax3.legend()\n\n# ── Chart 4: Per-position attention (last sample, both models) ──\nax4 = fig.add_subplot(2, 2, 4)\n# Average across middle layers (layers 8-24 where most reasoning happens)\nmid_start, mid_end = NUM_LAYERS // 4, 3 * NUM_LAYERS // 4\nfor enc_name, color in [('gat', '#4CAF50'), ('distmult', '#FF9800')]:\n    tok_a = attn_results[enc_name]['last_token_attn']\n    mid_avg = tok_a[mid_start:mid_end].mean(dim=0).numpy()  # [seq_len]\n    seq_len = len(mid_avg)\n    ax4.plot(range(seq_len), mid_avg, alpha=0.7, color=color, label=f'{enc_name.upper()}')\n\n# Highlight graph token region\nax4.axvspan(0, NUM_GRAPH_TOKENS - 0.5, alpha=0.15, color='blue', label='Graph tokens')\nax4.set_xlabel('Position')\nax4.set_ylabel(f'Attention to Graph (layers {mid_start}-{mid_end} avg)')\nax4.set_title('Per-Position Attention to Graph Tokens')\nax4.legend(fontsize=8)\n\n# Add token labels (sparse)\ntok_strs = attn_results['gat']['token_strs']\nstep = max(1, len(tok_strs) // 20)\nax4.set_xticks(range(0, len(tok_strs), step))\nax4.set_xticklabels([tok_strs[i] for i in range(0, len(tok_strs), step)],\n                     rotation=45, ha='right', fontsize=6)\n\nplt.suptitle(f'Graph Token Attention Analysis (D={GRAPH_DIM}, T={NUM_GRAPH_TOKENS})',\n             fontsize=14, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.show()\n\n\n# ── Summary statistics ──\nprint('\\n' + '=' * 70)\nprint(f'Attention Analysis Summary (T={NUM_GRAPH_TOKENS} graph tokens)')\nprint('=' * 70)\n\nfor enc_name in ['gat', 'distmult']:\n    a2g = attn_results[enc_name]['attn_to_graph']\n    layer_mean = a2g.mean(dim=1)\n\n    # Which layers attend most to graph?\n    top_layers = layer_mean.argsort(descending=True)[:5].tolist()\n    # Early (0-7), Mid (8-23), Late (24-31) layer groups\n    early = layer_mean[:8].mean().item()\n    mid = layer_mean[8:24].mean().item()\n    late = layer_mean[24:].mean().item()\n\n    print(f'\\n  {enc_name.upper()}:')\n    print(f'    Overall mean attention to graph: {a2g.mean():.4f}')\n    print(f'    Early layers (0-7):   {early:.4f}')\n    print(f'    Mid layers (8-23):    {mid:.4f}')\n    print(f'    Late layers (24-31):  {late:.4f}')\n    print(f'    Top-5 layers: {top_layers}')\n    print(f'    Max attention: layer={a2g.max(dim=1).values.argmax().item()}, '\n          f'head={a2g.max(dim=0).values.argmax().item()}, val={a2g.max():.4f}')\n\n# Compare GAT vs DistMult\ngat_mean = attn_results['gat']['attn_to_graph'].mean().item()\ndm_mean = attn_results['distmult']['attn_to_graph'].mean().item()\nratio = gat_mean / dm_mean if dm_mean > 0 else float('inf')\nprint(f'\\n  GAT vs DistMult attention ratio: {ratio:.2f}x')\nif ratio > 1.1:\n    print('  -> GAT graph tokens receive MORE attention (richer graph signal)')\nelif ratio < 0.9:\n    print('  -> DistMult graph tokens receive MORE attention')\nelse:\n    print('  -> Similar attention levels (graph encoder choice has limited impact on attention)')\n\nprint('\\nTrack B complete.')\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Track A: Representation Quality Evaluation\n\nEvaluate how well learned graph embeddings (384d) capture answer-relevant information via **cosine retrieval** against SentenceTransformer answer embeddings.\n\n**Models compared:** GAT (LPG) vs TransE (RDF) vs DistMult (RDF) vs Question-only baseline\n\n### Cell 6: Graph Embedding Extraction\n\n**Objective:** Extract per-question graph-level embeddings from all 3 trained models (Track A's 384d models).\n\n**Process:**\n- GAT: `forward(x, edge_index, batch)` → `global_mean_pool` → [B, 384]\n- TransE/DistMult: `forward(batch)` → triple aggregation via `scatter(mean)` → [B, 384]\n- All embeddings L2-normalized for cosine similarity\n\n**Outputs:** `train_emb`, `val_emb`, `test_emb` dicts with keys: `gat`, `transe`, `distmult`, `questions`, `answers`, `categories`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Graph Embedding Extraction\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(gat_model, kge_models, loader):\n",
    "    \"\"\"Extract per-question graph embeddings from all models.\n",
    "\n",
    "    Returns dict with:\n",
    "        'gat': [N, 384] LPG-GAT embeddings\n",
    "        'transe': [N, 384] RDF-TransE embeddings\n",
    "        'distmult': [N, 384] RDF-DistMult embeddings\n",
    "        'questions': list of question strings\n",
    "        'answers': list of answer strings\n",
    "        'question_ids': list of question IDs\n",
    "        'categories': list of category strings\n",
    "    \"\"\"\n",
    "    gat_model.eval()\n",
    "    for m in kge_models.values():\n",
    "        m.eval()\n",
    "\n",
    "    all_gat, all_transe, all_distmult = [], [], []\n",
    "    all_questions, all_answers, all_qids, all_cats = [], [], [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch_gpu = batch.to(device)\n",
    "\n",
    "        # GAT embedding\n",
    "        gat_emb = gat_model(batch_gpu.lpg_x, batch_gpu.lpg_edge_index, batch_gpu.lpg_batch)\n",
    "        all_gat.append(gat_emb.cpu())\n",
    "\n",
    "        # KGE embeddings\n",
    "        for name, model in kge_models.items():\n",
    "            emb = model(batch_gpu)\n",
    "            if name == 'transe':\n",
    "                all_transe.append(emb.cpu())\n",
    "            else:\n",
    "                all_distmult.append(emb.cpu())\n",
    "\n",
    "        all_questions.extend(batch.questions)\n",
    "        all_answers.extend(batch.answers)\n",
    "        all_qids.extend(batch.question_ids)\n",
    "        all_cats.extend(batch.categories)\n",
    "\n",
    "    return {\n",
    "        'gat': F.normalize(torch.cat(all_gat, dim=0), dim=-1),\n",
    "        'transe': F.normalize(torch.cat(all_transe, dim=0), dim=-1),\n",
    "        'distmult': F.normalize(torch.cat(all_distmult, dim=0), dim=-1),\n",
    "        'questions': all_questions,\n",
    "        'answers': all_answers,\n",
    "        'question_ids': all_qids,\n",
    "        'categories': all_cats,\n",
    "    }\n",
    "\n",
    "\n",
    "print('Extracting embeddings...')\n",
    "train_emb = extract_embeddings(gat_model, kge_models, train_loader)\n",
    "val_emb   = extract_embeddings(gat_model, kge_models, val_loader)\n",
    "test_emb  = extract_embeddings(gat_model, kge_models, test_loader)\n",
    "\n",
    "print(f'  Train: {train_emb[\"gat\"].shape[0]} samples')\n",
    "print(f'  Val:   {val_emb[\"gat\"].shape[0]} samples')\n",
    "print(f'  Test:  {test_emb[\"gat\"].shape[0]} samples')\n",
    "print(f'  Embedding dim: {train_emb[\"gat\"].shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 7: Question-Answer Embedding Baseline\n\n**Objective:** Create text-only baseline using SentenceTransformer (`all-MiniLM-L6-v2`, 384d).\n\n**Process:**\n1. Encode all test answers → `test_answer_emb` [N, 384]\n2. Encode all test questions → `test_question_emb` [N, 384]\n3. Quick sanity check: mean cosine similarity between graph embeddings and answer embeddings\n\n**Note:** This baseline measures how well pure text similarity (question↔answer) performs, without any graph structure. It's the lower bound that graph models must beat to justify their complexity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Question-Answer Embedding Baseline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2', device=str(device))\n",
    "\n",
    "\n",
    "def encode_texts(texts, batch_size=64):\n",
    "    \"\"\"Encode texts with sentence-transformers → [N, 384] normalized.\"\"\"\n",
    "    embs = st_model.encode(texts, batch_size=batch_size, show_progress_bar=False,\n",
    "                           convert_to_tensor=True, normalize_embeddings=True)\n",
    "    return embs.cpu()\n",
    "\n",
    "\n",
    "print('Encoding answers with sentence-transformers...')\n",
    "test_answer_emb = encode_texts(test_emb['answers'])\n",
    "test_question_emb = encode_texts(test_emb['questions'])\n",
    "\n",
    "# Quick check: cosine similarity between graph embeddings and answer embeddings\n",
    "print('\\nMean cosine similarity (graph_emb · answer_emb):')\n",
    "for name in ['gat', 'transe', 'distmult']:\n",
    "    cos_sim = (test_emb[name] * test_answer_emb).sum(dim=-1).mean().item()\n",
    "    print(f'  {name:10s}: {cos_sim:.4f}')\n",
    "\n",
    "# Baseline: question-only embedding\n",
    "q_cos = (test_question_emb * test_answer_emb).sum(dim=-1).mean().item()\n",
    "print(f'  {\"question\":10s}: {q_cos:.4f} (text-only baseline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 8: Retrieval Evaluation (Graph → Answer)\n\n**Objective:** Core evaluation — can graph embeddings retrieve the correct answer?\n\n**Task setup:**\n- Query: `graph_emb[i]` (384d, from GAT/TransE/DistMult)\n- Corpus: `answer_emb[j]` (384d, from SentenceTransformer)\n- Score: cosine similarity\n- Ground truth: `i == j` (diagonal of similarity matrix)\n\n**Metrics:**\n- **MRR** (Mean Reciprocal Rank) — average of 1/rank of correct answer\n- **Recall@1/5/10** — fraction of queries where correct answer is in top-K\n\n**Baseline:** Question text embedding → Answer text embedding (no graph). If this beats graph models, graph information adds no value over text."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Retrieval Evaluation\n",
    "\n",
    "def retrieval_eval(query_emb, corpus_emb, ks=(1, 5, 10)):\n",
    "    \"\"\"Compute retrieval metrics: Recall@K and MRR.\n",
    "\n",
    "    Each query[i] should retrieve corpus[i] (diagonal = ground truth).\n",
    "\n",
    "    Args:\n",
    "        query_emb: [N, D] query embeddings (graph or question)\n",
    "        corpus_emb: [N, D] corpus embeddings (answers)\n",
    "        ks: tuple of K values for Recall@K\n",
    "\n",
    "    Returns:\n",
    "        dict with 'mrr' and 'recall@k' for each k\n",
    "    \"\"\"\n",
    "    # Similarity matrix [N, N]\n",
    "    sim = query_emb @ corpus_emb.T\n",
    "    N = sim.shape[0]\n",
    "\n",
    "    # Rank of the correct answer (diagonal)\n",
    "    diag = sim.diag().unsqueeze(1)  # [N, 1]\n",
    "    ranks = (sim >= diag).sum(dim=1).float()  # [N] — 1-based rank\n",
    "\n",
    "    results = {'mrr': (1.0 / ranks).mean().item()}\n",
    "    for k in ks:\n",
    "        results[f'recall@{k}'] = (ranks <= k).float().mean().item()\n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "print(f'{\"Model\":<12} {\"MRR\":>8} {\"R@1\":>8} {\"R@5\":>8} {\"R@10\":>8}')\n",
    "print('-' * 48)\n",
    "\n",
    "retrieval_results = {}\n",
    "for name in ['gat', 'transe', 'distmult']:\n",
    "    r = retrieval_eval(test_emb[name], test_answer_emb)\n",
    "    retrieval_results[name] = r\n",
    "    print(f'{name:<12} {r[\"mrr\"]:8.4f} {r[\"recall@1\"]:8.4f} {r[\"recall@5\"]:8.4f} {r[\"recall@10\"]:8.4f}')\n",
    "\n",
    "# Baseline: question text → answer text retrieval\n",
    "r_baseline = retrieval_eval(test_question_emb, test_answer_emb)\n",
    "retrieval_results['question'] = r_baseline\n",
    "print(f'{\"question\":<12} {r_baseline[\"mrr\"]:8.4f} {r_baseline[\"recall@1\"]:8.4f} {r_baseline[\"recall@5\"]:8.4f} {r_baseline[\"recall@10\"]:8.4f}')\n",
    "print('  (question = text-only baseline, no graph)')\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "models = list(retrieval_results.keys())\n",
    "metrics = ['mrr', 'recall@1', 'recall@5', 'recall@10']\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    vals = [retrieval_results[model][m] for model in models]\n",
    "    ax.bar(x + i * width, vals, width, label=m.upper())\n",
    "\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels([m.upper() for m in models])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Graph → Answer Retrieval Performance')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 9: Category-wise Analysis\n\n**Objective:** Break down retrieval performance by FinDER's 8 question categories.\n\n**Method:**\n- For each category, compute MRR and Recall@K using only that category's samples\n- Identify best model per category\n- Bar chart comparison across categories\n\n**Categories:** Financials, Insurance, Banking, Real Estate, Securities, Economics, Accounting, General\n\n**Key insight:** Different graph structures may capture different types of financial relationships better — e.g., GAT may excel in densely connected LPG categories while KGE models may perform better for categories with clear directional relationships."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Category-wise Analysis\n",
    "\n",
    "def category_retrieval_eval(emb_dict, answer_emb, categories):\n",
    "    \"\"\"Evaluate retrieval per category.\n",
    "\n",
    "    Returns DataFrame: rows=categories, columns=model×metric.\n",
    "    \"\"\"\n",
    "    cats = sorted(set(categories))\n",
    "    cat_array = np.array(categories)\n",
    "    rows = []\n",
    "\n",
    "    for cat in cats:\n",
    "        mask = cat_array == cat\n",
    "        n = mask.sum()\n",
    "        if n < 2:\n",
    "            continue\n",
    "        indices = np.where(mask)[0]\n",
    "\n",
    "        row = {'category': cat, 'n': int(n)}\n",
    "        for model_name in ['gat', 'transe', 'distmult']:\n",
    "            q_emb = emb_dict[model_name][indices]\n",
    "            a_emb = answer_emb[indices]\n",
    "            r = retrieval_eval(q_emb, a_emb)\n",
    "            for metric, val in r.items():\n",
    "                row[f'{model_name}_{metric}'] = val\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "cat_df = category_retrieval_eval(test_emb, test_answer_emb, test_emb['categories'])\n",
    "print('Category-wise MRR:')\n",
    "print(cat_df[['category', 'n', 'gat_mrr', 'transe_mrr', 'distmult_mrr']].to_string(index=False))\n",
    "\n",
    "# Bar chart: category × model MRR\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "cats = cat_df['category'].values\n",
    "x = np.arange(len(cats))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(['gat', 'transe', 'distmult']):\n",
    "    vals = cat_df[f'{model}_mrr'].values\n",
    "    ax.bar(x + i * width, vals, width, label=model.upper())\n",
    "\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(cats, rotation=30, ha='right')\n",
    "ax.set_ylabel('MRR')\n",
    "ax.set_title('Category-wise Graph → Answer Retrieval MRR')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap: model advantage per category\n",
    "print('\\nBest model per category (MRR):')\n",
    "for _, row in cat_df.iterrows():\n",
    "    mrrs = {m: row[f'{m}_mrr'] for m in ['gat', 'transe', 'distmult']}\n",
    "    best = max(mrrs, key=mrrs.get)\n",
    "    print(f'  {row[\"category\"]:25s} → {best.upper()} ({mrrs[best]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 10: Graph Size Effect\n\n**Objective:** Analyze correlation between subgraph size and retrieval quality.\n\n**Method:**\n- Collect per-sample graph sizes (LPG nodes, RDF triples) from test set\n- Compute per-sample reciprocal rank for each model\n- Scatter plot with binned trend lines (decile bins)\n- Spearman rank correlation with p-values\n\n**Expected patterns:**\n- Positive correlation → more context helps capture answer information\n- Negative correlation → noise from mean pooling dilutes signal\n- No correlation → graph size is orthogonal to answer relevance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Graph Size Effect\n",
    "\n",
    "def compute_per_sample_rank(query_emb, corpus_emb):\n",
    "    \"\"\"Compute rank of correct answer for each sample.\"\"\"\n",
    "    sim = query_emb @ corpus_emb.T\n",
    "    diag = sim.diag().unsqueeze(1)\n",
    "    ranks = (sim >= diag).sum(dim=1).float()\n",
    "    return ranks.numpy()\n",
    "\n",
    "\n",
    "# Collect graph sizes from test set\n",
    "test_lpg_nodes = []\n",
    "test_lpg_edges = []\n",
    "test_rdf_triples = []\n",
    "\n",
    "for i in range(len(test_ds)):\n",
    "    d = test_ds[i]\n",
    "    test_lpg_nodes.append(d.lpg_num_nodes.item())\n",
    "    test_lpg_edges.append(d.lpg_edge_index.shape[1])\n",
    "    test_rdf_triples.append(d.rdf_edge_index.shape[1])\n",
    "\n",
    "test_lpg_nodes = np.array(test_lpg_nodes)\n",
    "test_lpg_edges = np.array(test_lpg_edges)\n",
    "test_rdf_triples = np.array(test_rdf_triples)\n",
    "\n",
    "# Compute per-sample reciprocal rank\n",
    "rr = {}\n",
    "for name in ['gat', 'transe', 'distmult']:\n",
    "    ranks = compute_per_sample_rank(test_emb[name], test_answer_emb)\n",
    "    rr[name] = 1.0 / ranks\n",
    "\n",
    "# Scatter plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "for ax, (name, size_arr, size_label) in zip(axes, [\n",
    "    ('gat', test_lpg_nodes, 'LPG Nodes'),\n",
    "    ('transe', test_rdf_triples, 'RDF Triples'),\n",
    "    ('distmult', test_rdf_triples, 'RDF Triples'),\n",
    "]):\n",
    "    ax.scatter(size_arr, rr[name], alpha=0.3, s=10)\n",
    "    ax.set_xlabel(size_label)\n",
    "    ax.set_ylabel('Reciprocal Rank')\n",
    "    ax.set_title(f'{name.upper()}')\n",
    "\n",
    "    # Trend line via binning\n",
    "    bins = np.percentile(size_arr, np.linspace(0, 100, 11))\n",
    "    bins = np.unique(bins)\n",
    "    if len(bins) >= 2:\n",
    "        bin_idx = np.digitize(size_arr, bins) - 1\n",
    "        bin_idx = np.clip(bin_idx, 0, len(bins) - 2)\n",
    "        bin_centers = []\n",
    "        bin_means = []\n",
    "        for b in range(len(bins) - 1):\n",
    "            mask = bin_idx == b\n",
    "            if mask.sum() > 0:\n",
    "                bin_centers.append((bins[b] + bins[b+1]) / 2)\n",
    "                bin_means.append(rr[name][mask].mean())\n",
    "        ax.plot(bin_centers, bin_means, 'r-o', linewidth=2, markersize=4, label='Binned mean')\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle('Graph Size vs Retrieval Quality', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "print('Spearman correlation (graph size vs reciprocal rank):')\n",
    "from scipy.stats import spearmanr\n",
    "for name, sizes in [('gat', test_lpg_nodes), ('transe', test_rdf_triples), ('distmult', test_rdf_triples)]:\n",
    "    corr, pval = spearmanr(sizes, rr[name])\n",
    "    print(f'  {name:10s}: rho={corr:.3f}, p={pval:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 11: TransE vs DistMult Deep Dive\n\n**Objective:** Head-to-head comparison of the two KGE models on RDF triples.\n\n**Analysis:**\n1. **Training dynamics** — Loss curves + validation MRR over epochs\n2. **Per-sample win/loss** — For each test sample, which model ranks the correct answer higher?\n3. **Category-level advantage** — MRR difference (TransE − DistMult) per category; identifies which categories benefit from asymmetric (TransE) vs symmetric (DistMult) modeling\n4. **t-SNE embedding visualization** — 2D projection of graph embeddings colored by category; reveals clustering quality\n\n**Key question:** Does TransE's ability to model directed relations (OWNS, REPORTED) outweigh DistMult's training stability in specific categories?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: TransE vs DistMult Deep Dive\n",
    "\n",
    "# 1. Loss curve comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(kge_histories['transe']['train_loss'], label='TransE')\n",
    "axes[0].plot(kge_histories['distmult']['train_loss'], label='DistMult')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Val MRR over time\n",
    "eval_epochs_kge = [e for e in range(1, KGE_EPOCHS+1) if e % 10 == 0 or e == 1]\n",
    "for mt in ['transe', 'distmult']:\n",
    "    n = len(kge_histories[mt]['val_mrr'])\n",
    "    axes[1].plot(eval_epochs_kge[:n], kge_histories[mt]['val_mrr'][:n], '-o', label=mt.upper())\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Val MRR')\n",
    "axes[1].set_title('Validation MRR Over Training')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. Per-sample comparison: where does one beat the other?\n",
    "transe_rr = rr['transe']\n",
    "distmult_rr = rr['distmult']\n",
    "diff = transe_rr - distmult_rr  # positive = TransE better\n",
    "\n",
    "transe_wins = (diff > 0).sum()\n",
    "distmult_wins = (diff < 0).sum()\n",
    "ties = (diff == 0).sum()\n",
    "print(f'Per-sample comparison (test set, N={len(diff)}):')\n",
    "print(f'  TransE wins:  {transe_wins} ({100*transe_wins/len(diff):.1f}%)')\n",
    "print(f'  DistMult wins: {distmult_wins} ({100*distmult_wins/len(diff):.1f}%)')\n",
    "print(f'  Ties:         {ties} ({100*ties/len(diff):.1f}%)')\n",
    "\n",
    "\n",
    "# 3. Category-level TransE vs DistMult advantage\n",
    "print('\\nCategory-level advantage (MRR difference = TransE - DistMult):')\n",
    "for _, row in cat_df.iterrows():\n",
    "    delta = row['transe_mrr'] - row['distmult_mrr']\n",
    "    arrow = '→ TransE' if delta > 0 else '→ DistMult'\n",
    "    print(f'  {row[\"category\"]:25s}: {delta:+.4f} {arrow}')\n",
    "\n",
    "\n",
    "# 4. t-SNE visualization of embedding spaces\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Use test set embeddings (subsample if needed)\n",
    "n_vis = min(300, test_emb['gat'].shape[0])\n",
    "vis_idx = np.random.choice(test_emb['gat'].shape[0], n_vis, replace=False)\n",
    "vis_cats = np.array(test_emb['categories'])[vis_idx]\n",
    "unique_cats = sorted(set(vis_cats))\n",
    "cat_colors = {c: plt.cm.tab10(i) for i, c in enumerate(unique_cats)}\n",
    "colors = [cat_colors[c] for c in vis_cats]\n",
    "\n",
    "for ax, name in zip(axes, ['gat', 'transe', 'distmult']):\n",
    "    emb = test_emb[name][vis_idx].numpy()\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    proj = tsne.fit_transform(emb)\n",
    "\n",
    "    for cat in unique_cats:\n",
    "        mask = vis_cats == cat\n",
    "        ax.scatter(proj[mask, 0], proj[mask, 1], c=[cat_colors[cat]],\n",
    "                   s=15, alpha=0.6, label=cat[:15])\n",
    "    ax.set_title(f'{name.upper()} Embedding Space')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "plt.suptitle('t-SNE of Graph Embeddings (colored by category)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Cell 12: Summary & Conclusions\n\n**Objective:** Consolidate all results into a final summary table.\n\n**Outputs:**\n- Overall retrieval results table (MRR, Recall@K for all 3 models)\n- Best model identification (by MRR)\n- Per-category best model recommendation\n- Model strengths & weaknesses interpretation\n- Verification assertions (sanity checks)\n\n**Interpretation guide:** See `docs/design/g_retrieval_experiment.md` § \"예상 결과 해석 가이드\" for pattern-to-interpretation mapping."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Summary & Conclusions\n",
    "\n",
    "print('=' * 60)\n",
    "print('SUMMARY: G-Retrieval Style Comparison')\n",
    "print('=' * 60)\n",
    "\n",
    "# Overall retrieval results table\n",
    "results_df = pd.DataFrame([\n",
    "    {'model': name, **metrics}\n",
    "    for name, metrics in retrieval_results.items()\n",
    "    if name != 'question'\n",
    "])\n",
    "print('\\n--- Graph → Answer Retrieval (Test Set) ---')\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Best model\n",
    "best_model = results_df.loc[results_df['mrr'].idxmax(), 'model']\n",
    "print(f'\\nBest overall model: {best_model.upper()} (MRR={results_df[\"mrr\"].max():.4f})')\n",
    "\n",
    "# Category breakdown summary\n",
    "print('\\n--- Best Model per Category (MRR) ---')\n",
    "category_results = []\n",
    "for _, row in cat_df.iterrows():\n",
    "    mrrs = {m: row[f'{m}_mrr'] for m in ['gat', 'transe', 'distmult']}\n",
    "    best = max(mrrs, key=mrrs.get)\n",
    "    category_results.append({'category': row['category'], 'best_model': best,\n",
    "                             'mrr': mrrs[best], 'n': row['n']})\n",
    "    print(f'  {row[\"category\"]:25s} → {best.upper():10s} (MRR={mrrs[best]:.4f}, n={int(row[\"n\"])})')\n",
    "\n",
    "category_results = pd.DataFrame(category_results)\n",
    "\n",
    "# Model strengths\n",
    "print('\\n--- Model Strengths & Weaknesses ---')\n",
    "print('GAT (LPG):      Pre-computed 384d node features + message passing.')\n",
    "print('                 Best for: categories with rich LPG structure.')\n",
    "print('TransE (RDF):    Translation h+r≈t. Asymmetric, handles directed relations.')\n",
    "print('                 Best for: categories with directional relationships (OWNS, REPORTED).')\n",
    "print('DistMult (RDF):  Bilinear h·r·t. Symmetric, simpler training dynamics.')\n",
    "print('                 Best for: symmetric or co-occurrence patterns.')\n",
    "\n",
    "# Verification assertions\n",
    "assert len(results_df) == 3, f'Expected 3 models, got {len(results_df)}'\n",
    "assert all(col in results_df.columns for col in ['model', 'mrr', 'recall@1', 'recall@5'])\n",
    "assert len(category_results) >= 1, 'No category results'\n",
    "print(f'\\nVerification passed: {len(results_df)} models, {len(category_results)} categories.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}