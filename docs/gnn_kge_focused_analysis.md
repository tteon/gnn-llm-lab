# FinDER KG: GNN vs KGE Focused Analysis

> **ë¶„ì„ ëª©ì **: GNNì€ finderlpgì—ì„œ, Knowledge Graph Embeddingì€ finderrdfì—ì„œ ì‚¬ìš©í•  ì˜ˆì •. ê° ë°ì´í„°ë² ì´ìŠ¤ì˜ íŠ¹ì„±ì„ í•´ë‹¹ ëª¨ë¸ ê´€ì ì—ì„œ ë¶„ì„.

---

## Part 1: finderlpg â†’ GNN (Message-Passing)

### 1.1 ê°œìš”

| í•­ëª© | ê°’ | GNN ê´€ì  |
|------|-----|----------|
| Total Nodes | 17,060 | ì ì ˆí•œ ê·œëª¨, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  |
| Total Edges | 18,892 | Sparse graph (avg degree 2.21) |
| Unique Labels | 2,497+ | Multi-label ê³ ë ¤ í•„ìš” |
| Unique Rel Types | 2,971 | Edge featureë¡œ í™œìš© ê°€ëŠ¥ |

### 1.2 Node Features for GNN

#### í…ìŠ¤íŠ¸ ê¸°ë°˜ ì†ì„± (Primary Features)

| Property | Coverage | Feature Generation |
|----------|----------|-------------------|
| `name` | 7,401 nodes (43%) | SentenceTransformer â†’ 384-dim |
| `label` | 13,920 nodes (82%) | One-hot ë˜ëŠ” Label Embedding |
| `type` | 5,021 nodes (29%) | Categorical encoding |
| `description` | 1,281 nodes (8%) | Text embedding (optional) |
| `text` | 3,141 nodes (18%) | Long-form embedding |

**ê¶Œì¥ Feature ì¡°í•©**:
```python
# Primary: í…ìŠ¤íŠ¸ ì„ë² ë”©
text = f"{label}: {name}"  # "Company: Apple Inc"
node_feat = sentence_transformer.encode(text)  # [384]

# Optional: Concatenate with label embedding
label_emb = label_encoder[node_label]  # [32]
node_feat = concat(text_emb, label_emb)  # [416]
```

#### êµ¬ì¡°ì  ì†ì„± (Structural Features)

| Property | Coverage | Feature Type |
|----------|----------|--------------|
| `category` | 3,184 nodes | Categorical |
| `sentiment` | 1,426 nodes | Float [-1, 1] |
| `risk` | 1,516 nodes | Categorical/Ordinal |
| `amount` | 760 nodes | Numerical (normalize) |
| `year` | 622 nodes | Temporal |

### 1.3 Degree Distribution Analysis

```
Min: 0, Max: 3,121, Avg: 2.21
Median: 1, P90: 4, P99: 18
```

**ë¬¸ì œì  ë° í•´ê²°ì±…**:

| ë¬¸ì œ | ê°’ | í•´ê²°ì±… |
|------|-----|--------|
| Isolated nodes | 3,765 (22%) | Question ë…¸ë“œ í¬í•¨. ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œì‹œ ì œì™¸ ê°€ëŠ¥ |
| Hub node | 1ê°œ (degree 3,121) | Neighbor sampling í•„ìˆ˜ |
| Low-degree nodes | 14,552 (85%, degâ‰¤2) | PCST pruningìœ¼ë¡œ ê´€ë¦¬ |

**GAT/GraphSAGEë¥¼ ìœ„í•œ ìƒ˜í”Œë§ ì „ëµ**:
```python
# NeighborLoader ì„¤ì • (PyG)
loader = NeighborLoader(
    data,
    num_neighbors=[15, 10],  # 2-hop: 15 â†’ 10
    batch_size=32,
)
```

### 1.4 Multi-Label Analysis

| # Labels per Node | Count | ë¹„ìœ¨ |
|-------------------|-------|------|
| 1 | 3,521 | 21% |
| 2 | 13,539 | 79% |

**ëŒ€ë¶€ë¶„ 2ê°œ ë ˆì´ë¸”** (e.g., `Entity:Company`). GNN ì„¤ê³„ ì‹œ:

```python
# Option 1: Primary labelë§Œ ì‚¬ìš©
primary_label = labels[0] if 'Entity' not in labels[0] else labels[1]

# Option 2: Multi-hot encoding
multi_hot = torch.zeros(num_label_types)
for lbl in labels:
    multi_hot[label2idx[lbl]] = 1
```

### 1.5 Relationship Types (Edge Features)

**Top 10 Relationship Types**:

| Relationship | Count | ë¹„ìœ¨ |
|--------------|-------|------|
| COMPETES_WITH | 2,229 | 11.8% |
| INCLUDES | 1,358 | 7.2% |
| INVOLVES | 247 | 1.3% |
| EMPLOYS | 208 | 1.1% |
| isDomiciledIn | 199 | 1.1% |
| includes | 196 | 1.0% |
| RELATED_TO | 186 | 1.0% |
| OPERATES_IN | 173 | 0.9% |
| ... | ... | ... |

**2,971ê°œì˜ ê³ ìœ  ê´€ê³„ íƒ€ì…** â†’ Edge type embedding ê¶Œì¥

```python
# Heterogeneous GNN approach (HGT/HAN)
edge_types = [('Company', 'COMPETES_WITH', 'Company'),
              ('Company', 'EMPLOYS', 'Person'), ...]

# Or: Edge feature embedding
edge_feat = relation_embedding[rel_type]  # [64]
```

### 1.6 Graph Connectivity

| Metric | Value | ì˜ë¯¸ |
|--------|-------|------|
| Isolated nodes | 3,765 | Question ë…¸ë“œ ëŒ€ë¶€ë¶„ |
| Self-loops | 23 | ë¬´ì‹œ ê°€ëŠ¥ |
| Bidirectional pairs | 961 | ì•½ 5% ëŒ€ì¹­ ê´€ê³„ |
| Avg clustering coeff | 0.53 | ì ë‹¹í•œ í´ëŸ¬ìŠ¤í„°ë§ |

### 1.7 GNN Architecture ê¶Œì¥ì‚¬í•­

#### GAT (Graph Attention Network) âœ… ì¶”ì²œ

```python
class GATEncoder(nn.Module):
    def __init__(self, in_dim=384, hidden_dim=256, out_dim=128, heads=4):
        self.conv1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=0.6)
        self.conv2 = GATConv(hidden_dim * heads, out_dim, heads=1, dropout=0.6)

    def forward(self, x, edge_index):
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x  # [N, 128]
```

**ì í•© ì´ìœ **:
- Sparse graphì—ì„œ attentionì´ íš¨ê³¼ì 
- Hub nodeì—ì„œ ì¤‘ìš” ì´ì›ƒë§Œ ì§‘ì¤‘
- Self-attentionìœ¼ë¡œ ë…¸ë“œë³„ ê°€ì¤‘ì¹˜ í•™ìŠµ

#### GATv2 (Improved Attention) âœ… ì¶”ì²œ

```python
# Static attention problem í•´ê²°
self.conv1 = GATv2Conv(in_dim, hidden_dim, heads=4)
```

**GAT vs GATv2**:
- GAT: `attention(q, k) = LeakyReLU(a^T [Wq || Wk])`
- GATv2: `attention(q, k) = a^T LeakyReLU(W [q || k])` â†’ **ë™ì  attention**

#### GraphTransformer ğŸ”¶ ëŒ€ì•ˆ

```python
# Positional encoding ì¶”ê°€ í•„ìš”
from torch_geometric.nn import TransformerConv

class GraphTransformerEncoder(nn.Module):
    def __init__(self, in_dim, hidden_dim, heads=4):
        self.conv1 = TransformerConv(in_dim, hidden_dim, heads=heads)
        self.conv2 = TransformerConv(hidden_dim * heads, hidden_dim)
```

**ê³ ë ¤ì‚¬í•­**:
- RWPE (Random Walk Positional Encoding) ì¶”ê°€ ì‹œ ì„±ëŠ¥ í–¥ìƒ
- ê³„ì‚° ë¹„ìš©ì´ GATë³´ë‹¤ ë†’ìŒ

### 1.8 Feature Engineering Pipeline

```python
def prepare_lpg_for_gnn(nodes, edges, sentence_transformer):
    """LPG ë°ì´í„°ë¥¼ GNN ì…ë ¥ìœ¼ë¡œ ë³€í™˜"""

    # 1. Node features: í…ìŠ¤íŠ¸ ì„ë² ë”©
    texts = [f"{n.get('label', 'Entity')}: {n.get('name', n['id'])}"
             for n in nodes]
    node_features = sentence_transformer.encode(texts)  # [N, 384]

    # 2. Label encoding (optional)
    label_vocab = build_label_vocab(nodes)
    label_ids = [label_vocab[n['label']] for n in nodes]

    # 3. Edge index
    node2idx = {n['id']: i for i, n in enumerate(nodes)}
    edge_index = torch.tensor([
        [node2idx[e['source']], node2idx[e['target']]]
        for e in edges if e['source'] in node2idx and e['target'] in node2idx
    ]).T  # [2, E]

    # 4. Edge type encoding
    rel_vocab = build_relation_vocab(edges)
    edge_type = torch.tensor([rel_vocab[e['type']] for e in edges])

    return Data(
        x=torch.tensor(node_features),
        edge_index=edge_index,
        edge_attr=edge_type,
        y=label_ids  # for node classification, if needed
    )
```

---

## Part 2: finderrdf â†’ KGE (TransE/RotatE)

### 2.1 ê°œìš”

| í•­ëª© | ê°’ | KGE ê´€ì  |
|------|-----|----------|
| Total Entities | 15,505 | Entity embedding í¬ê¸° |
| Total Triples | 12,609 | í•™ìŠµ ë°ì´í„° í¬ê¸° |
| Unique Predicates | 3,371 | Relation embedding í¬ê¸° |
| Head Entities | 3,136 | ì£¼ë¡œ ì£¼ì–´ ì—­í•  |
| Tail Entities | 7,955 | ì£¼ë¡œ ëª©ì ì–´ ì—­í•  |

### 2.2 Predicate (Relation) Distribution

**Top 15 Predicates**:

| Predicate | Count | ë¹„ìœ¨ |
|-----------|-------|------|
| `fibo-fnd-rel-rel:competesWith` | 986 | 7.8% |
| `fibo-fnd-rel-rel:includes` | 581 | 4.6% |
| `rdf:type` | 341 | 2.7% |
| `fibo-fnd-rel-rel:hasCompetitor` | 198 | 1.6% |
| `fibo-fnd-rel-rel:involves` | 180 | 1.4% |
| `fibo-fnd-rel-rel:hasPosition` | 156 | 1.2% |
| `fibo-be-le-lp:isDomiciledIn` | 147 | 1.2% |
| `fibo:includes` | 129 | 1.0% |
| `fibo-fnd-rel-rel:hasEmployee` | 100 | 0.8% |
| `fibo-fnd-rel-rel:hasCharacteristic` | 93 | 0.7% |
| `fibo-fnd-rel-rel:competesOn` | 87 | 0.7% |
| `fibo-fnd-rel-rel:operatesIn` | 86 | 0.7% |
| `fibo-fnd-rel-rel:employs` | 85 | 0.7% |
| `fibo-fnd-rel-rel:provides` | 65 | 0.5% |
| `fibo-fnd-rel-rel:hasPart` | 63 | 0.5% |

**3,371ê°œì˜ ê³ ìœ  predicate** â†’ Long-tail distribution

### 2.3 Relation Pattern Analysis

#### Cardinality Patterns (1-to-1, 1-to-N, N-to-1, N-to-N)

**High Fan-out Relations (1-to-N)**:

| Relation | Avg Tails per Head |
|----------|-------------------|
| `hasRemedy` | 15.00 |
| `isAffectedBy` | 12.00 |
| `hasEmployeeInRegion` | 11.00 |
| `hasFactor` | 10.00 |
| `hasPotentialClaim` | 10.00 |

**ëŒ€ë¶€ë¶„ì˜ ê´€ê³„ê°€ 1-to-N ë˜ëŠ” N-to-N íŒ¨í„´** â†’ TransEë³´ë‹¤ **RotatE/ComplEx** ì í•©

#### Inverse Relations (ì—­ê´€ê³„ íŒ¨í„´)

| Relation 1 | Relation 2 | Count |
|------------|------------|-------|
| `promiseInContract` | `hasPerformanceObligation` | 14 |
| `hasRevenueRecognition` | `hasPerformanceObligation` | 10 |

**ì—­ê´€ê³„ ì¡´ì¬** â†’ TransEëŠ” ì—­ê´€ê³„ ëª¨ë¸ë§ ì–´ë ¤ì›€, **RotatE** ê¶Œì¥

#### Symmetric Relations (ëŒ€ì¹­ ê´€ê³„)

| Relation | Symmetric Pairs |
|----------|-----------------|
| `fibo-fnd-rel-rel:defines` | 1 |

**ëŒ€ì¹­ ê´€ê³„ ê±°ì˜ ì—†ìŒ** â†’ TransE ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ RotatEê°€ ë” ì¼ë°˜ì 

### 2.4 FIBO Ontology Structure

finderrdfëŠ” **FIBO (Financial Industry Business Ontology)** ê¸°ë°˜:

```
fibo-fnd-rel-rel:  (Fundamental Relations)
â”œâ”€â”€ competesWith
â”œâ”€â”€ includes
â”œâ”€â”€ hasCompetitor
â”œâ”€â”€ involves
â”œâ”€â”€ employs
â””â”€â”€ ...

fibo-be-le-lp:  (Business Entities - Legal Persons)
â””â”€â”€ isDomiciledIn

fibo-fnd-agr-ctr:  (Agreements - Contracts)
â”œâ”€â”€ hasPerformanceObligation
â”œâ”€â”€ promiseInContract
â””â”€â”€ hasRevenueRecognition
```

**ì˜ë¯¸ë¡ ì  ê³„ì¸µ êµ¬ì¡°** â†’ Relation clustering ë˜ëŠ” hierarchical relation embedding ê³ ë ¤

### 2.5 KGE Model ê¶Œì¥ì‚¬í•­

#### TransE âš ï¸ ì œí•œì  ì¶”ì²œ

```python
# TransE: h + r â‰ˆ t
class TransE(nn.Module):
    def __init__(self, num_entities, num_relations, dim=200):
        self.entity_emb = nn.Embedding(num_entities, dim)
        self.relation_emb = nn.Embedding(num_relations, dim)

    def score(self, h, r, t):
        # Distance: ||h + r - t||
        return -torch.norm(self.entity_emb(h) + self.relation_emb(r)
                          - self.entity_emb(t), dim=-1)
```

**ë¬¸ì œì **:
- âŒ 1-to-N ê´€ê³„ ëª¨ë¸ë§ ì–´ë ¤ì›€ (ë§ìŒ)
- âŒ ëŒ€ì¹­/ì—­ê´€ê³„ í‘œí˜„ ì œí•œ
- âœ… ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ í•™ìŠµ
- âœ… í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ

#### RotatE âœ… ì¶”ì²œ

```python
# RotatE: t = h âˆ˜ r (rotation in complex space)
class RotatE(nn.Module):
    def __init__(self, num_entities, num_relations, dim=200):
        self.entity_emb = nn.Embedding(num_entities, dim * 2)  # complex
        self.relation_emb = nn.Embedding(num_relations, dim)  # phase

    def score(self, h, r, t):
        # h, t: complex vectors [re, im]
        # r: rotation angle
        h_re, h_im = h[..., :dim], h[..., dim:]
        t_re, t_im = t[..., :dim], t[..., dim:]

        r_phase = self.relation_emb(r) / (embedding_range / pi)

        # Rotate h by r
        rotated_re = h_re * cos(r_phase) - h_im * sin(r_phase)
        rotated_im = h_re * sin(r_phase) + h_im * cos(r_phase)

        return -torch.norm(rotated_re - t_re, dim=-1) \
               -torch.norm(rotated_im - t_im, dim=-1)
```

**ì¥ì **:
- âœ… 1-to-N ê´€ê³„ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
- âœ… ëŒ€ì¹­ ê´€ê³„ (r = 0Â° or 180Â°)
- âœ… ì—­ê´€ê³„ (râ‚ = -râ‚‚)
- âœ… Composition (râ‚ âˆ˜ râ‚‚)

#### ComplEx ğŸ”¶ ëŒ€ì•ˆ

```python
# ComplEx: Re(<h, r, conj(t)>)
class ComplEx(nn.Module):
    def score(self, h, r, t):
        # All embeddings are complex
        return torch.sum(h_re * r_re * t_re + h_im * r_im * t_re
                        + h_re * r_im * t_im - h_im * r_re * t_im, dim=-1)
```

**ì¥ì **:
- âœ… Symmetric/antisymmetric ê´€ê³„ ëª¨ë‘ ì²˜ë¦¬
- âœ… ì´ë¡ ì ìœ¼ë¡œ ê°€ì¥ í‘œí˜„ë ¥ ë†’ìŒ

### 2.6 Long-tail Predicate ë¬¸ì œ

```
Unique predicates: 3,371
Total triples: 12,609
Average triples per predicate: 3.74
```

**ëŒ€ë¶€ë¶„ì˜ predicateê°€ ë§¤ìš° sparse** â†’ í•´ê²°ì±…:

1. **Predicate Clustering**: ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ predicate ê·¸ë£¹í™”
   ```python
   # FIBO namespace ê¸°ë°˜ ê·¸ë£¹í™”
   def cluster_predicates(pred):
       if 'competes' in pred.lower():
           return 'COMPETITION'
       elif 'employ' in pred.lower() or 'position' in pred.lower():
           return 'EMPLOYMENT'
       ...
   ```

2. **Hierarchical Relation Embedding**: FIBO ê³„ì¸µ êµ¬ì¡° í™œìš©
   ```python
   # Parent relation embedding ê³µìœ 
   rel_emb = parent_emb + specific_emb
   ```

3. **Relation Frequency Filtering**: í¬ì†Œ predicate ì œê±°
   ```python
   # ìµœì†Œ 5ê°œ ì´ìƒ tripleë§Œ ì‚¬ìš©
   filtered_triples = [t for t in triples if pred_count[t.pred] >= 5]
   ```

### 2.7 Training Pipeline for KGE

```python
def prepare_rdf_for_kge(triples):
    """RDF triplesë¥¼ KGE í•™ìŠµìš©ìœ¼ë¡œ ë³€í™˜"""

    # 1. Entity/Relation vocabulary
    entities = set()
    relations = set()
    for h, r, t in triples:
        entities.add(h)
        entities.add(t)
        relations.add(r)

    entity2idx = {e: i for i, e in enumerate(entities)}
    relation2idx = {r: i for i, r in enumerate(relations)}

    # 2. Index triples
    indexed_triples = torch.tensor([
        [entity2idx[h], relation2idx[r], entity2idx[t]]
        for h, r, t in triples
    ])  # [N, 3]

    return indexed_triples, entity2idx, relation2idx

def train_kge(model, triples, epochs=100, lr=0.001):
    """Negative sampling + margin ranking loss"""
    optimizer = Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        # Positive triples
        pos_scores = model.score(triples[:, 0], triples[:, 1], triples[:, 2])

        # Negative sampling (corrupt tail)
        neg_t = torch.randint(0, num_entities, (len(triples),))
        neg_scores = model.score(triples[:, 0], triples[:, 1], neg_t)

        # Margin ranking loss
        loss = F.relu(margin - pos_scores + neg_scores).mean()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 2.8 KGE â†’ LLM Integration

```python
def kge_enhanced_context(question, model, entity2idx, idx2entity, top_k=10):
    """KGE ì„ë² ë”©ì„ í™œìš©í•œ context ìƒì„±"""

    # 1. Questionì—ì„œ entity ì¶”ì¶œ
    question_entities = extract_entities(question)

    # 2. ê° entityì˜ ê´€ë ¨ triple ê²€ìƒ‰
    relevant_triples = []
    for ent in question_entities:
        if ent in entity2idx:
            ent_idx = entity2idx[ent]
            # Find nearest entities in embedding space
            ent_emb = model.entity_emb(ent_idx)
            similarities = cosine_similarity(ent_emb, model.entity_emb.weight)
            top_entities = similarities.topk(top_k).indices
            # ... retrieve triples involving these entities

    # 3. Tripleì„ ìì—°ì–´ë¡œ ë³€í™˜
    context = format_triples_for_llm(relevant_triples)

    return context
```

---

## Part 3: Implementation Roadmap

### 3.1 GNN Pipeline (finderlpg)

```
1. Data Extraction (Neo4j â†’ PyG Data)
   â””â”€â”€ Query subgraph per question
   â””â”€â”€ Build node features (text embedding)
   â””â”€â”€ Build edge index & edge types

2. GNN Training
   â””â”€â”€ GATv2 or GraphTransformer
   â””â”€â”€ 2-3 layers, 4 heads
   â””â”€â”€ Output: graph-level embedding [256-dim]

3. LLM Integration
   â””â”€â”€ Soft prompt: project GNN output â†’ LLM space
   â””â”€â”€ Or: Hard prompt: format graph as text
```

### 3.2 KGE Pipeline (finderrdf)

```
1. Triple Extraction (Neo4j â†’ PyKEEN/Custom)
   â””â”€â”€ Query triples per question
   â””â”€â”€ Build entity/relation vocabularies

2. KGE Training
   â””â”€â”€ RotatE (recommended) or TransE
   â””â”€â”€ Embedding dim: 200-400
   â””â”€â”€ Negative sampling ratio: 10-50

3. LLM Integration
   â””â”€â”€ Retrieve relevant triples via embedding similarity
   â””â”€â”€ Format as structured text for LLM
```

### 3.3 Comparison Experiment Design

| Experiment | Data Source | Model | Context Format |
|------------|-------------|-------|----------------|
| **[A] LLM Only** | Question text | Llama 3.1 8B | None |
| **[B] Text RAG** | references | Llama 3.1 8B | Text chunks |
| **[C] Graph LPG** | finderlpg | GAT/GATv2 + LLM | Soft prompt |
| **[D] Graph RDF** | finderrdf | RotatE + LLM | Triple text |

---

## Part 4: Key Recommendations

### For GNN (finderlpg)

1. **Architecture**: GATv2 > GAT > GraphTransformer
2. **Node Features**: SentenceTransformer (384-dim) + Label embedding (32-dim)
3. **Edge Features**: Relation type embedding (64-dim)
4. **Sampling**: NeighborLoader with [15, 10] neighbors
5. **Hub Handling**: Attention-based neighbor selection

### For KGE (finderrdf)

1. **Architecture**: RotatE > ComplEx > TransE
2. **Embedding Dim**: 200-400
3. **Predicate Handling**: Cluster or filter sparse predicates
4. **Training**: Negative sampling with margin ranking loss
5. **Integration**: Embedding-based triple retrieval

### Common

1. **PCST**: Prize-Collecting Steiner Treeë¡œ ì„œë¸Œê·¸ë˜í”„ pruning
2. **Soft Prompting**: G-Retriever ë°©ì‹ì˜ MLP projection
3. **Evaluation**: EM, F1, BERTScore on FinDER QA
