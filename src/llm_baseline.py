import torch
import os

# 1. í˜„ì¬ Colabì˜ PyTorch & CUDA ë²„ì „ í™•ì¸
TORCH = torch.__version__.split('+')[0]
CUDA = 'cu' + torch.version.cuda.replace('.', '')

print(f"Detected PyTorch: {TORCH}")
print(f"Detected CUDA: {CUDA}")

# 2. PyTorch Geometric ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ë²„ì „ í˜¸í™˜ì„± ë§ì¶¤)
# pyg_lib, torch_scatter, torch_sparse ë“±ì„ í˜„ì¬ í™˜ê²½ì— ë§ì¶° ì„¤ì¹˜í•©ë‹ˆë‹¤.
install_cmd = f"pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html"
print(f"Executing: {install_cmd}")
os.system(install_cmd)

# 3. ê·¸ ì™¸ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Transformers, GNN, ë“±)
!pip install -q torch_geometric transformers accelerate huggingface_hub

import torch
import ast
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM, AutoTokenizer

class BaselineAnalyzer:
    def __init__(self, model_id="meta-llama/Meta-Llama-3.1-8B-Instruct"):
        print(f"ğŸ”„ Loading Model: {model_id}...")

        # 1. Device ì„¤ì •
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   Diagnostic: Detected device is '{self.device}'")

        # 2. Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # 3. Model ë¡œë“œ
        # device_map="auto"ê°€ ê°€ë” CPUë¡œ ì¡ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´
        # ëª…ì‹œì ìœ¼ë¡œ .to(device)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            # device_map="auto", # ì£¼ì„ ì²˜ë¦¬: ì§ì ‘ ì œì–´í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „í•¨
            attn_implementation="eager"
        )

        # [í•µì‹¬ ìˆ˜ì •] ëª¨ë¸ì„ ê°•ì œë¡œ í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ… Model Loaded Successfully on {self.model.device} (Eager Mode On)")

    def run_experiment(self, df, sample_idx=0, use_context=False, visualize=True):
        """
        ë©”ì¸ ì‹¤í—˜ í•¨ìˆ˜
        """
        row = df.iloc[sample_idx]
        question = row['text']
        ground_truth = row['answer']

        mode_str = "Text RAG (with Context)" if use_context else "LLM Only (No Context)"
        print(f"\nğŸš€ Running Experiment: [ {mode_str} ] | Sample {sample_idx}")
        print("="*80)

        # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if use_context:
            ref_str = row['references']
            try:
                parsed = ast.literal_eval(ref_str) if isinstance(ref_str, str) else ref_str
                context_text = "\n".join(parsed) if isinstance(parsed, list) else str(ref_str)
            except:
                context_text = str(ref_str)

            messages = [
                {"role": "system", "content": "You are a financial expert. Answer based ONLY on the provided context."},
                {"role": "user", "content": f"Context:\n{context_text}\n\nQuestion:\n{question}"}
            ]
        else:
            messages = [
                {"role": "system", "content": "You are a financial expert. Answer based on your general knowledge."},
                {"role": "user", "content": question}
            ]

        # 2. í† í¬ë‚˜ì´ì§•
        # [í•µì‹¬ ìˆ˜ì •] self.device ëŒ€ì‹  self.model.deviceë¥¼ ì‚¬ìš©í•˜ì—¬ 100% ì¼ì¹˜ì‹œí‚´
        input_ids = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(self.model.device)

        # 3. ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_new_tokens=100,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id
            )

        # 4. ê²°ê³¼ ì¶œë ¥
        generated_text = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
        print(f"ğŸ“Œ Question: {question}")
        if use_context:
            print(f"ğŸ“„ Context Length: {len(context_text)} chars")
        print(f"ğŸ¤– Prediction: {generated_text.strip()}")
        print(f"âœ… Ground Truth: {ground_truth}")
        print("-" * 80)

        # 5. ì‹œê°í™”
        if visualize:
            self._visualize_results(outputs, title_suffix=mode_str)

    def _visualize_results(self, generated_ids, title_suffix=""):
        """ì‹œê°í™” í•¨ìˆ˜"""
        print("ğŸ¨ Generating Attention Maps...")
        tokens = self.tokenizer.convert_ids_to_tokens(generated_ids[0])

        # Attention Score ê³„ì‚°
        with torch.no_grad():
            out = self.model(generated_ids, output_attentions=True)

        view_len = min(len(tokens), 150)
        tokens_view = tokens[-view_len:]

        # Plot 1: Last Layer Map
        attn_matrix = out.attentions[-1][0].mean(dim=0).float().cpu().numpy()
        attn_view = attn_matrix[-view_len:, -view_len:]

        plt.figure(figsize=(20, 8))

        plt.subplot(1, 2, 1)
        sns.heatmap(attn_view, xticklabels=tokens_view, yticklabels=tokens_view, cmap="viridis", square=True)
        plt.title(f"Last Layer Attention Map ({title_suffix})")
        plt.xticks(rotation=90)
        plt.yticks(rotation=0)

        # Plot 2: Layer-wise Flow
        target_idx = -1
        layer_scores = []
        for layer_attn in out.attentions:
            avg_head = layer_attn[0].mean(dim=0)
            layer_scores.append(avg_head[target_idx, :].float().cpu().numpy())

        layer_matrix = np.stack(layer_scores)
        layer_matrix[:, 0] = 0 # BOS ë…¸ì´ì¦ˆ ì œê±°
        layer_view = layer_matrix[:, -view_len:]

        plt.subplot(1, 2, 2)
        sns.heatmap(layer_view, xticklabels=tokens_view, yticklabels=[f"L{i}" for i in range(len(layer_scores))], cmap="magma")
        plt.title(f"Layer-wise Attention Flow ({title_suffix})")
        plt.xlabel("Source Tokens")
        plt.ylabel("Model Depth")
        plt.xticks(rotation=90)

        plt.tight_layout()
        plt.show()

# =============================================================================
# ğŸš€ ì‹¤í–‰
# =============================================================================

# 1. ì´ˆê¸°í™” (ê¸°ì¡´ analyzer ë®ì–´ì“°ê¸°)
analyzer = BaselineAnalyzer()

# 2. ì‹¤í—˜ ì‹¤í–‰ (df_merged í•„ìš”)
if 'df_merged' in locals():
    SAMPLE_IDX = 0

    # [ì‹¤í—˜ A] LLM Only
    analyzer.run_experiment(df_merged, sample_idx=SAMPLE_IDX, use_context=False)

    # [ì‹¤í—˜ B] Text RAG
    analyzer.run_experiment(df_merged, sample_idx=SAMPLE_IDX, use_context=True)


